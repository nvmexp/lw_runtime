#include "utils.h"
#define WINDOWSPARSE_THREADS 128
#define WINDOWSPARSE_STREAMS 8

__global__ void lwnnx_WindowSparse_copyBiasOutput_kernel(
  float *output, const float** bias, int outputWindowSize)
{
  unsigned int k = blockIdx.x;
  const float *bias_k = bias[k];
  float *output_k = output + outputWindowSize*k;
  
  for (unsigned int i=threadIdx.x; i<outputWindowSize; i+=blockDim.x)
  {
    output_k[i] = bias_k[i];
  }
}

  
static int lwnnx_WindowSparse_updateOutput(lua_State *L)
{ 
  /* input, inputIndice, outputIndice, gradOutput*/
  THCState *state = getLwtorchState(L);
  // batchSize x inputWindowSize x inputSize
  THLwdaTensor *input = (THLwdaTensor*)luaT_checkudata(L, 2, "torch.LwdaTensor");  
  // batchSize
  THLongTensor *inputIndice = (THLongTensor*)luaT_checkudata(L, 3, "torch.LongTensor");
  THLongTensor *outputIndice = (THLongTensor*)luaT_checkudata(L, 4, "torch.LongTensor");
  
  int batchedGemmMax = luaT_getfieldcheckint(L, 1, "batchedGemmMax");
  int inputSize = luaT_getfieldcheckint(L, 1, "inputSize");
  int outputSize = luaT_getfieldcheckint(L, 1, "outputSize");
  int outputWindowSize = luaT_getfieldcheckint(L, 1, "outputWindowSize");
  int batchSize, inputWindowSize;
  
  // outputSize x inputSize
  THLwdaTensor *weight = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "weight", "torch.LwdaTensor");
  // outputSize
  THLwdaTensor *bias = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "bias", "torch.LwdaTensor");
  // batchSize
  THCharTensor *biasHost = (THCharTensor*)luaT_getfieldcheckudata(L, 1, "inputHost", "torch.CharTensor");
  THLwdaTensor *biasLwda = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "inputLwda", "torch.LwdaTensor");
  // batchSize x outputWindowSize
  THLwdaTensor *output = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "_output", "torch.LwdaTensor");
  
  THLwdaTensor* output_, *weight_, *_weight_, *bias_, *input_;
  
  lwblasStatus_t stat;
  lwblasHandle_t handle;
  
  float alpha = 1;
  float beta = 1;
  
  luaL_argcheck(L, input->nDimension == 2, 2, "2D(batch mode) tensor expected");
  luaL_argcheck(L, input->size[1] <= inputSize, 2, "invalid input size"); 
  luaL_argcheck(L, inputIndice->nDimension == 1, 3, "1D(batch mode) tensor expected");
  luaL_argcheck(L, outputIndice->nDimension == 1, 4, "1D(batch mode) tensor expected");
  luaL_argcheck(L, THLwdaTensor_isContiguous(state, input), 2, "Expecting contiguous input");
  
  batchSize = input->size[0];
  inputWindowSize = input->size[1];
  
  THLwdaTensor_resize2d(state, output, input->size[0], outputWindowSize);
    
  stat = lwblasCreate(&handle);
  if (stat != LWBLAS_STATUS_SUCCESS) 
    THError("LWBLAS initialization failed");
    
  output_ = THLwdaTensor_new(state);
  weight_ = THLwdaTensor_new(state);
  _weight_ = THLwdaTensor_new(state);
  bias_ = THLwdaTensor_new(state);
  input_ = THLwdaTensor_new(state);
  
  /* copy bias into output */
  THCharTensor_resize1d(biasHost, batchSize*sizeof(float*));
  THLwdaTensor_resize1d(state, biasLwda, batchSize*sizeof(float*)/sizeof(float));
  
  const float **biasB = (const float **)THCharTensor_data(biasHost);
  const float **biasB_d = (const float **)THLwdaTensor_data(state, biasLwda);
  
  for (int i=0; i<batchSize; i++)
  {
    int outputIdx = THLongTensor_get1d(outputIndice, i) - 1;
    THLwdaTensor_narrow(state, bias_, bias, 0, outputIdx, outputWindowSize);
    biasB[i] = THLwdaTensor_data(state, bias_);
  }
  
  if(lwdaMemcpy(biasB_d, biasB, sizeof(float*) * batchSize, lwdaMemcpyHostToDevice) != lwdaSuccess)
    THError("lwdaMemcpy failed");
  
  /* call lwdakernel */
  dim3 blocks(batchSize); // each lwca-block is an example
  dim3 threads(WINDOWSPARSE_THREADS);
  lwnnx_WindowSparse_copyBiasOutput_kernel<<<blocks,threads>>>(
    THLwdaTensor_data(state, output), biasB_d, outputWindowSize
  );
  
  lwdaError errcode = lwdaGetLastError();
  if(errcode != lwdaSuccess)
    THError(lwdaGetErrorString(errcode));
  
  if (sqrt(inputWindowSize*outputWindowSize) > batchedGemmMax)
  {
    lwdaStream_t streams[WINDOWSPARSE_STREAMS];
    
    for (int i=0; i<WINDOWSPARSE_STREAMS; i++)
    {
      if (lwdaStreamCreate(&streams[i]) != lwdaSuccess)
        THError("error initializing stream");
    }
    lwdaDeviceSynchronize();
    
    for (int i=0; i<batchSize; i++)
    {
      lwblasSetStream(handle, streams[i%WINDOWSPARSE_STREAMS]);
      
      int inputIdx = THLongTensor_get1d(inputIndice, i) - 1;
      int outputIdx = THLongTensor_get1d(outputIndice, i) - 1;
      
      THLwdaTensor_select(state, output_, output, 0, i);
      THLwdaTensor_select(state, input_, input, 0, i);
      THLwdaTensor_narrow(state, _weight_, weight, 1, inputIdx, inputWindowSize);
      THLwdaTensor_narrow(state, weight_, _weight_, 0, outputIdx, outputWindowSize);
      
      stat = lwblasSgemv(handle, LWBLAS_OP_T,  inputWindowSize, outputWindowSize,
                        &alpha, (const float*)THLwdaTensor_data(state, weight_), inputSize,
                        (const float*)THLwdaTensor_data(state, input_), 1,
                        &beta, THLwdaTensor_data(state, output_), 1);
    }
    
    lwblasSetStream(handle, NULL);
    lwdaDeviceSynchronize();
    
    for (int i=0; i<WINDOWSPARSE_STREAMS; i++)
    {
      if (lwdaStreamDestroy(streams[i]) != lwdaSuccess)
        THError("error destroying stream");
    }
    
    
  }
  else
  {  
    THCharTensor *inputHost = (THCharTensor*)luaT_getfieldcheckudata(L, 1, "inputHost", "torch.CharTensor");
    THCharTensor *weightHost = (THCharTensor*)luaT_getfieldcheckudata(L, 1, "weightHost", "torch.CharTensor");
    THCharTensor *outputHost = (THCharTensor*)luaT_getfieldcheckudata(L, 1, "outputHost", "torch.CharTensor");
    
    THLwdaTensor *inputLwda = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "inputLwda", "torch.LwdaTensor");
    THLwdaTensor *weightLwda = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "weightLwda", "torch.LwdaTensor");
    THLwdaTensor *outputLwda = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "outputLwda", "torch.LwdaTensor");
    // put output back on top of the stack
    output = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "_output", "torch.LwdaTensor");
    
    lwblasSetStream(handle, NULL);
    
    THCharTensor_resize1d(inputHost, batchSize*sizeof(float*));
    THCharTensor_resize1d(weightHost, batchSize*sizeof(float*));
    THCharTensor_resize1d(outputHost, batchSize*sizeof(float*));
    
    THLwdaTensor_resize1d(state, inputLwda, batchSize*sizeof(float*)/sizeof(float));
    THLwdaTensor_resize1d(state, weightLwda, batchSize*sizeof(float*)/sizeof(float));
    THLwdaTensor_resize1d(state, outputLwda, batchSize*sizeof(float*)/sizeof(float));
    
    const float **inputB = (const float **)THCharTensor_data(inputHost);
    const float **weightB = (const float **)THCharTensor_data(weightHost);
    float **outputB = (float **)THCharTensor_data(outputHost);
    
    const float **inputB_d = (const float **)THLwdaTensor_data(state, inputLwda);
    const float **weightB_d = (const float **)THLwdaTensor_data(state, weightLwda);
    float **outputB_d = (float **)THLwdaTensor_data(state, outputLwda);
    
    for (int i=0; i<batchSize; i++)
    {
      int inputIdx = THLongTensor_get1d(inputIndice, i) - 1;
      int outputIdx = THLongTensor_get1d(outputIndice, i) - 1;
      
      THLwdaTensor_select(state, output_, output, 0, i);
      THLwdaTensor_select(state, input_, input, 0, i);
      THLwdaTensor_narrow(state, _weight_, weight, 1, inputIdx, inputWindowSize);
      THLwdaTensor_narrow(state, weight_, _weight_, 0, outputIdx, outputWindowSize);
      
      inputB[i] = THLwdaTensor_data(state, input_);
      weightB[i] = THLwdaTensor_data(state, weight_);
      outputB[i] = THLwdaTensor_data(state, output_);
    }
    
    if(lwdaMemcpy(inputB_d, inputB, sizeof(float*) * batchSize, lwdaMemcpyHostToDevice) != lwdaSuccess)
      THError("lwdaMemcpy failed");
    if(lwdaMemcpy(weightB_d, weightB, sizeof(float*) * batchSize, lwdaMemcpyHostToDevice) != lwdaSuccess)
      THError("lwdaMemcpy failed");
    if(lwdaMemcpy(outputB_d, outputB, sizeof(float*) * batchSize, lwdaMemcpyHostToDevice) != lwdaSuccess)
      THError("lwdaMemcpy failed");
    
                  
    stat = lwblasSgemmBatched(handle, LWBLAS_OP_T, LWBLAS_OP_N,
                             outputWindowSize, 1, inputWindowSize,
                             &alpha, weightB_d, inputSize, 
                             inputB_d, inputWindowSize, 
                             &beta, outputB_d, outputWindowSize, 
                             batchSize);
    
    if (stat != LWBLAS_STATUS_SUCCESS) 
      THError("lwblasSgemmBatched failed");
    
    
  }
  
  lwblasDestroy(handle);
  
  THLwdaTensor_free(state, input_);
  THLwdaTensor_free(state, weight_);
  THLwdaTensor_free(state, _weight_);
  THLwdaTensor_free(state, output_);
  THLwdaTensor_free(state, bias_);

  return 1;
}



static int lwnnx_WindowSparse_updateGradInput(lua_State *L)
{ 
  /* input, inputIndice, outputIndice, gradOutput*/
  THCState *state = getLwtorchState(L);
  THLwdaTensor *input = (THLwdaTensor*)luaT_checkudata(L, 2, "torch.LwdaTensor");  
  THLongTensor *inputIndice = (THLongTensor*)luaT_checkudata(L, 3, "torch.LongTensor");
  THLongTensor *outputIndice = (THLongTensor*)luaT_checkudata(L, 4, "torch.LongTensor");
  THLwdaTensor *gradOutput = (THLwdaTensor*)luaT_checkudata(L, 5, "torch.LwdaTensor");
  
  int batchedGemmMax = luaT_getfieldcheckint(L, 1, "batchedGemmMax");
  int inputSize = luaT_getfieldcheckint(L, 1, "inputSize");
  int outputSize = luaT_getfieldcheckint(L, 1, "outputSize");
  int outputWindowSize = luaT_getfieldcheckint(L, 1, "outputWindowSize");
  int batchSize, inputWindowSize;
  
  THLwdaTensor *weight = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "weight", "torch.LwdaTensor");
  THLwdaTensor *gradInput = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "_gradInput", "torch.LwdaTensor");
  THLwdaTensor* gradOutput_, *weight_, *_weight_, *gradInput_;
  
  lwblasStatus_t stat;
  lwblasHandle_t handle;
  
  float alpha = 1;
  float beta = 0;
  
  luaL_argcheck(L, input->nDimension == 2, 2, "2D(batch mode) tensor expected");
  luaL_argcheck(L, input->size[1] <= inputSize, 2, "invalid input size"); 
  luaL_argcheck(L, inputIndice->nDimension == 1, 3, "1D(batch mode) tensor expected");
  luaL_argcheck(L, outputIndice->nDimension == 1, 4, "1D(batch mode) tensor expected");
  
  THLwdaTensor_resizeAs(state, gradInput, input); 
  
  batchSize = input->size[0];
  inputWindowSize = input->size[1];
    
  stat = lwblasCreate(&handle);
  if (stat != LWBLAS_STATUS_SUCCESS) 
    THError("LWBLAS initialization failed");
    
  gradOutput_ = THLwdaTensor_new(state);
  weight_ = THLwdaTensor_new(state);
  _weight_ = THLwdaTensor_new(state);
  gradInput_ = THLwdaTensor_new(state);
  

  if (sqrt(inputWindowSize*outputWindowSize) > batchedGemmMax)
  {
    lwdaStream_t streams[WINDOWSPARSE_STREAMS];
    
    for (int i=0; i<WINDOWSPARSE_STREAMS; i++)
    {
      if (lwdaStreamCreate(&streams[i]) != lwdaSuccess)
        THError("error initializing stream");
    }
    lwdaDeviceSynchronize();
    
    for (int i=0; i<batchSize; i++)
    {
      lwblasSetStream(handle, streams[i%WINDOWSPARSE_STREAMS]);
      
      int inputIdx = THLongTensor_get1d(inputIndice, i) - 1;
      int outputIdx = THLongTensor_get1d(outputIndice, i) - 1;
      
      THLwdaTensor_select(state, gradOutput_, gradOutput, 0, i);
      THLwdaTensor_select(state, gradInput_, gradInput, 0, i);
      THLwdaTensor_narrow(state, _weight_, weight, 1, inputIdx, inputWindowSize);
      THLwdaTensor_narrow(state, weight_, _weight_, 0, outputIdx, outputWindowSize);
      
      stat = lwblasSgemv(handle, LWBLAS_OP_N,  outputWindowSize, inputWindowSize,
                        &alpha, (const float*)THLwdaTensor_data(state, weight_), inputSize,
                        (const float*)THLwdaTensor_data(state, gradOutput_), 1,
                        &beta, THLwdaTensor_data(state, gradInput_), 1);
                        
      if (stat != LWBLAS_STATUS_SUCCESS) 
        THError("lwblasSgemv failed");
    }
    
    lwblasSetStream(handle, NULL);
    lwdaDeviceSynchronize();
  
    for (int i=0; i<WINDOWSPARSE_STREAMS; i++)
    {
      if (lwdaStreamDestroy(streams[i]) != lwdaSuccess)
        THError("error destroying stream");
    }
  
  }
  else
  {  
    THCharTensor *inputHost = (THCharTensor*)luaT_getfieldcheckudata(L, 1, "inputHost", "torch.CharTensor");
    THCharTensor *weightHost = (THCharTensor*)luaT_getfieldcheckudata(L, 1, "weightHost", "torch.CharTensor");
    THCharTensor *outputHost = (THCharTensor*)luaT_getfieldcheckudata(L, 1, "outputHost", "torch.CharTensor");
    
    THLwdaTensor *inputLwda = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "inputLwda", "torch.LwdaTensor");
    THLwdaTensor *weightLwda = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "weightLwda", "torch.LwdaTensor");
    THLwdaTensor *outputLwda = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "outputLwda", "torch.LwdaTensor");
    // put gradInput back on top of the stack
    gradInput = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "_gradInput", "torch.LwdaTensor");
    
    lwblasSetStream(handle, NULL);
    
    THCharTensor_resize1d(inputHost, batchSize*sizeof(float*));
    THCharTensor_resize1d(weightHost, batchSize*sizeof(float*));
    THCharTensor_resize1d(outputHost, batchSize*sizeof(float*));
    
    THLwdaTensor_resize1d(state, inputLwda, batchSize*sizeof(float*)/sizeof(float));
    THLwdaTensor_resize1d(state, weightLwda, batchSize*sizeof(float*)/sizeof(float));
    THLwdaTensor_resize1d(state, outputLwda, batchSize*sizeof(float*)/sizeof(float));
    
    float **gradInputB = (float **)THCharTensor_data(inputHost);
    const float **weightB = (const float **)THCharTensor_data(weightHost);
    const float **gradOutputB = (const float **)THCharTensor_data(outputHost);
    
    float **gradInputB_d = (float **)THLwdaTensor_data(state, inputLwda);
    const float **weightB_d = (const float **)THLwdaTensor_data(state, weightLwda);
    const float **gradOutputB_d = (const float **)THLwdaTensor_data(state, outputLwda);
    
    for (int i=0; i<batchSize; i++)
    {
      int inputIdx = THLongTensor_get1d(inputIndice, i) - 1;
      int outputIdx = THLongTensor_get1d(outputIndice, i) - 1;
      
      THLwdaTensor_select(state, gradOutput_, gradOutput, 0, i);
      THLwdaTensor_select(state, gradInput_, gradInput, 0, i);
      THLwdaTensor_narrow(state, _weight_, weight, 1, inputIdx, inputWindowSize);
      THLwdaTensor_narrow(state, weight_, _weight_, 0, outputIdx, outputWindowSize);
      
      gradInputB[i] = THLwdaTensor_data(state, gradInput_);
      weightB[i] = THLwdaTensor_data(state, weight_);
      gradOutputB[i] = THLwdaTensor_data(state, gradOutput_);
    }
    
    if(lwdaMemcpy(gradInputB_d, gradInputB, sizeof(float*) * batchSize, lwdaMemcpyHostToDevice) != lwdaSuccess)
      THError("lwdaMemcpy failed");
    if(lwdaMemcpy(weightB_d, weightB, sizeof(float*) * batchSize, lwdaMemcpyHostToDevice) != lwdaSuccess)
      THError("lwdaMemcpy failed");
    if(lwdaMemcpy(gradOutputB_d, gradOutputB, sizeof(float*) * batchSize, lwdaMemcpyHostToDevice) != lwdaSuccess)
      THError("lwdaMemcpy failed");
                  
    stat = lwblasSgemmBatched(handle, LWBLAS_OP_N, LWBLAS_OP_N,
                             inputWindowSize, 1, outputWindowSize,
                             &alpha, weightB_d, inputSize, 
                             gradOutputB_d, outputWindowSize, 
                             &beta, gradInputB_d, inputWindowSize, 
                             batchSize);
    
    if (stat != LWBLAS_STATUS_SUCCESS) 
      THError("lwblasSgemmBatched failed");
    
    
  }
  
  lwblasDestroy(handle);
  
  THLwdaTensor_free(state, gradInput_);
  THLwdaTensor_free(state, weight_);
  THLwdaTensor_free(state, _weight_);
  THLwdaTensor_free(state, gradOutput_);

  return 1;
}
  
__global__ void lwnnx_WindowSparse_accGradParameters_kernel(
  float *gradWeight, float* gradBias, float *gradOutput, 
  float *input, float *inputIndice, float *outputIndice, 
  int inputWindowSize, int outputWindowSize, 
  int inputSize, int outputSize, float scale)
{
  __shared__ float buffer[WINDOWSPARSE_THREADS];
  int tx = threadIdx.x;
  int i_step = blockDim.x;
  int k = blockIdx.x;
  
  int inputIdx = (int)inputIndice[k] - 1;
  int outputIdx = (int)outputIndice[k] - 1;
  
  float *input_k = input + k*inputWindowSize;
  float *gradOutput_k = gradOutput + k*outputWindowSize;
  float *gradWeight_k = gradWeight + outputIdx*inputSize + inputIdx;
  float *gradBias_k = gradBias + outputIdx;

  // addr weights (scalar-products)
  for (int i=tx; i<inputWindowSize; i+=i_step)
  {
    // copy input to buffer
    buffer[tx] = input_k[i]*scale;
  
    // multiply accumulate weights
    for (int j=0; j<outputWindowSize; j++)
      atomicAdd(&(gradWeight_k[j*inputSize + i]), gradOutput_k[j]*buffer[tx]);
  }
  
  // cadd bias i.e. multiply accumulate biases
  for (int j=tx; j<outputWindowSize; j+=i_step)
    atomicAdd(&(gradBias_k[j]), gradOutput_k[j]*scale);
}


static int lwnnx_WindowSparse_accGradParameters(lua_State *L)
{ 
  /* input, inputIndice, outputIndice, gradOutput, scale */
  THCState *state = getLwtorchState(L);
  THLwdaTensor *input = (THLwdaTensor*)luaT_checkudata(L, 2, "torch.LwdaTensor");  
  THLongTensor *inputIndice = (THLongTensor*)luaT_checkudata(L, 3, "torch.LongTensor");
  THLongTensor *outputIndice = (THLongTensor*)luaT_checkudata(L, 4, "torch.LongTensor");
  THLwdaTensor *gradOutput = (THLwdaTensor*)luaT_checkudata(L, 5, "torch.LwdaTensor");
  float scale = luaL_optnumber(L, 6, 1);
  
  int inputSize = luaT_getfieldcheckint(L, 1, "inputSize");
  int outputSize = luaT_getfieldcheckint(L, 1, "outputSize");
  int outputWindowSize = luaT_getfieldcheckint(L, 1, "outputWindowSize");
  int batchSize, inputWindowSize;
  
  // nOutputBlock x nInputBlock x outputSize x inputSize
  THLwdaTensor *gradWeight = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "gradWeight", "torch.LwdaTensor");
  THLwdaTensor *gradBias = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "gradBias", "torch.LwdaTensor");
  
  THLwdaTensor *inputIndiceLwda = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "inputIndiceLwda", "torch.LwdaTensor");
  THLwdaTensor *outputIndiceLwda = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "outputIndiceLwda", "torch.LwdaTensor");

  luaL_argcheck(L, input->nDimension == 2, 2, "2D(batch mode) tensor expected");
  luaL_argcheck(L, input->size[1] <= inputSize, 2, "invalid input size"); 
  luaL_argcheck(L, inputIndice->nDimension == 1, 3, "1D(batch mode) tensor expected");
  luaL_argcheck(L, outputIndice->nDimension == 1, 4, "1D(batch mode) tensor expected");
  
  batchSize = input->size[0];
  inputWindowSize = input->size[1];
  
  THLwdaTensor_resize1d(state, inputIndiceLwda, batchSize);
  THLwdaTensor_resize1d(state, outputIndiceLwda, batchSize);
  
  THLwdaTensor_copyLong(state, inputIndiceLwda, inputIndice);
  THLwdaTensor_copyLong(state, outputIndiceLwda, outputIndice);
  
  /* call lwdakernel */
  dim3 blocks(batchSize); // each lwca-block is an example
  dim3 threads(WINDOWSPARSE_THREADS);
  lwnnx_WindowSparse_accGradParameters_kernel<<<blocks,threads>>>(
    THLwdaTensor_data(state, gradWeight), THLwdaTensor_data(state, gradBias), 
    THLwdaTensor_data(state, gradOutput), THLwdaTensor_data(state, input),
    THLwdaTensor_data(state, inputIndiceLwda), THLwdaTensor_data(state, outputIndiceLwda), 
    inputWindowSize, outputWindowSize, inputSize, outputSize, scale
  );
  
  lwdaError errcode = lwdaGetLastError();
  if(errcode != lwdaSuccess)
    THError(lwdaGetErrorString(errcode));  

  return 0;
}  
  
static const struct luaL_Reg lwnnx_WindowSparse__ [] = {
  {"WindowSparse_updateOutput", lwnnx_WindowSparse_updateOutput},
  {"WindowSparse_updateGradInput", lwnnx_WindowSparse_updateGradInput},
  {"WindowSparse_accGradParameters", lwnnx_WindowSparse_accGradParameters},
  {NULL, NULL}
};

static void lwnnx_WindowSparse_init(lua_State *L)
{
  luaT_pushmetatable(L, "torch.LwdaTensor");
  luaT_registeratname(L, lwnnx_WindowSparse__, "nn");
  lua_pop(L,1);
}
