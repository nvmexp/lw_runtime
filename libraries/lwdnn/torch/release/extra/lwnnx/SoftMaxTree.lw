#include "utils.h"
#define SOFTMAXTREE_THREADS 32
#define SOFTMAXTREE_MAXCHILDREN 10000

__global__ void lwnnx_SoftMaxTree_updateOutput_kernel(
  float *output, float *logsoftOutput, float *input, float *weight, 
  float *bias, float *target, float *childParent, float *parentChildren, 
  int nInput, int rootId, int maxFamilyPath)
{
  __shared__ float buffer[SOFTMAXTREE_THREADS+1];
  __shared__ float linearOutput[SOFTMAXTREE_MAXCHILDREN];
  int tx = threadIdx.x;
  int i_step = blockDim.x;
  int k = blockIdx.x;
  float *input_k = input + k*nInput;
  float *nodeOutput, *nodeWeight, *nodeBias;
  float narrowsum = 0;
  int childId = target[k] - 1;
  int parentId, parentIdx, childIdx, nChildren;
  float *node;
  int n = 0;

  // loop through nodes
  while(1)
  {
    /* get next Node in Tree */
    node = childParent + childId*2;
    parentId = (int)node[0] - 1;
    childIdx = (int)node[1] - 1;
    
    node = parentChildren + parentId*2;
    parentIdx = (int)node[0] - 1;
    nChildren = (int)node[1];
    
    LwdaAssert(childIdx < nChildren)
    /* Linear */
    
    nodeWeight = weight + parentIdx*nInput;
    nodeBias = bias + parentIdx;
    
    // addmv (dot products)
    for (int j=0; j<nChildren; j++)
    {
      // zero buffer
      buffer[tx] = 0;
      
      // multiply
      for (int i=tx; i<nInput; i+=i_step)
      {
        buffer[tx] += input_k[i]*nodeWeight[j*nInput + i];
      }
      // add (reduce)
      for (unsigned int stride = blockDim.x >> 1; stride > 0; stride >>= 1)
      {
        __syncthreads();
        if (tx < stride)
          buffer[tx] += buffer[tx+stride];
      }
      
      if (tx == 0) 
      {
        linearOutput[j] = buffer[0] + nodeBias[j];
      }
    }
    
    __syncthreads();
    
    /* LogSoftMax */
    nodeOutput = logsoftOutput + maxFamilyPath*k + n;
    
    // max?
    buffer[tx] = -FLT_MAX;
    for (int i=tx; i<nChildren; i+=i_step)
    {
      float z = linearOutput[i];
      if(buffer[tx] < z)
        buffer[tx] = z;
    }
    
    for (unsigned int stride = blockDim.x >> 1; stride > 0; stride >>= 1)
    {
      __syncthreads();
      if ((tx < stride) && (buffer[tx] < buffer[tx+stride]))
        buffer[tx] = buffer[tx+stride];
    }
    if (tx == 0)
    {
      float max_k = -FLT_MAX;
      if(max_k < buffer[0])
        max_k = buffer[0];
      buffer[SOFTMAXTREE_THREADS] = max_k;
    }

    __syncthreads();
    
    // logadd?
    float max_k = buffer[SOFTMAXTREE_THREADS];
    buffer[tx] = 0;
    for (int i=tx; i<nChildren; i+=i_step)
    {
      buffer[tx] += expf(linearOutput[i]-max_k);
    }

    // reduce
    for (unsigned int stride = blockDim.x >> 1; stride > 0; stride >>= 1)
    {
      __syncthreads();
      if (tx < stride)
        buffer[tx] += buffer[tx+stride];
    }
    if (tx == 0)
    {
      float m = max_k + logf(buffer[0]);
      buffer[SOFTMAXTREE_THREADS] = m;
    }

    __syncthreads();

    // logsoftmax
    float logsum_k = buffer[SOFTMAXTREE_THREADS];
    for (int i=tx; i<nChildren; i+=i_step)
    {
      nodeOutput[i] = linearOutput[i] - logsum_k;
    }
      
    __syncthreads();
    
    /* Narrow + CAddTable (without log, would have been CMulTable) */
    if (tx == 0)
      narrowsum += nodeOutput[childIdx];
      
    n += nChildren;
    LwdaAssert((n <= maxFamilyPath))
    /* Break when root is reached */
    if (parentId == rootId) 
    {
      break;
    }
    childId = parentId;
  }
  if (tx == 0) 
  {
    output[k] = narrowsum;
  }
}


static int lwnnx_SoftMaxTree_updateOutput(lua_State *L)
{ 
  THCState *state = getLwtorchState(L);
  THLwdaTensor *input = (THLwdaTensor*)luaT_checkudata(L, 2, "torch.LwdaTensor");  
  THLwdaTensor *target = (THLwdaTensor*)luaT_checkudata(L, 3, "torch.LwdaTensor");  
  int inputSize = luaT_getfieldcheckint(L, 1, "inputSize");
  int rootId = luaT_getfieldcheckint(L, 1, "rootId") - 1;
  int maxFamilyPath = (int)luaT_getfieldcheckint(L, 1, "maxFamilyPath");
  int maxFamily = (int)luaT_getfieldcheckint(L, 1, "maxFamily");
  
  THLwdaTensor *childParent = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "childParentLwda", "torch.LwdaTensor");
  THLwdaTensor *parentChildren = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "parentChildrenLwda", "torch.LwdaTensor");

  THLwdaTensor *logsoftOutput = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "_multiBuffer", "torch.LwdaTensor");
  
  THLwdaTensor *weight = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "weight", "torch.LwdaTensor");
  THLwdaTensor *bias = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "bias", "torch.LwdaTensor");
  THLwdaTensor *output = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "output", "torch.LwdaTensor");
  
  luaL_argcheck(L, input->nDimension == 2, 2, "2D(batch mode) tensor expected");
  luaL_argcheck(L, input->size[1] == inputSize, 2, "invalid input size");  
  luaL_argcheck(L, maxFamily <= SOFTMAXTREE_MAXCHILDREN, 2, "Hierarchy has node(s) with too many children");
  
  input = THLwdaTensor_newContiguous(state, input);
  THLwdaTensor_resize1d(state, output, input->size[0]);
  
  /* call lwdakernel */
  dim3 blocks(input->size[0]); // each block is an example
  dim3 threads(SOFTMAXTREE_THREADS);
  lwnnx_SoftMaxTree_updateOutput_kernel<<<blocks,threads>>>(
    THLwdaTensor_data(state, output), THLwdaTensor_data(state, logsoftOutput), 
    THLwdaTensor_data(state, input), THLwdaTensor_data(state, weight), 
    THLwdaTensor_data(state, bias), THLwdaTensor_data(state, target), 
    THLwdaTensor_data(state, childParent), THLwdaTensor_data(state, parentChildren), 
    input->size[1], rootId, maxFamilyPath
  );
  
  lwdaError errcode = lwdaGetLastError();
  if(errcode != lwdaSuccess)
    THError(lwdaGetErrorString(errcode));
  
  THLwdaTensor_free(state, input);
  return 1;
}


__global__ void lwnnx_SoftMaxTree_updateGradInput_kernel(
  float *gradInput, float *logsoftOutput, float *gradOutput, float* weight,
  float *target, float *childParent, float *parentChildren, 
  int nInput, int rootId, int maxFamilyPath)
{
  __shared__ float buffer[SOFTMAXTREE_THREADS];
  int tx = threadIdx.x;
  int i_step = blockDim.x;
  int k = blockIdx.x;
  float *gradInput_k = gradInput + k*nInput;
  float *nodeGrad, *nodeWeight;
  float grad = gradOutput[k];
  int childId = target[k] - 1;
  int parentId, parentIdx, childIdx, nChildren;
  float *node;
  int n = 0;
  
  // zero gradInputs (for aclwmulation)
  for (int i=tx; i<nInput; i+=i_step)
    gradInput_k[i] = 0;

  // loop through nodes
  while(1)
  {
    /* get next Node in Tree */
    node = childParent + childId*2;
    parentId = (int)node[0] - 1;
    childIdx = (int)node[1] - 1;
    
    node = parentChildren + parentId*2;
    parentIdx = (int)node[0] - 1;
    nChildren = (int)node[1];
    
    /* CAddTable + Narrow + LogSoftMax */
    // AKA linearGradOutput (we reuse the _multiBuffer Tensor)
    nodeGrad = logsoftOutput + maxFamilyPath*k + n; 

    for(int i=tx; i<nChildren; i+=i_step)
    {
      nodeGrad[i] = -expf(nodeGrad[i])*grad;
    }
    
    __syncthreads();
    if (tx == 0)
    {
      nodeGrad[childIdx] += grad;
    }
      
    __syncthreads();

    /* Linear */
    nodeWeight = weight + parentIdx*nInput;
    
    // addmv (dot products)
    for (int i=tx; i<nInput; i+=i_step)
    {
     // zero buffer
      buffer[tx] = 0;
      
      for (int j=0; j<nChildren; j++)
      {
        // multiply
        buffer[tx] += nodeGrad[j]*nodeWeight[j*nInput + i];
      }
      // accumulate into global memory
      gradInput_k[i] += buffer[tx];
    }
    
    n += nChildren;
    LwdaAssert((n <= maxFamilyPath))
    /* Break when root is reached */
    if (parentId == rootId)
    {
      break;
    }
    childId = parentId;
  }
}

static int lwnnx_SoftMaxTree_updateGradInput(lua_State *L)
{
  THCState *state = getLwtorchState(L);
  THLwdaTensor *input = (THLwdaTensor*)luaT_checkudata(L, 2, "torch.LwdaTensor");  
  THLwdaTensor *gradOutput = (THLwdaTensor*)luaT_checkudata(L, 3, "torch.LwdaTensor");  
  THLwdaTensor *target = (THLwdaTensor*)luaT_checkudata(L, 4, "torch.LwdaTensor");  
  int inputSize = luaT_getfieldcheckint(L, 1, "inputSize");
  int rootId = luaT_getfieldcheckint(L, 1, "rootId") - 1;
  int maxFamilyPath = (int)luaT_getfieldcheckint(L, 1, "maxFamilyPath");
  
  THLwdaTensor *childParent = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "childParentLwda", "torch.LwdaTensor");
  THLwdaTensor *parentChildren = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "parentChildrenLwda", "torch.LwdaTensor");
  
  THLwdaTensor *logsoftOutput = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "_multiBuffer", "torch.LwdaTensor");
  
  THLwdaTensor *weight = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "weight", "torch.LwdaTensor");
  THLwdaTensor *bias = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "bias", "torch.LwdaTensor");
  THLwdaTensor *gradInput = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "_gradInput", "torch.LwdaTensor");
  
  luaL_argcheck(L, input->nDimension == 2, 2, "2D(batch mode) tensor expected");
  luaL_argcheck(L, input->size[1] == inputSize, 2, "invalid input size");  
  
  luaL_argcheck(L, gradOutput->nDimension == 1, 2, "1D tensor expected");
  
  THLwdaTensor_resizeAs(state, gradInput, input);
  
  /* call lwdakernel */
  dim3 blocks(input->size[0]); // each block is an example
  dim3 threads(SOFTMAXTREE_THREADS);
  lwnnx_SoftMaxTree_updateGradInput_kernel<<<blocks,threads>>>(
    THLwdaTensor_data(state, gradInput), THLwdaTensor_data(state, logsoftOutput), 
    THLwdaTensor_data(state, gradOutput), THLwdaTensor_data(state, weight), 
    THLwdaTensor_data(state, target), THLwdaTensor_data(state, childParent), 
    THLwdaTensor_data(state, parentChildren), 
    input->size[1], rootId, maxFamilyPath
  );
  
  lwdaError errcode = lwdaGetLastError();
  if(errcode != lwdaSuccess)
    THError(lwdaGetErrorString(errcode));
  
  return 1;
}

__global__ void lwnnx_SoftMaxTree_accGradParameters_kernel(
  float *gradWeight, float *gradBias, float *input, 
  float *linearGradOutput, int *nodeUpdateLwda, float *target, 
  float *childParent, float *parentChildren, 
  int nInput, int rootId, int maxFamilyPath, int maxDept, float scale)
{
  __shared__ float buffer[SOFTMAXTREE_THREADS];
  int tx = threadIdx.x;
  int i_step = blockDim.x;
  int k = blockIdx.x;
  float *input_k = input + k*nInput;
  float *nodeGradOutput, *nodeGradWeight, *nodeGradBias;
  // reuse _multiBuffer for keeping track of which node gets gradients
  int *nodeUpdate = nodeUpdateLwda + maxDept*k; 
  int childId = target[k] - 1;
  int parentId, parentIdx, nChildren;
  float *node;
  int n = 0;
  int m = 0;
  
  // loop through nodes
  while(1)
  {
    /* get next Node in Tree */
    node = childParent + childId*2;
    parentId = (int)node[0] - 1;
    
    node = parentChildren + parentId*2;
    parentIdx = (int)node[0] - 1;
    nChildren = (int)node[1];
    
    nodeGradOutput = linearGradOutput + maxFamilyPath*k + n; 
    nodeGradWeight = gradWeight + parentIdx*nInput;
    nodeGradBias = gradBias + parentIdx;
    
    // addr weights (scalar-products)
    for (int i=tx; i<nInput; i+=i_step)
    {
      // copy input to buffer
      buffer[tx] = input_k[i]; // replace shared with register?
    
      for (int j=0; j<nChildren; j++)
      {
        // multiply accumulate weights
        float dw = scale*nodeGradOutput[j]*buffer[tx];
        atomicAdd(&nodeGradWeight[j*nInput + i], dw);
      }
    }
    
    // cadd bias
    for (int j=tx; j<nChildren; j+=i_step)
    {
      // multiply accumulate biases
      float db = scale*nodeGradOutput[j];
      atomicAdd(&nodeGradBias[j], db);
    }
    
    // keep track of which node gets gradients
    nodeUpdate[m] = (int)parentId;
    
    n += nChildren;
    LwdaAssert((n <= maxFamilyPath))
    m += 1;
    LwdaAssert((m <= maxDept))
    /* Break when root is reached */
    if (parentId == rootId)
    {
      if (m < maxDept)
        nodeUpdate[m] = -1; // zero means end of buffer
      break;
    }
    childId = parentId;
  }
}

static int lwnnx_SoftMaxTree_accGradParameters(lua_State *L)
{
  THCState *state = getLwtorchState(L);
  THLwdaTensor *input = (THLwdaTensor*)luaT_checkudata(L, 2, "torch.LwdaTensor");  
  THLwdaTensor *target = (THLwdaTensor*)luaT_checkudata(L, 4, "torch.LwdaTensor");  
  float scale = luaL_optnumber(L, 5, 1);
  int inputSize = luaT_getfieldcheckint(L, 1, "inputSize");
  int rootId = luaT_getfieldcheckint(L, 1, "rootId") - 1;
  int maxFamilyPath = (int)luaT_getfieldcheckint(L, 1, "maxFamilyPath");
  int maxDept = (int)luaT_getfieldcheckint(L, 1, "maxDept");
  
  THLwdaTensor *childParent = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "childParentLwda", "torch.LwdaTensor");
  THLwdaTensor *parentChildren = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "parentChildrenLwda", "torch.LwdaTensor");
  
  THLwdaTensor *linearGradOutput = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "_multiBuffer", "torch.LwdaTensor");
  THLwdaIntTensor *nodeUpdateLwda = (THLwdaIntTensor*)luaT_getfieldcheckudata(L, 1, "_nodeUpdateLwda", "torch.LwdaIntTensor");
  THIntTensor *nodeUpdateHost = (THIntTensor*)luaT_getfieldcheckudata(L, 1, "_nodeUpdateHost", "torch.IntTensor");
  
  THLwdaTensor *gradWeight = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "gradWeight", "torch.LwdaTensor");
  THLwdaTensor *gradBias = (THLwdaTensor*)luaT_getfieldcheckudata(L, 1, "gradBias", "torch.LwdaTensor");
  
  int i, j;
  THIntTensor *nodeUpdate;
  
  lua_getfield(L, 1, "updates");
  
  luaL_argcheck(L, input->nDimension == 2, 2, "2D(batch mode) tensor expected");
  luaL_argcheck(L, input->size[1] == inputSize, 2, "invalid input size"); 
  
  input = THLwdaTensor_newContiguous(state, input); 
  
  /* call lwdakernel */
  dim3 blocks(input->size[0]); // each block is an example
  dim3 threads(SOFTMAXTREE_THREADS);
  lwnnx_SoftMaxTree_accGradParameters_kernel<<<blocks,threads>>>(
    THLwdaTensor_data(state, gradWeight), THLwdaTensor_data(state, gradBias), 
    THLwdaTensor_data(state, input), THLwdaTensor_data(state, linearGradOutput), 
    THLwdaIntTensor_data(state, nodeUpdateLwda), THLwdaTensor_data(state, target), 
    THLwdaTensor_data(state, childParent), THLwdaTensor_data(state, parentChildren), 
    input->size[1], rootId, maxFamilyPath, maxDept, scale 
  );
  
  lwdaError errcode = lwdaGetLastError();
  if(errcode != lwdaSuccess)
    THError(lwdaGetErrorString(errcode));
  
  // copy updated nodeIds from device to host
  THIntTensor_copyLwda(state, nodeUpdateHost, nodeUpdateLwda);
  nodeUpdate = THIntTensor_new();
  
  // fill updates table
  for (i=0; i<nodeUpdateHost->size[0]; i++)
  {
    THIntTensor_select(nodeUpdate, nodeUpdateHost, 0, i);
    
    for (j=0; j<nodeUpdateHost->size[1]; j++)
    {
      int nodeId = THIntTensor_get1d(nodeUpdate, j);
      double count;
      
      if (nodeId == -1)
      {
        break;
      }
      
      /* updates will contain nodeId (key) sum of scales (value)*/
      lua_pushinteger(L, (int)(nodeId+1));
      lua_gettable(L, -2);
      count = lua_tonumber(L, -1) + scale;
      lua_pop(L, 1);
      
      lua_pushinteger(L, (int)(nodeId+1)); /* key */
      lua_pushnumber(L, count); /* value */
      lua_settable(L, -3);
    }
  }
  
  THIntTensor_free(nodeUpdate);
  return 0;
}

static const struct luaL_Reg lwnnx_SoftMaxTree__ [] = {
  {"SoftMaxTree_updateOutput", lwnnx_SoftMaxTree_updateOutput},
  {"SoftMaxTree_updateGradInput", lwnnx_SoftMaxTree_updateGradInput},
  {"SoftMaxTree_accGradParameters", lwnnx_SoftMaxTree_accGradParameters},
  {NULL, NULL}
};

static void lwnnx_SoftMaxTree_init(lua_State *L)
{
  luaT_pushmetatable(L, "torch.LwdaTensor");
  luaT_registeratname(L, lwnnx_SoftMaxTree__, "nn");
  lua_pop(L,1);
}
