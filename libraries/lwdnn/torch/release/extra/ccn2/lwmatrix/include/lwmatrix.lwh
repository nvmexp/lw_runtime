/*
 * Copyright 2014 Google Inc. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#ifndef LWMATRIX_H_
#define LWMATRIX_H_

#include <map>
#include <vector>
#include <lwblas_v2.h>
#include <lwca.h>
#include <lwrand.h>
#include <time.h>
#include <lwrand_kernel.h>

#include <helper_lwda.h>
#include "../../util/include/matrix.h"
#include "lwmatrix_kernels.lwh"
#include "lwmatrix_operators.lwh"
#include "memory.lwh"

#ifdef WARNINGS
#define WARN(msg) printf("WARN: File %s, line %d: %s\n", __FILE__, __LINE__, msg);
#else
#define WARN(msg) ;
#endif

#define LWRAND_CALL(x) do { if((x) != LWRAND_STATUS_SUCCESS) { \
                            printf("LWRAND Error at %s:%d\n",__FILE__,__LINE__);\
                            exit(EXIT_FAILURE);}} while(0)

#define LWBLAS_CALL(x) do { if((x) != LWBLAS_STATUS_SUCCESS) { \
                            printf("LWBLAS Error at %s:%d\n",__FILE__,__LINE__);\
                            exit(EXIT_FAILURE);}} while(0)

/*
 * Memory manager to use for GPU memory allocations.
 *
 * LWDAMemoryManager: Default Lwpu memory manager; just calls lwdaMalloc / lwdaFree.
 *                    Allocating and freeing memory is slow.
 * FastMemoryManager: A GPU memory manager with very fast (constant time)
 *                    alloc / free, but possibly more wasteful of memory.
 */
#define DEVICE_MEMORY_MANAGER       LWDAMemoryManager

/*
 * Memory manager to use for host memory allocations.
 *
 * LWDAHostMemoryManager: Default Lwpu memory manager; just calls lwdaHostAlloc / lwdaFreeHost.
 *                        Allocating and freeing memory is slow.
 * FastHostMemoryManager: A host memory manager with very fast (constant time)
 *                        alloc / free, but possibly more wasteful of memory.
 */
#define HOST_MEMORY_MANAGER         LWDAHostMemoryManager

class LWMatrix;
typedef std::vector<LWMatrix*> LWMatrixV;

class LWMatrix {
protected:
    int _numCols, _numRows;
    int _numElements;
    int _stride;
//    float* getDevData();
    MemorySegment* _memSegment;
    bool _isTrans;
    bool _ownsData;
    // This flag makes sure that the LWMatrix destructor does nothing
    // when called on HostLWMatrix instance.
    bool _deleted;
    lwdaTextureObject_t _texObj;

//    static std::map<int,lwrandGenerator_t> rndGen;
    static std::map<int,MemorySegment*> _rndDevStates;
    static std::map<int,lwblasHandle_t> _lwblasHandles;
    // Map from device id --> # of random streams initialized on that device
    static std::map<int,int> _rndDevThreads;
    static pthread_mutex_t *_rndMutex, *_lwblasMutex, *_streamMutex;
    // Map from device id --> default stream
    static std::map<int,lwdaStream_t> _defaultStreams;

    lwblasOperation_t getTransChar() const {
        /*
         * not a typo! return opposite character because a
         * non-transposed lwmatrix is in row-major order while a non-transposed
         * lwblas matrix is in column-major order.
         */
        return _isTrans ? LWBLAS_OP_N : LWBLAS_OP_T;
    }
    
    void _init(bool isTrans);
    void _sum_setParams(int n, dim3* blocks, dim3* threads);
    template<class Agg> float cpuAgg(Agg agg, lwdaStream_t stream);
    template<class Agg> float _totalAgg(Agg agg);
    template<class Agg> float _totalAgg(Agg agg, lwdaStream_t stream);
    template<class Agg> float _totalAgg(Agg agg, LWMatrix& tmpbuf, lwdaStream_t stream);
    template<class Agg, class UnaryOp, class BinaryOp> void _aggregate(int axis, LWMatrix& target, Agg agg, UnaryOp uop, BinaryOp bop, lwdaStream_t stream);
    template<class Agg, class UnaryOp, class BinaryOp> void _aggregate(int axis, LWMatrix& target, Agg agg, UnaryOp uop, BinaryOp bop);
    template<class Agg, class BinaryOp> void _aggregate(int axis, LWMatrix& target, Agg agg, BinaryOp bop, lwdaStream_t stream);
    template<class Agg, class BinaryOp> void _aggregate(int axis, LWMatrix& target, Agg agg, BinaryOp bop);
    template<class Agg, class BinaryOp> LWMatrix& _aggregate(int axis, Agg agg, BinaryOp bop, lwdaStream_t stream);
    template<class Agg, class BinaryOp> LWMatrix& _aggregate(int axis, Agg agg, BinaryOp bop);
    template<class Agg, class UnaryOp, class BinaryOp> LWMatrix& _aggregate(int axis, Agg agg, UnaryOp, BinaryOp bop, lwdaStream_t stream);
    template<class Agg, class UnaryOp, class BinaryOp> LWMatrix& _aggregate(int axis, Agg agg, UnaryOp, BinaryOp bop);
    template <class Randomizer> void _unaryRandomize(LWMatrix& target, Randomizer rnd, lwdaStream_t stream);
    template <class Randomizer> void _unaryRandomize(LWMatrix& target, Randomizer rnd);
    template <class Randomizer> void _binaryRandomize(LWMatrix& data2, LWMatrix& target, Randomizer rnd);
    template <class Randomizer> void _binaryRandomize(LWMatrix& data2, LWMatrix& target, Randomizer rnd, lwdaStream_t stream);

    virtual void alloc(int numElements);
    virtual void dealloc();
    void deallocTexture();
    virtual LWMatrix& construct() const;
    virtual LWMatrix& construct(bool isTrans) const;
    virtual LWMatrix& construct(int numRows, int numCols, bool isTrans=false) const;
    virtual LWMatrix& construct(const Matrix& like, bool copy) const;
    virtual LWMatrix& construct(const LWMatrix& like, bool copy) const;
    virtual LWMatrix& construct(const LWMatrix& like) const;
    virtual LWMatrix& construct(const Matrix& like) const;
    virtual LWMatrix& construct(MemorySegment* mem, int numRows, int numCols, int stride, bool isTrans) const;
    static lwblasHandle_t getLwblasHandle();
    static lwblasHandle_t getLwblasHandle(int deviceID);
public:
    LWMatrix();
    LWMatrix(bool isTrans);
    LWMatrix(int numRows, int numCols, bool isTrans=false);
    LWMatrix(const Matrix& like, bool copy);
    LWMatrix(const LWMatrix& like, bool copy);
    LWMatrix(const LWMatrix& like);
    LWMatrix(const Matrix& like);
    LWMatrix(MemorySegment* mem, int numRows, int numCols, int stride, bool isTrans);
    virtual ~LWMatrix();

    // Returns the device ID on which the data pointer is allocated
    int getDataDeviceID() const;
    static void initRandom(unsigned long long seed, int numStreams, lwdaStream_t stream);
    static void initRandom(unsigned long long seed, int numStreams);
    static void initRandom(unsigned long long seed);
    static void initRandom();
    static void initLwblas();
    static void destroyLwblas();

    // Returns the lwrrently-active device ID for calling thread
    static int getDeviceID();
    static void setDeviceID(int d);
    static bool canAccessPeer(int srcDevice, int tgtDevice);
    static bool isRndInitialized();
    static bool isRndInitialized(bool haveLock);
    static lwrandState* getLwrandState();
    static lwrandState* getLwrandState(int numStreams);
    static void destroyRandom();
    static pthread_mutex_t* makeMutex();
    static lwdaStream_t getDefaultStream(int deviceID);
    static lwdaStream_t getDefaultStream();
    static void syncDevice();
    static void syncStream();
    static void syncStream(lwdaStream_t stream);

    /*
     * DO NOT DEREFERENCE IN HOST CODE! This is a device memory pointer.
     */
    float* getCellPtr(int i, int j) const {
        if (_isTrans) {
            return &getDevData()[j * _numRows + i];
        }
        return &getDevData()[i * _numCols + j];
    }

    bool isSameDims(const Matrix& m) const {
        return m.getNumRows() == _numRows && m.getNumCols() == _numCols;
    }

    bool isSameDims(const LWMatrix& m) const {
        return m.getNumRows() == _numRows && m.getNumCols() == _numCols;
    }

    int getNumRows() const {
        return _numRows;
    }

    int getNumCols() const {
        return _numCols;
    }

    int getStride() const {
        return _stride;
    }

    int getLeadingDim() const {
        return _isTrans ? _numRows : _numCols;
    }

    int getFollowingDim() const {
        return !_isTrans ? _numRows : _numCols;
    }

    /*
     * FALSE:    Row-major order.
     * TRUE:     Column-major order.
     */
    bool isTrans() const {
        return _isTrans;
    }

    bool isView() const {
        return !_ownsData;
    }

    float* getDevData() const {
        return _memSegment == NULL ? NULL : _memSegment->getData<float>();
    }
    
    MemorySegment& getMemorySegment() const {
        return *_memSegment;
    }

    int getNumElements() const {
        return _numElements;
    }
    
    size_t getNumDataBytes() const {
        return size_t(_numElements) * 4;
    }

    /*
     * Only use if you know what you're doing!
     * Does not actually transpose matrix.
     */
    void setTrans(bool trans) {
        if (trans != _isTrans) {
            assert(isContiguous());
            _isTrans = trans;
            _stride = getLeadingDim();
        }
    }
    
    /*
     * Only use if you know what you're doing!
     * This toggles whether this object will free its GPU memory when it's destroyed.
     */
    void setIsView(bool isView) {
        _ownsData = !isView;
    }

    bool isContiguous() const {
        return _stride == getLeadingDim() || getFollowingDim() == 1;
    }
    
    void truncate() {
        resize(0,0);
    }

    virtual lwdaTextureObject_t getTextureObject();
   
    virtual void copyFromHost(const Matrix& hostMatrix);
    virtual void copyFromHost(const Matrix& hostMatrix, bool resizeTarget);
    virtual void copyFromHost(const Matrix& hostMatrix, bool resizeTarget, lwdaStream_t stream);
    virtual void copyToHost(Matrix& hostMatrix) const;
    virtual void copyToHost(Matrix& hostMatrix, bool resizeTarget) const;
    virtual void copyToHost(Matrix& hostMatrix, bool resizeTarget, lwdaStream_t stream) const;
    void copy(LWMatrix& dest) const;
    void copy(LWMatrix& dest, lwdaStream_t stream) const;
    LWMatrix& copy() const;
    void addProduct(LWMatrix& a, LWMatrix &b, float scaleThis, float scaleAB, lwdaStream_t stream);
    void addProduct(LWMatrix& a, LWMatrix &b, float scaleThis, float scaleAB);
    void addProduct(LWMatrix& a, LWMatrix &b);
    void rightMult(LWMatrix &b, float scaleAB, LWMatrix &target, lwdaStream_t stream);
    void rightMult(LWMatrix &b, float scaleAB, LWMatrix &target);
    void rightMult(LWMatrix &b, LWMatrix &target);
    void rightMult(LWMatrix &b, float scaleAB);
    void randomizeUniform();
    void addGaussianNoise(LWMatrix& stdevs, bool var, LWMatrix& target);
    void addGaussianNoise(float stdev, LWMatrix& target);
    void addGaussianNoise(LWMatrix& stdevs, bool var);
    void addGaussianNoise(LWMatrix& stdevs);
    void addGaussianNoise(float stdev);
    void addGaussianNoise();
    void randomizeGaussian();
    void randomizeGaussian(float stdev);
    void randomizeGaussian(float mean, float stdev);
    void randomizeGaussian(float mean, LWMatrix& stdevs);
    void randomizeGaussian(float mean, float stdevMult, LWMatrix& stdevs);
    void randomizeGaussian(LWMatrix& stdevs);
    void randomizeGaussian(LWMatrix& stdevs, LWMatrix& target);
    void binarizeProbs();
    void binarizeProbs(LWMatrix& target);

    void biggerThan(LWMatrix& m, LWMatrix& target);
    void biggerThan(LWMatrix& m);
    void biggerThalwector(LWMatrix& vec, LWMatrix& target);
    void biggerThalwector(LWMatrix& vec);
    void equals(LWMatrix& m, LWMatrix& target);
    void equals(LWMatrix& m);

    void _checkBounds(int startRow, int endRow, int startCol, int endCol) const;
    LWMatrix& slice(int startRow, int endRow, int startCol, int endCol) const;
    void slice(int startRow, int endRow, int startCol, int endCol, LWMatrix& target) const;
    LWMatrix& sliceRows(int startRow, int endRow) const;
    void sliceRows(int startRow, int endRow, LWMatrix& target) const;
    LWMatrix& sliceCols(int startCol, int endCol) const;
    void sliceCols(int startCol, int endCol, LWMatrix& target) const;

    LWMatrixV& splitRows(int numParts);
    LWMatrixV& splitCols(int numParts);

    template <class Op> void apply(Op op, LWMatrix& target, lwdaStream_t stream) {
        if (!target.isSameDims(*this)) {
            target.resize(*this);
        }
        if (getNumElements() > 0) {
            int height = target.getFollowingDim(), width = target.getLeadingDim();

            if (target.isTrans() == isTrans()) {
                if (!isContiguous() || !target.isContiguous()) {
                    dim3 blocks(std::min(NUM_BLOCKS_MAX, DIVUP(width, ELTWISE_THREADS_X)),
                            std::min(NUM_BLOCKS_MAX, DIVUP(height, ELTWISE_THREADS_Y)));
                    dim3 threads(ELTWISE_THREADS_X, ELTWISE_THREADS_Y);
                    kEltwiseUnaryOp<Op><<<blocks, threads, 0, stream>>>(getDevData(), target.getDevData(), height, width, getStride(), target.getStride(), op);
                    getLastLwdaError("kEltwiseUnaryOp: Kernel exelwtion failed");
                } else {
                    dim3 threads = dim3(ELTWISE_FLAT_THREADS_X);
                    dim3 blocks = dim3(std::min(128, DIVUP(_numElements, ELTWISE_FLAT_THREADS_X)));
                    kEltwiseUnaryOpFlat<Op><<<blocks, threads, 0, stream>>>(getDevData(), target.getDevData(), _numElements, op);
                    getLastLwdaError("kEltwiseUnaryOpFlat: Kernel exelwtion failed");
                }
            } else {
                dim3 blocks(std::min(NUM_BLOCKS_MAX, DIVUP(width, ELTWISE_THREADS_X)),
                        std::min(NUM_BLOCKS_MAX, DIVUP(height, ELTWISE_THREADS_Y)));
                dim3 threads(ELTWISE_THREADS_X, ELTWISE_THREADS_Y);
                bool checkBounds = !(width % ELTWISE_THREADS_X == 0 && height % ELTWISE_THREADS_X == 0);
    //            printf("height: %d, width: %d, stride: %d, target stride: %d, check bounds: %d, threads.x: %d, threads.y: %d, blocks.x: %d, blocks.y: %d\n",
    //                    height, width, getStride(), target.getStride(), checkBounds, threads.x, threads.y, blocks.x, blocks.y);
                if (checkBounds) {
                    kEltwiseUnaryOpTrans<Op, true><<<blocks, threads, 0, stream>>>(getDevData(), target.getDevData(), height, width, getStride(), target.getStride(), op);
                } else {
                    kEltwiseUnaryOpTrans<Op, false><<<blocks, threads, 0, stream>>>(getDevData(), target.getDevData(), height, width, getStride(), target.getStride(), op);
                }
                getLastLwdaError("kEltwiseUnaryOpTrans: Kernel exelwtion failed");
            }
        }
    }
    
    template <class Op> void apply(Op op, lwdaStream_t stream) {
        apply(op, *this, stream);
    }

    template <class Op> void apply(Op op, LWMatrix& target) {
        apply(op, target, getDefaultStream());
    }

    template <class Op> void apply(Op op) {
        apply(op, *this);
    }
    
    template <class Op> void applyBinary(Op op, LWMatrix& b) {
        applyBinary(op, b, *this);
    }

    template <class Op> void applyBinary(Op op, LWMatrix& b, LWMatrix& target) {
        applyBinary(op, b, target, getDefaultStream());
    }

    template <class Op> void applyBinary(Op op, LWMatrix& b, LWMatrix& target, lwdaStream_t stream) {
        assert(this->isSameDims(b));

        if (!target.isSameDims(*this)) {
            target.resize(*this);
        }

        if (getNumElements() > 0) {
            int height = target.getFollowingDim(), width = target.getLeadingDim();
            if (target.isTrans() == isTrans() && target.isTrans() == b.isTrans()) {
                if (!isContiguous() || !b.isContiguous() || !target.isContiguous()) {
                    dim3 blocks(std::min(128, DIVUP(width, ELTWISE_THREADS_X)),
                                std::min(128, DIVUP(height, ELTWISE_THREADS_Y)));
                    dim3 threads(ELTWISE_THREADS_X, ELTWISE_THREADS_Y);
                    kEltwiseBinaryOp<Op><<<blocks, threads, 0, stream>>>(getDevData(), b.getDevData(), target.getDevData(), height, width, getStride(),
                                                              b.getStride(), target.getStride(), op);
                } else {
                    dim3 threads = dim3(ELTWISE_FLAT_THREADS_X);
                    dim3 blocks = dim3(std::min(128, DIVUP(_numElements, ELTWISE_FLAT_THREADS_X)));
                    kEltwiseBinaryOpFlat<Op><<<blocks, threads, 0, stream>>>(getDevData(), b.getDevData(), target.getDevData(), _numElements, op);
                }
                getLastLwdaError("kEltwiseBinaryOp: Kernel exelwtion failed");
            } else {

                dim3 blocks(std::min(128, DIVUP(width, ELTWISE_THREADS_X)),
                            std::min(128, DIVUP(height, ELTWISE_THREADS_Y)));
                dim3 threads(ELTWISE_THREADS_X, ELTWISE_THREADS_Y);
                //  both x here since y divides x
                bool checkBounds = !(width % ELTWISE_THREADS_X == 0 && height % ELTWISE_THREADS_X == 0);
                if (target.isTrans() == isTrans() && target.isTrans() != b.isTrans()) {
                    if (checkBounds) {
                        kEltwiseBinaryOpTrans<Op,true,false,false><<<blocks, threads, 0, stream>>>(getDevData(), b.getDevData(), target.getDevData(), height, width,getStride(),
                                                                   b.getStride(), target.getStride(), op);
                    } else {
                        kEltwiseBinaryOpTrans<Op,false,false,false><<<blocks, threads, 0, stream>>>(getDevData(), b.getDevData(), target.getDevData(), height, width,getStride(),
                                                                   b.getStride(), target.getStride(), op);
                    }
                } else if (target.isTrans() != isTrans() && target.isTrans() != b.isTrans()) {
                    if (checkBounds) {
                        kEltwiseBinaryOpTrans<Op,true,true,false><<<blocks, threads, 0, stream>>>(getDevData(), b.getDevData(), target.getDevData(), height, width,getStride(),
                                                                   b.getStride(), target.getStride(), op);
                    } else {
                        kEltwiseBinaryOpTrans<Op,false,true,false><<<blocks, threads, 0, stream>>>(getDevData(), b.getDevData(), target.getDevData(), height, width,getStride(),
                                                                   b.getStride(), target.getStride(), op);
                    }
                } else if (target.isTrans() != isTrans() && target.isTrans() == b.isTrans()) {
                    if (checkBounds) {
                        kEltwiseBinaryOpTrans<Op,true,false,true><<<blocks, threads, 0, stream>>>(b.getDevData(), getDevData(), target.getDevData(), height, width,b.getStride(),
                                                                   getStride(), target.getStride(), op);
                    } else {
                        kEltwiseBinaryOpTrans<Op,false,false,true><<<blocks, threads, 0, stream>>>(b.getDevData(), getDevData(), target.getDevData(), height, width, b.getStride(),
                                                                   getStride(), target.getStride(), op);
                    }
                }
                getLastLwdaError("kEltwiseBinaryOpTrans: Kernel exelwtion failed");
            }
        }
    }
    
    template <class Op> void applyTernary(Op op, LWMatrix& b, LWMatrix& c, LWMatrix& target) {
        applyTernary(op, b, c, target, getDefaultStream());
    }

    template <class Op> void applyTernary(Op op, LWMatrix& b, LWMatrix& c, LWMatrix& target, lwdaStream_t stream) {
        assert(isSameDims(b));
        assert(isSameDims(c));
        // For now ternary ops are only supported for matrices of same transposedness
        assert(isTrans() == b.isTrans());
        assert(isTrans() == c.isTrans());
        if (!target.isSameDims(*this) || target.isTrans() != isTrans()) {
            target.resize(*this);
        }
        if (getNumElements() > 0) {
            int height = target.getFollowingDim(), width = target.getLeadingDim();
            if (!isContiguous() || !b.isContiguous() || !c.isContiguous() || !target.isContiguous()) {
                dim3 blocks(std::min(128, DIVUP(width, ELTWISE_THREADS_X)),
                            std::min(128, DIVUP(height, ELTWISE_THREADS_Y)));
                dim3 threads(ELTWISE_THREADS_X, ELTWISE_THREADS_Y);
                kEltwiseTernaryOp<Op><<<blocks, threads, 0, stream>>>(getDevData(), b.getDevData(), c.getDevData(), target.getDevData(), height, width,
                                                                       getStride(), b.getStride(), c.getStride(), target.getStride(), op);
                getLastLwdaError("kEltwiseTernaryOp: Kernel exelwtion failed");
            } else {
                dim3 threads = dim3(ELTWISE_FLAT_THREADS_X);
                dim3 blocks = dim3(std::min(128, DIVUP(_numElements, ELTWISE_FLAT_THREADS_X)));
                kEltwiseTernaryOpFlat<Op><<<blocks, threads, 0, stream>>>(getDevData(), b.getDevData(), c.getDevData(), target.getDevData(), _numElements, op);
                getLastLwdaError("kEltwiseTernaryOpFlat: Kernel exelwtion failed");
            }
        }
    }


    bool resize(int numRows, int numCols, bool trans);
    bool resize(int numRows, int numCols);
    bool resize(const LWMatrix &like);
    bool resize(const Matrix &like);
    void reshape(int numRows, int numCols);
    LWMatrix& reshaped(int numRows, int numCols) const;
    void copy(LWMatrix &dest, int srcStartRow, int srcEndRow, int srcStartCol, int srcEndCol, int destStartRow, int destStartCol) const;
    void copy(LWMatrix &dest, int srcStartRow, int srcEndRow, int srcStartCol, int srcEndCol, int destStartRow, int destStartCol, lwdaStream_t stream) const;
    void add(LWMatrix& b, float scaleA, float scaleB, LWMatrix& target, lwdaStream_t stream);
    void add(LWMatrix& b, float scaleA, float scaleB, LWMatrix& target);
    void add(LWMatrix& b, float scaleB, LWMatrix& target);
    void add(LWMatrix& b, LWMatrix& target);
    void add(LWMatrix& b, float scaleB);
    void add(LWMatrix& b, float scaleA, float scaleB);
    void add(LWMatrix& b);
    void eltwiseMult(LWMatrix& b);
    void eltwiseMult(LWMatrix& b, LWMatrix& target);
    void eltwiseDivide(LWMatrix& b);
    void eltwiseDivide(LWMatrix& b, LWMatrix& target);
    void squaredDiff(LWMatrix& b);
    void squaredDiff(LWMatrix& b, LWMatrix& target);
    void subtract(LWMatrix& b, LWMatrix& target);
    void subtract(LWMatrix& b);
    void addVector(LWMatrix& vec, float scaleVec, LWMatrix& target, lwdaStream_t stream);
    void addVector(LWMatrix& vec, float scaleVec, LWMatrix& target);
    void addVector(LWMatrix& vec);
    void addVector(LWMatrix& vec, float scaleVec);
    void addVector(LWMatrix& vec, LWMatrix& target);
    void equalsVector(LWMatrix& vec, LWMatrix& target);
    void equalsVector(LWMatrix& vec);
    void eltwiseMultByVector(LWMatrix& vec, LWMatrix& target, lwdaStream_t stream);
    void eltwiseMultByVector(LWMatrix& vec, LWMatrix& target);
    void eltwiseMultByVector(LWMatrix& vec);
    void eltwiseMultByVector(LWMatrix& vec, lwdaStream_t stream);
    void eltwiseDivideByVector(LWMatrix& vec, LWMatrix& target);
    void eltwiseDivideByVector(LWMatrix& vec);
    void tile(int timesY, int timesX, LWMatrix& target);
    void tile(int timesY, int timesX, LWMatrix& target, lwdaStream_t stream);

    void addSum(LWMatrix& a, int axis, float scaleThis, float scaleSum);
    void addSum(LWMatrix& a, int axis, float scaleThis, float scaleSum, lwdaStream_t stream);
    void sum(int axis, LWMatrix& target, lwdaStream_t stream);
    void sum(int axis, LWMatrix& target);
    LWMatrix& sum(int axis);
    void max(int axis, LWMatrix& target);
    LWMatrix& max(int axis);
    void min(int axis, LWMatrix& target);
    LWMatrix& min(int axis);
    void sumOfSquares(int axis, LWMatrix& target, lwdaStream_t stream);
    void sumOfSquares(int axis, LWMatrix& target);
    LWMatrix& sumOfSquares(int axis);
    float mean();
    float sum();
    float sum(LWMatrix& tmpbuf);
    float max();
    float min();
    float countInf();
    float countNan();
    float norm2();
    float norm();
    
    void inRangeInc(float lower, float upper);
    void inRangeInc(float lower, float upper, LWMatrix& target);
    void inRangeExc(float lower, float upper);
    void inRangeExc(float lower, float upper, LWMatrix& target);
    void biggerThanScalar(float scalar);
    void biggerThanScalar(float scalar, LWMatrix& target);
    void smallerThanScalar(float scalar);
    void smallerThanScalar(float scalar, LWMatrix& target);
    void addScalar(float scaleThis, float scalar, LWMatrix& target);
    void addScalar(float scalar, LWMatrix& target);
    void addScalar(float scalar);
    void minWithScalar(float scalar, LWMatrix& target);
    void minWithScalar(float scalar);
    void maxWithScalar(float scalar, LWMatrix& target);
    void maxWithScalar(float scalar);
    void pow(float p, LWMatrix& target);
    void pow(float p);
    void scale(float _scale);
    void scale(float _scale, LWMatrix& target);
    void scale(float _scale, LWMatrix& target, lwdaStream_t stream);
    void scale(float _scale, lwdaStream_t stream);
    void zero();
    void zero(LWMatrix& like);

    float dotProduct(LWMatrix& b, LWMatrix& tmp, lwdaStream_t stream);
    float dotProduct(LWMatrix& b, lwdaStream_t stream);
    float dotProduct(LWMatrix& b);

    /*
     * Does SOFT transpose and returns result, leaving this matrix unchanged
     */
    LWMatrix& getTranspose();
    LWMatrix& getClone();

    /*
     * Does HARD transpose and puts result in target
     */
    void transpose(LWMatrix& target);

    /*
     * Does SOFT transpose
     */
    void transpose();
    bool transpose(bool trans);

    void flipTrans(LWMatrix& target, lwdaStream_t stream);
    void flipTrans(LWMatrix& target);
    LWMatrix& flipTrans();

    void print(int startRow, int rows, int startCol, int cols) const;
    void print(int rows, int cols) const;
    void printShape(const char* name) const;

    template <class Op> void applyBinaryV(Op op, LWMatrix& vec, LWMatrix& target) {
        applyBinaryV(op, vec, target, getDefaultStream());
    }

    template <class Op> void applyBinaryV(Op op, LWMatrix& vec, LWMatrix& target, lwdaStream_t stream) {
        assert(&target != &vec); // for now
        if (isSameDims(vec)) {
            applyBinary(op, vec, target, stream);
            return;
        }
        assert(vec.getNumRows() == 1 || vec.getNumCols() == 1);
        assert(vec.getNumRows() == _numRows || vec.getNumCols() == _numCols);
        assert(vec.isContiguous());

        target.resize(*this); // target must be same orientation as me for now
        int width = getLeadingDim(); //_isTrans ? _numRows : _numCols;
        int height = getFollowingDim(); //_isTrans ? _numCols : _numRows;
        dim3 threads(ADD_VEC_THREADS_X, ADD_VEC_THREADS_Y);

        if ((vec.getNumRows() == _numRows && !isTrans()) || (vec.getNumCols() == _numCols && isTrans())) {
            dim3 blocks(std::min(512, DIVUP(width, ADD_VEC_THREADS_X)), std::min(NUM_BLOCKS_MAX, DIVUP(height, ADD_VEC_THREADS_Y)));
            kColVectorOp<Op><<<blocks, threads, 0, stream>>>(getDevData(), vec.getDevData(), target.getDevData(), width, height, getStride(), target.getStride(), op);
        } else {
            dim3 blocks(std::min(NUM_BLOCKS_MAX, DIVUP(width, ADD_VEC_THREADS_X)), std::min(NUM_BLOCKS_MAX, DIVUP(height, ADD_VEC_THREADS_Y)));
            kRowVectorOp<Op><<<blocks, threads, 0, stream>>>(getDevData(), vec.getDevData(), target.getDevData(), width, height, getStride(), target.getStride(), op);
        }
        getLastLwdaError("Kernel exelwtion failed");
    //    lwdaThreadSynchronize();
    }

    template<class UnaryOperator> float argMax(UnaryOperator u) {
       return _totalAgg(LWMatrixAggs::ArgMax<UnaryOperator>(u));
    }
    static void batchedMatrixMultiply(LWMatrixV& a, LWMatrixV& b, LWMatrixV& target, float scaleTarget, float scaleAB, lwdaStream_t stream, const float** aPtrsDev, const float** bPtrsDev, float** tgtPtrsDev);
    static void batchedMatrixMultiply(LWMatrixV& a, LWMatrixV& b, LWMatrixV& target, float scaleTarget, float scaleAB, lwdaStream_t stream);
    static void batchedMatrixMultiply(LWMatrixV& a, LWMatrixV& b, LWMatrixV& target, float scaleTarget, float scaleAB, const float** aPtrsDev, const float** bPtrsDev, float** tgtPtrsDev);
    static void batchedMatrixMultiply(LWMatrixV& a, LWMatrixV& b, LWMatrixV& target, float scaleTarget, float scaleAB);

    static void assertSame(LWMatrixV& a);
};

class HostLWMatrix : public LWMatrix {
protected:
    void alloc(int numElements);
    void dealloc();
    LWMatrix& construct() const;
    LWMatrix& construct(bool isTrans) const;
    LWMatrix& construct(int numRows, int numCols, bool isTrans=false) const;
    LWMatrix& construct(const Matrix& like, bool copy) const;
    LWMatrix& construct(const LWMatrix& like, bool copy) const;
    LWMatrix& construct(const LWMatrix& like) const;
    LWMatrix& construct(const Matrix& like) const;
    LWMatrix& construct(MemorySegment* mem, int numRows, int numCols, int stride, bool isTrans) const;
public:
    ~HostLWMatrix();
    HostLWMatrix();
    HostLWMatrix(bool isTrans);
    HostLWMatrix(int numRows, int numCols, bool isTrans=false);
    HostLWMatrix(const Matrix& like, bool copy);
    HostLWMatrix(const LWMatrix& like, bool copy);
    HostLWMatrix(const LWMatrix& like);
    HostLWMatrix(const Matrix& like);
    HostLWMatrix(MemorySegment* mem, int numRows, int numCols, int stride, bool isTrans);
    void copyFromHost(const Matrix& hostMatrix);
    void copyFromHost(const Matrix& hostMatrix, bool resizeTarget);
    void copyFromHost(const Matrix& hostMatrix, bool resizeTarget, lwdaStream_t stream);
    void copyToHost(Matrix& hostMatrix) const;
    void copyToHost(Matrix& hostMatrix, bool resizeTarget) const;
    void copyToHost(Matrix& hostMatrix, bool resizeTarget, lwdaStream_t stream) const;
    lwdaTextureObject_t getTextureObject();
};

#endif /* LWMATRIX_H_ */
