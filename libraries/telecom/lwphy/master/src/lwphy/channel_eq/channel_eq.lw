/*
 * Copyright (c) 2020, LWPU CORPORATION.  All rights reserved.
 *
 * LWPU CORPORATION and its licensors retain all intellectual property
 * and proprietary rights in and to this software, related documentation
 * and any modifications thereto.  Any use, reproduction, disclosure or
 * distribution of this software and related documentation without an express
 * license agreement from LWPU CORPORATION is strictly prohibited.
 */

#include <algorithm>
#include "lwComplex.h"
#include <cooperative_groups.h>
#include "channel_eq.hpp"
#include "type_colwert.hpp"
#include <vector>

using namespace cooperative_groups;

namespace channel_eq
{
// #define ENABLE_PROFILING
// #define ENABLE_DEBUG
// #define ENABLE_DEBUG_SOFT_DEMAP

#define LEGACY_LLR_SCALE
/// #define WRITE_EQ_OUTPUT
#define EQ_COEF_COMP_H_MIMO_VER (2)
#define EQ_COEF_APPLY_VER (2) // Options: 1,2

static constexpr uint32_t N_THREADS_PER_WARP = 32; // lwdaDeviceProp::warpSize;

template <typename TElem, int NDim>
struct tensor_ref
{
    TElem* addr;
    int    dim[NDim];
    int    strides[NDim];
    size_t n_elem = 1;
    tensor_ref(tensor_pair& tp) :
        addr(static_cast<TElem*>(tp.second)),
        n_elem(1)
    {
        const tensor_layout_any& layout = tp.first.get().layout();
#pragma unroll
        for(int i = 0; i < NDim; ++i)
        {
            dim[i]     = (layout.rank() > i) ? layout.dimensions[i] : 1;
            strides[i] = (layout.rank() > i) ? layout.strides[i] : 0;
            n_elem *= dim[i];
        }
    }
    tensor_ref(const_tensor_pair& tp) :
        addr(static_cast<TElem*>(tp.second)),
        n_elem(1)
    {
        const tensor_layout_any& layout = tp.first.get().layout();
#pragma unroll
        for(int i = 0; i < NDim; ++i)
        {
            dim[i]     = (layout.rank() > i) ? layout.dimensions[i] : 1;
            strides[i] = (layout.rank() > i) ? layout.strides[i] : 0;
            n_elem *= dim[i];
        }
    }
    LWDA_BOTH long offset(int i0) const
    {
        return (strides[0] * (long)i0);
    }
    LWDA_BOTH long offset(int i0, int i1) const
    {
        return (strides[0] * (long)i0) + (strides[1] * (long)i1);
    }
    LWDA_BOTH long offset(int i0, int i1, int i2) const
    {
        return (strides[0] * (long)i0) + (strides[1] * (long)i1) + (strides[2] * (long)i2);
    };
    LWDA_BOTH long offset(int i0, int i1, int i2, int i3) const
    {
        return (strides[0] * (long)i0) + (strides[1] * (long)i1) + (strides[2] * (long)i2) + (strides[3] * (long)i3);
    };
    LWDA_BOTH TElem& operator()(int i0) { return *(addr + offset(i0)); }
    LWDA_BOTH const TElem& operator()(int i0) const { return *(addr + offset(i0)); }
    LWDA_BOTH TElem& operator()(int i0, int i1) { return *(addr + offset(i0, i1)); }
    LWDA_BOTH const TElem& operator()(int i0, int i1) const { return *(addr + offset(i0, i1)); }
    LWDA_BOTH TElem& operator()(int i0, int i1, int i2) { return *(addr + offset(i0, i1, i2)); }
    LWDA_BOTH const TElem& operator()(int i0, int i1, int i2) const { return *(addr + offset(i0, i1, i2)); }
    LWDA_BOTH TElem& operator()(int i0, int i1, int i2, int i3) { return *(addr + offset(i0, i1, i2, i3)); }
    LWDA_BOTH const TElem& operator()(int i0, int i1, int i2, int i3) const { return *(addr + offset(i0, i1, i2, i3)); }

    LWDA_BOTH size_t num_elem() { return n_elem; };
};

template <typename T, int M>
struct block_1D
{
    T         data[M];
    LWDA_BOTH T& operator()(int idx) { return data[idx]; }
};

template <typename T, int M, int N>
struct block_2D
{
    T         data[M * N];
    LWDA_BOTH T& operator()(int m, int n) { return data[(n * M) + m]; }
};

template <typename T, int L, int M, int N>
struct block_3D
{
    T         data[L * M * N];
    LWDA_BOTH T& operator()(int l, int m, int n) { return data[((n * M) + m) * L + l]; }
};

// Partial specialization of block_1D to use shared memory pointers
template <typename T, int M>
struct block_1D<T*, M>
{
    LWDA_BOTH block_1D(T* pData) :
        m_pData(pData){}; // static_assert(std::is_pointer<T>::value, "Must be a pointer type")
    block_1D()                    = delete;
    block_1D(block_1D const& blk) = delete;
    LWDA_BOTH block_1D& operator  =(block_1D const& block) { m_pData = block.m_pData; };
    ~block_1D()                   = default;

    LWDA_BOTH T&               operator()(int idx) { return m_pData[idx]; }
    static constexpr LWDA_BOTH size_t num_elem() { return M; }

private:
    T* m_pData = nullptr;
};

// Partial specialization of block_2D to use shared memory pointers
template <typename T, int M, int N>
struct block_2D<T*, M, N>
{
    LWDA_BOTH block_2D(T* pData) :
        m_pData(pData){};
    block_2D()                    = delete;
    block_2D(block_2D const& blk) = delete;
    LWDA_BOTH block_2D& operator  =(block_2D const& block) { m_pData = block.m_pData; };
    ~block_2D()                   = default;

    LWDA_BOTH T&               operator()(int m, int n) { return m_pData[(n * M) + m]; }
    static constexpr LWDA_BOTH size_t num_elem() { return M * N; }

private:
    T* m_pData = nullptr;
};

// Partial specialization of block_3D to use shared memory pointers
template <typename T, int L, int M, int N>
struct block_3D<T*, L, M, N>
{
    LWDA_BOTH block_3D(T* pData) :
        m_pData(pData){};
    block_3D()                    = delete;
    block_3D(block_3D const& blk) = delete;
    LWDA_BOTH block_3D& operator  =(block_3D const& block) { m_pData = block.m_pData; };
    ~block_3D()                   = default;

    LWDA_BOTH T&               operator()(int l, int m, int n) { return m_pData[((n * M) + m) * L + l]; }
    static constexpr LWDA_BOTH size_t num_elem() { return L * M * N; }

private:
    T* m_pData = nullptr;
};

// clang-format off
template <typename T> LWDA_BOTH_INLINE T         lwGet(int);
template<>            LWDA_BOTH_INLINE float     lwGet(int x) { return(float(x)); }
template<>            LWDA_BOTH_INLINE lwComplex lwGet(int x) { return(make_lwComplex(float(x), 0.0f)); }

template <typename T> LWDA_BOTH_INLINE T         lwGet(float);
template<>            LWDA_BOTH_INLINE float     lwGet(float x) { return(float(x)); }
template<>            LWDA_BOTH_INLINE lwComplex lwGet(float x) { return(make_lwComplex(x, 0.0f)); }

template <typename T> LWDA_BOTH_INLINE T         lwAbs(T);
template<>            LWDA_BOTH_INLINE float     lwAbs(float x) { return(fabsf(x)); }

static LWDA_BOTH_INLINE float     lwReal(lwComplex x)                   { return(lwCrealf(x)); }
static LWDA_BOTH_INLINE float     lwImag(lwComplex x)                   { return(lwCimagf(x)); }
static LWDA_BOTH_INLINE lwComplex lwConj(lwComplex x)                   { return(lwConjf(x)); }
static LWDA_BOTH_INLINE lwComplex operator-(lwComplex x, lwComplex y)   { return(lwCsubf(x, y)); }
static LWDA_BOTH_INLINE lwComplex operator*(lwComplex x, float y)       { return(make_lwComplex(lwCrealf(x)*y, lwCimagf(x)*y)); }
static LWDA_BOTH_INLINE lwComplex operator*(lwComplex x, lwComplex y)   { return(lwCmulf(x, y)); }
static LWDA_BOTH_INLINE lwComplex operator+=(lwComplex &x, float y)     { x = make_lwComplex(lwCrealf(x) + y, lwCimagf(x)); return x; }
static LWDA_BOTH_INLINE lwComplex operator+=(lwComplex &x, lwComplex y) { x = lwCaddf(x, y); return x; };
static LWDA_BOTH_INLINE lwComplex operator*=(lwComplex &x, float y)     { x = make_lwComplex(lwCrealf(x)*y, lwCimagf(x)*y); return x; }
static LWDA_BOTH_INLINE lwComplex lwFma(lwComplex x, lwComplex y, lwComplex a) { return lwCfmaf(x,y,a); }// a = (x*y) + a;


#if 0
static LWDA_BOTH_INLINE float lwCRmul(lwComplex x, lwComplex y) { return((lwCrealf(x) * lwCrealf(y)) - (lwCimagf(x) * lwCimagf(y))); }
static LWDA_BOTH_INLINE float lwCImul(lwComplex x, lwComplex y) { return((lwCrealf(x) * lwCimagf(y)) + (lwCimagf(x) * lwCrealf(y))); }
static LWDA_BOTH_INLINE lwComplex lwNeg(lwComplex x)                  { return(make_lwComplex(-lwCrealf(x), -lwCimagf(x))); }
static LWDA_BOTH_INLINE lwComplex lwAdd(lwComplex x, lwComplex y)       { return(lwCaddf(x, y)); }
static LWDA_BOTH_INLINE lwComplex lwMul(lwComplex x, lwComplex y)       { return(lwCmulf(x, y)); }
static LWDA_BOTH_INLINE lwComplex lwDiv(lwComplex x, lwComplex y)       { return(lwCdivf(x, y)); }
static LWDA_BOTH_INLINE lwComplex operator+(lwComplex x, lwComplex y)   { return(lwCaddf(x, y)); }
static LWDA_BOTH_INLINE lwComplex operator+=(lwComplex x, lwComplex y)  { return(lwCaddf(x, y)); };
static LWDA_BOTH_INLINE lwComplex operator-=(lwComplex x, lwComplex y)  { return(lwCsubf(x, y)); }
static LWDA_BOTH_INLINE lwComplex operator*(lwComplex x, lwComplex y)   { return(lwCmulf(x, y)); }
static LWDA_BOTH_INLINE lwComplex operator/(lwComplex x, float y)       { return(make_lwComplex(lwCrealf(x)/y, lwCimagf(x)/y)); }
static LWDA_BOTH_INLINE lwComplex operator*=(lwComplex &x, lwComplex y) { x = lwCmulf(x, y); return x; };
static LWDA_BOTH_INLINE lwComplex operator/=(lwComplex &x, float y)     { x = make_lwComplex(lwCrealf(x)/y, lwCimagf(x)/y); return x; }

// lwda_fp16.hpp
//__device__ __forceinline__ __half operator*(const __half &lh, const __half &rh) { return __hmul(lh, rh); }  
// __device__ __forceinline__ __half2& operator*=(__half2 &lh, const __half2 &rh) { lh = __hmul2(lh, rh); return lh; } 

// static LWDA_BOTH_INLINE __half2 lwConj(__half2 &hc) { __half2 t; t.x = hc.x; t.y = -hc.y; return t; }
// static LWDA_BOTH_INLINE __half2 lwGet(int x) {  __half2 t; t.x = __half(x); t.y = __float2half(0.0f); return t; } 
#endif
// clang-format on

// Use tag dispatching to ilwoke different behaviours (different functions) for each QAM
template <QAM_t>
struct QAMEnumToTagMap;

// Only even QAMs supported by 3GPP
template <>
struct QAMEnumToTagMap<QAM_t::QAM_4>
{
    struct QAM4Tag
    {};
};
template <>
struct QAMEnumToTagMap<QAM_t::QAM_16>
{
    struct QAM16Tag
    {};
};
template <>
struct QAMEnumToTagMap<QAM_t::QAM_64>
{
    struct QAM64Tag
    {};
};
template <>
struct QAMEnumToTagMap<QAM_t::QAM_256>
{
    struct QAM256Tag
    {};
};

// Soft demapper: maps soft estimates to LLRs
// Soft demapper overload for QAM64
template <typename TStorageIn,
          typename TStorageOut,
          typename TCompute,
          uint32_t N_LAYERS, // # of layers (# of cols in H matrix)
          uint32_t ND>
__device__ void soft_demap(QAMEnumToTagMap<QAM_t::QAM_64>,
                           block_2D<typename complex_from_scalar<TCompute>::type*, N_LAYERS + 1, ND>&       Data_eq,
                           block_2D<typename complex_from_scalar<TCompute>::type*, N_LAYERS + 1, N_LAYERS>& Ree,
                           tensor_ref<typename complex_from_scalar<TStorageOut>::type, 3>                   tData_eq,  // (N_LAYERS, NF, ND)
                           tensor_ref<TStorageOut, 3>                                                       tRee_diag, // (N_LAYERS, NF, NH)
                           tensor_ref<TStorageOut, 4>                                                       tLLR)                                                            // (N_LLR, N_LAYERS, NF, ND)
{
    // Assumption: there are atleast 2*N_LAYERS number of threads in the thread block
    // constexpr uint32_t NH_IDX = 0; // @todo: support mobility

    // Scaling which needs to be applied on soft estimates and Reer for comparision thresholds to be valid
#ifndef LEGACY_LLR_SCALE
    // scaling needed on REE so that its on the same scale as soft estimates
    constexpr TCompute QAM64_REE_SCALE = 42.0f;

    // scaling needed on the soft estimates before using them in the threshold comparisions below
    constexpr TCompute QAM64_SOFT_EST_SCALE = 6.480740698407860f; // sqrt(QAM64_REE_SCALE)

    // to match with LLRs from 5G comms toolbox an additional scaling by 4 is needed
    constexpr TCompute QAM64_MODEL_SCALE = 4.0f;
#else
    // scaling needed on the soft estimates before using them in the threshold comparisions below
    constexpr TCompute QAM64_SOFT_EST_SCALE = 9.2376f;

    // scaling needed on REE so that its on the same scale as soft estimates
    constexpr TCompute QAM64_REE_SCALE = QAM64_SOFT_EST_SCALE * QAM64_SOFT_EST_SCALE;

    // to match with LLRs from 5G comms toolbox an additional scaling by 4 is needed
    constexpr TCompute QAM64_MODEL_SCALE = 1.0f;
#endif

    constexpr uint32_t N_IQ = 2; // 2 samples: 1 I + 1 Q

    // # of LLRs per layer
    // constexpr uint32_t N_LLR = static_cast<uint32_t>(QAM_t::QAM_64);

    const uint32_t FREQ_IDX   = blockIdx.x;
    const uint32_t THREAD_IDX = threadIdx.x;
    const uint32_t N_THREADS  = blockDim.x;

    // Number of threads needed to map all the layers in a given frequency-time bin
    constexpr uint32_t N_THREADS_PER_FREQ_TIME_BIN = N_LAYERS * N_IQ;

    // Each thread processes I or Q component of a given layer
    const uint32_t LAYER_IDX = (THREAD_IDX / N_IQ) % N_LAYERS;
    const uint32_t IQ_IDX    = THREAD_IDX % N_IQ;

    // Number of data symbols which can be processed conlwrrently for a given frequency row
    const uint32_t N_DATA_SYM_PER_ITER = N_THREADS / N_THREADS_PER_FREQ_TIME_BIN;
    // Round up loop count to next integer
    const uint32_t N_ITER_TO_MAP_DATA_SYM = (0 == N_DATA_SYM_PER_ITER) ?
                                                ND :
                                                (ND + N_DATA_SYM_PER_ITER - 1) / N_DATA_SYM_PER_ITER;
    const uint32_t DATA_IDX = (0 == N_DATA_SYM_PER_ITER) ?
                                  0 :
                                  THREAD_IDX / N_THREADS_PER_FREQ_TIME_BIN;

    typedef typename complex_from_scalar<TCompute>::type TComplexCompute;

    // Roll in the both model and Ree scale factors to reduce number of multiplies
    // TCompute reeIlw = lwGet<TCompute>(QAM64_MODEL_SCALE)/(QAM64_LLR_SCALE*QAM64_LLR_SCALE*tRee_diag(LAYER_IDX, FREQ_IDX, NH_IDX));
    TCompute reeIlw = lwGet<TCompute>(QAM64_MODEL_SCALE) / (QAM64_REE_SCALE * Ree(LAYER_IDX, LAYER_IDX).x);

#ifdef ENABLE_DEBUG_SOFT_DEMAP
    if((0 == blockIdx.x) && (0 == blockIdx.y) && (0 == blockIdx.z) && (0 == threadIdx.x) && (0 == threadIdx.y) && (0 == threadIdx.z))
        printf("tIdx %d QAM64_LLR_SCALE %f N_DATA_SYM_PER_ITER %d, N_ITER_TO_MAP_DATA_SYM %d IQ_IDX %d LAYER_IDX %d DATA_IDX %d\n", threadIdx.x, QAM64_LLR_SCALE, N_DATA_SYM_PER_ITER, N_ITER_TO_MAP_DATA_SYM, IQ_IDX, LAYER_IDX, DATA_IDX);

    if(0 != blockIdx.x) return;
#endif

    for(uint32_t i = 0; i < N_ITER_TO_MAP_DATA_SYM; ++i)
    {
        // Data column being processed by this thread
        uint32_t iDataCol = i * N_DATA_SYM_PER_ITER + DATA_IDX;

        // Bounds check to ensure we stop at valid # of data symbols and layer index
        if((iDataCol >= ND) && (LAYER_IDX >= N_LAYERS)) break;

        // select and scale sample to be mapped to LLR
        // TComplexCompute softEst = type_colwert<TComplexCompute>(tData_eq(LAYER_IDX, FREQ_IDX, iDataCol));
        TComplexCompute softEst = Data_eq(LAYER_IDX, iDataCol);
        TCompute        sample  = (0 == IQ_IDX) ? softEst.x : softEst.y;
        sample *= QAM64_SOFT_EST_SCALE;

        TCompute absSample = fabs(sample);

        TCompute llr{};

        //-------------------------------------------------------------------------------------------------------
        // D1I, D1Q
        uint32_t LLR_VEC_IDX = IQ_IDX;

        if(absSample <= 2)
            llr = sample;
        else if((sample > 2) && (sample <= 4))
            llr = 2 * (sample - 1);
        else if((sample > 4) && (sample <= 6))
            llr = 3 * (sample - 2);
        else if(sample > 6)
            llr = 4 * (sample - 3);
        else if((sample >= -4) && (sample < -2))
            llr = 2 * (sample + 1);
        else if((sample >= -6) && (sample < -4))
            llr = 3 * (sample + 2);
        else if(sample < -6)
            llr = 4 * (sample + 3);

        tLLR(LLR_VEC_IDX, LAYER_IDX, FREQ_IDX, iDataCol) = type_colwert<TStorageOut>(reeIlw * llr);
#ifdef ENABLE_DEBUG_SOFT_DEMAP
        printf("LLR[%d][%d][%d][%d] = %f, softEst = %f + i%f, sample = %f, reeIlw = %f, llr = %f\n", LLR_VEC_IDX, LAYER_IDX, FREQ_IDX, iDataCol, tLLR(LLR_VEC_IDX, LAYER_IDX, FREQ_IDX, iDataCol), softEst.x, softEst.y, sample, reeIlw, llr);
#endif

        //-------------------------------------------------------------------------------------------------------
        // D2I, D2Q
        LLR_VEC_IDX = 1 * N_IQ + IQ_IDX;

        if(absSample <= 2)
            llr = 2 * (-absSample + 3);
        else if((absSample > 2) && (absSample <= 6))
            llr = 4 - absSample;
        else if(absSample > 6)
            llr = 2 * (-absSample + 5);

        tLLR(LLR_VEC_IDX, LAYER_IDX, FREQ_IDX, iDataCol) = type_colwert<TStorageOut>(reeIlw * llr);
#ifdef ENABLE_DEBUG_SOFT_DEMAP
        printf("LLR[%d][%d][%d][%d] = %f, softEst = %f + i%f, sample = %f, reeIlw = %f, llr = %f\n", LLR_VEC_IDX, LAYER_IDX, FREQ_IDX, iDataCol, tLLR(LLR_VEC_IDX, LAYER_IDX, FREQ_IDX, iDataCol), softEst.x, softEst.y, sample, reeIlw, llr);
#endif

        //-------------------------------------------------------------------------------------------------------
        // D3I, D3Q
        LLR_VEC_IDX = 2 * N_IQ + IQ_IDX;

        if(absSample <= 4)
            llr = absSample - 2;
        else if(absSample > 4)
            llr = -absSample + 6;

        tLLR(LLR_VEC_IDX, LAYER_IDX, FREQ_IDX, iDataCol) = type_colwert<TStorageOut>(reeIlw * llr);
#ifdef ENABLE_DEBUG_SOFT_DEMAP
        printf("LLR[%d][%d][%d][%d] = %f, softEst = %f + i%f, sample = %f, reeIlw = %f, llr = %f\n", LLR_VEC_IDX, LAYER_IDX, FREQ_IDX, iDataCol, tLLR(LLR_VEC_IDX, LAYER_IDX, FREQ_IDX, iDataCol), softEst.x, softEst.y, sample, reeIlw, llr);
#endif
    }
}

// Equalization kernel for static channel (zero to very low mobility)
// Inputs and outputs assumed to be column major
// dimGrid: (Nf)
template <typename TStorageIn,
          typename TDataRx,
          typename TStorageOut,
          typename TCompute,
          uint32_t N_BS_ANTS, // # of BS antenna (# of rows in H matrix)
          uint32_t N_LAYERS,  // # of layers (# of cols in H matrix)
          uint32_t ND,        // # of OFDM symbols bearing data
          uint32_t NH,        // # of estimates of H in time
          QAM_t    QAM>
__global__ void eq_mmse_kernel(tensor_ref<const uint8_t, 1>                                        tData_sym_loc, // ND
                               tensor_ref<const typename complex_from_scalar<TDataRx>::type, 3>    tData_rx,      // (NF, ND, N_BS_ANTS)
                               tensor_ref<const typename complex_from_scalar<TStorageIn>::type, 4> tH,            // (N_BS_ANTS, N_LAYERS, NF, NH)
                               tensor_ref<const TStorageIn, 2>                                     tNoise_pwr,    // (NF, NH)
                               tensor_ref<typename complex_from_scalar<TStorageOut>::type, 3>      tData_eq,      // (N_LAYERS, NF, ND)
                               tensor_ref<TStorageOut, 3>                                          tRee_diag,     // (N_LAYERS, NF, NH)
                               tensor_ref<TStorageOut, 4>                                          tLLR,          // (N_LLR, N_LAYERS, NF, ND)
                               int*                                                                debug)

{
    // H is channel matrix
    // G is the enhanced Gram matrix
    // Y is the received data matrix
    // A is the augmented matrix, A = [ G | I*Noise_pwr | B ]

    //--------------------------------------------------------------------------------------------------------
    // Dimensions
    constexpr uint32_t N_ROWS_H = N_BS_ANTS;
    constexpr uint32_t N_COLS_H = N_LAYERS;

    constexpr uint32_t N_ROWS_Y = N_BS_ANTS;
    constexpr uint32_t N_COLS_Y = ND;

    // B = H'*Y
    constexpr uint32_t N_ROWS_B = N_COLS_H;
    constexpr uint32_t N_COLS_B = N_COLS_Y;

    // G = H'*H + Noise_pwr*ilw(Rxx): N_LAYERS x N_LAYERS
    constexpr uint32_t N_ROWS_G = N_LAYERS;
    constexpr uint32_t N_COLS_G = N_LAYERS;

    constexpr uint32_t N_ROWS_NOISE_PWR = N_LAYERS;
    constexpr uint32_t N_COLS_NOISE_PWR = N_LAYERS;

    // A = [ G | I*Noise_pwr | B ]
    constexpr uint32_t N_ROWS_A = N_ROWS_G;
    constexpr uint32_t N_COLS_A = N_COLS_G + N_COLS_NOISE_PWR + N_COLS_B;

    constexpr uint32_t N_ROWS_U = N_ROWS_G;
    constexpr uint32_t N_COLS_U = N_COLS_G;

#ifdef WRITE_EQ_OUTPUT
    constexpr uint32_t N_ROWS_C = N_ROWS_B;
    constexpr uint32_t N_COLS_C = N_COLS_B;

    // constexpr uint32_t N_ROWS_L_ILW = N_ROWS_NOISE_PWR;
    constexpr uint32_t N_COLS_L_ILW = N_COLS_NOISE_PWR;

    constexpr uint32_t N_COLS_REE = N_COLS_L_ILW;

    constexpr uint32_t N_ROWS_DATA_EQ = N_ROWS_C;
    constexpr uint32_t N_COLS_DATA_EQ = N_COLS_C;
#endif

    // const uint32_t NF = gridDim.x; // # of subcarriers or frequency bins to be processed
    const uint32_t N_THREADS = blockDim.x;

    //--------------------------------------------------------------------------------------------------------
    // Compute matrix indices
    const uint32_t FREQ_IDX             = blockIdx.x;
    const uint32_t THREAD_IDX           = threadIdx.x;
    const uint32_t NH_IDX               = 0; // @todo: add high mobility support i.e. handle multiple estimates of H in time
    const uint32_t ROW_IDX_H            = THREAD_IDX % N_ROWS_H;
    const uint32_t COL_IDX_H            = THREAD_IDX / N_ROWS_H;
    const uint32_t N_COLS_H_RD_PER_ITER = N_THREADS / N_ROWS_H;
    // Round up to an integer
    const uint32_t N_ITER_TO_RD_H =
        (N_COLS_H + N_COLS_H_RD_PER_ITER - 1) / N_COLS_H_RD_PER_ITER;

    const uint32_t ROW_IDX_Y            = THREAD_IDX % N_ROWS_Y;
    const uint32_t COL_IDX_Y            = THREAD_IDX / N_ROWS_Y;
    const uint32_t N_COLS_Y_RD_PER_ITER = N_THREADS / N_ROWS_Y;
    // Round up to an integer
    const uint32_t N_ITER_TO_RD_Y =
        (N_COLS_Y + N_COLS_Y_RD_PER_ITER - 1) / N_COLS_Y_RD_PER_ITER;

    const uint32_t ROW_IDX_G                 = THREAD_IDX % N_ROWS_G;
    const uint32_t COL_IDX_G                 = THREAD_IDX / N_ROWS_G;
    const uint32_t N_COLS_G_COMPUTE_PER_ITER = N_THREADS / N_ROWS_G;
    // Round up to an integer
    const uint32_t N_ITER_TO_COMPUTE_G =
        (N_COLS_G + N_COLS_G_COMPUTE_PER_ITER - 1) / N_COLS_G_COMPUTE_PER_ITER;

    const uint32_t ROW_IDX_NOISE_PWR = THREAD_IDX % N_ROWS_NOISE_PWR;
    // const uint32_t COL_IDX_NOISE_PWR = THREAD_IDX / N_ROWS_NOISE_PWR;

#ifdef WRITE_EQ_OUTPUT
    const uint32_t ROW_IDX_DATA_EQ            = THREAD_IDX % N_ROWS_DATA_EQ;
    const uint32_t COL_IDX_DATA_EQ            = THREAD_IDX / N_ROWS_DATA_EQ;
    const uint32_t N_COLS_DATA_EQ_WR_PER_ITER = N_THREADS / N_ROWS_DATA_EQ;
    // Round up to an integer
    const uint32_t N_ITER_TO_WR_DATA_EQ =
        (N_COLS_DATA_EQ + N_COLS_DATA_EQ_WR_PER_ITER - 1) / N_COLS_DATA_EQ_WR_PER_ITER;
#endif

    //--------------------------------------------------------------------------------------------------------
    // Shared memory contents as processing progresses:
    // (H,Y) -> A = [ G | I*Noise_pwr | B ] -> [ U | Lilw*Noise_pwr | C ] -> [ U | Ree | DataEq ]
    // Shared memory allocation for bigger of A or (H and Y) and the results Ree and DataEq
    // auto uint_max = [](const uint32_t a, const uint32_t b) -> uint32_t {return a > b ? a : b;};

    constexpr uint32_t N_SMEM_ELEMS1 = (((N_ROWS_H + 1) * N_COLS_H) + ((N_ROWS_Y + 1) * N_COLS_Y)); // (N_ROWS_H + 1) and (N_ROWS_Y + 1) for SMEM padding to avoid bank conflicts
    constexpr uint32_t N_SMEM_ELEMS2 = ((N_ROWS_A + 1) * N_COLS_A);                                 // (N_ROWS_A + 1) for SMEM padding

    // For 4x4, the threads required for computing G processing fits within a warp. Hence its possible to
    // overlay (H and G) and (Y and B). However, once the thread count exceeds a warp, the assumption does not
    // hold
#if 1
    constexpr uint32_t N_SMEM_ELEMS = N_SMEM_ELEMS1 + N_SMEM_ELEMS2;
#else
    constexpr uint32_t N_ROWS_REE        = N_LAYERS;
    constexpr uint32_t N_ROWS_C          = N_ROWS_B;
    constexpr uint32_t N_COLS_C          = N_COLS_B;
    constexpr uint32_t N_ROWS_DATA_EQ    = N_ROWS_C;
    constexpr uint32_t N_COLS_DATA_EQ    = N_COLS_C;
    constexpr uint32_t N_SMEM_ELEMS =
        (N_SMEM_ELEMS1 > N_SMEM_ELEMS2 ? N_SMEM_ELEMS1 : N_SMEM_ELEMS2) +
        (N_ROWS_REE + N_ROWS_DATA_EQ * N_COLS_DATA_EQ);
#endif
    typedef typename complex_from_scalar<TCompute>::type    TComplexCompute;
    typedef typename complex_from_scalar<TStorageIn>::type  TComplexStorageIn;
    typedef typename complex_from_scalar<TStorageOut>::type TComplexStorageOut;

    __shared__ TComplexCompute smemBlk[N_SMEM_ELEMS];

    // Column offsets to Ree (which is followed by DataEq) matrix within the augmented matrix
    // which form the RHS of the back substition (and after back substitution the RHS is overwritten by the
    // unknowns being solved for)
    constexpr uint32_t START_COL_OFFSET_REE = N_COLS_U;

#if 1
    // SMEM overlay1
    constexpr uint32_t                                 SMEM_START_OFFSET_H = 0;
    block_2D<TComplexCompute*, N_ROWS_H + 1, N_COLS_H> shH(&smemBlk[SMEM_START_OFFSET_H]);

    constexpr uint32_t                                 SMEM_START_OFFSET_Y = SMEM_START_OFFSET_H + shH.num_elem();
    block_2D<TComplexCompute*, N_ROWS_Y + 1, N_COLS_Y> shY(&smemBlk[SMEM_START_OFFSET_Y]);

    constexpr uint32_t                                 SMEM_START_OFFSET_A = SMEM_START_OFFSET_Y + shY.num_elem();
    block_2D<TComplexCompute*, N_ROWS_A + 1, N_COLS_A> shA(&smemBlk[SMEM_START_OFFSET_A]);

    constexpr uint32_t                                 SMEM_START_OFFSET_G = SMEM_START_OFFSET_A;
    block_2D<TComplexCompute*, N_ROWS_G + 1, N_COLS_G> shG(&smemBlk[SMEM_START_OFFSET_G]);

    constexpr uint32_t                                                 SMEM_START_OFFSET_NOISE_PWR = SMEM_START_OFFSET_G + shG.num_elem();
    block_2D<TComplexCompute*, N_ROWS_NOISE_PWR + 1, N_COLS_NOISE_PWR> shNoisePwrMat(&smemBlk[SMEM_START_OFFSET_NOISE_PWR]);

    constexpr uint32_t                                 SMEM_START_OFFSET_B = SMEM_START_OFFSET_NOISE_PWR + shNoisePwrMat.num_elem();
    block_2D<TComplexCompute*, N_ROWS_B + 1, N_COLS_B> shB(&smemBlk[SMEM_START_OFFSET_B]);
#else
    constexpr uint32_t                                 SMEM_START_OFFSET_H = 0;
    block_2D<TComplexCompute*, N_ROWS_H + 1, N_COLS_H> shH(&smemBlk[SMEM_START_OFFSET_H]);

    constexpr uint32_t                                 SMEM_START_OFFSET_Y = SMEM_START_OFFSET_H + shH.num_elem();
    block_2D<TComplexCompute*, N_ROWS_Y + 1, N_COLS_Y> shY(&smemBlk[SMEM_START_OFFSET_Y]);

    // SMEM overlay: G replaces H and B replaces Y
    constexpr uint32_t                                 SMEM_START_OFFSET_A = 0;
    block_2D<TComplexCompute*, N_ROWS_A + 1, N_COLS_A> shA(&smemBlk[SMEM_START_OFFSET_A]);

    constexpr uint32_t                                 SMEM_START_OFFSET_G = SMEM_START_OFFSET_A;
    block_2D<TComplexCompute*, N_ROWS_G + 1, N_COLS_G> shG(&smemBlk[SMEM_START_OFFSET_G]);

    constexpr uint32_t                                                 SMEM_START_OFFSET_NOISE_PWR = SMEM_START_OFFSET_G + shG.num_elem();
    block_2D<TComplexCompute*, N_ROWS_NOISE_PWR + 1, N_COLS_NOISE_PWR> shNoisePwrMat(&smemBlk[SMEM_START_OFFSET_NOISE_PWR]);

    constexpr uint32_t                                 SMEM_START_OFFSET_B = SMEM_START_OFFSET_NOISE_PWR + shNoisePwrMat.num_elem();
    block_2D<TComplexCompute*, N_ROWS_B + 1, N_COLS_B> shB(&smemBlk[SMEM_START_OFFSET_B]);
#endif

    // SMEM overlay: after LU - U replaces G, C replaces B and Noise_pwr*Lilw replaces Noise_pwr*I
    auto& shU = shG;
    // auto &shLIlw = shNoisePwrMat;
    auto& shC = shB;

    // SMEM overlay: after back substitution - Ree replaces Noise_pwr*Lilw and DataEq replaces C
    // (i.e. results are stored inplace)
    auto& shRee    = shNoisePwrMat;
    auto& shDataEq = shC;

    //--------------------------------------------------------------------------------------------------------
    // Stage1: Load inputs
    thread_block const& thisThrdBlk = this_thread_block();

    TCompute Noise_pwr = type_colwert<TCompute>(tNoise_pwr(FREQ_IDX, NH_IDX));

#ifdef ENABLE_DEBUG
    if(0 == thisThrdBlk.thread_rank())
    {
        printf("BlockDim.x = %d, N0 = %f\n", blockDim.x, Noise_pwr);
    }
#endif

    // Prefetch Y into shared memory
    // Each iteration loads N_ROWS_Y x N_COLS_Y_RD_PER_ITER submatrix of Y. For the read only N_ROWS_Y elements
    // are coalesced
#pragma unroll
    for(uint32_t i = 0; i < N_ITER_TO_RD_Y; ++i)
    {
        uint32_t iCol = i * N_COLS_Y_RD_PER_ITER + COL_IDX_Y;
        // Not all threads would participate in the last iteration
        if(iCol < N_COLS_Y)
        {
            shY(ROW_IDX_Y, iCol) = type_colwert<TComplexCompute>(tData_rx(FREQ_IDX, tData_sym_loc(iCol), ROW_IDX_Y));

#ifdef ENABLE_DEBUG
            printf("Y[%d][%d] = %f+j%f\n", ROW_IDX_Y, iCol, shY(ROW_IDX_Y, iCol).x, shY(ROW_IDX_Y, iCol).y);
#endif
        }
    }

    // Prefetch H into shared memory
#pragma unroll
    for(uint32_t i = 0; i < N_ITER_TO_RD_H; ++i)
    {
        uint32_t iCol = i * N_COLS_H_RD_PER_ITER + COL_IDX_H;
        // All threads may not participate in the last iteration
        if(iCol < N_COLS_H)
        {
            shH(ROW_IDX_H, iCol) = type_colwert<TComplexCompute>(tH(ROW_IDX_H, iCol, FREQ_IDX, NH_IDX));

#ifdef ENABLE_DEBUG
            printf("H[%d][%d] = %f+j%f\n", ROW_IDX_H, iCol, shH(ROW_IDX_H, iCol).x, shH(ROW_IDX_H, iCol).y);
#endif
        }
    }

    // A thread can process elements different from what it loaded, so wait for all threads in the thread block
    // to complete
    thisThrdBlk.sync();

    //--------------------------------------------------------------------------------------------------------
    // Stage2: Compute the enhanced Gram matrix: G = (H'*H + Noise_pwr*ilw(Rxx)) and B = H'*Y

    // Compute B = H'*Y: N_COLS_H x N_COLS_Y = N_LAYERS x ND
    // Select a subset of the threads in the warp to access the columns of Y and each thread computes one
    // column of B
    if(THREAD_IDX < N_COLS_Y)
    {
        TComplexCompute prod[N_COLS_H]{}; // Note columns of H are rows of H'

        // Each iteration of outer loop walks through a row of Y (threads in the warp accessing all columns
        // of Y) i.e. Y(i,:)
#pragma unroll
        for(uint32_t i = 0; i < N_ROWS_Y; ++i)
        {
            TComplexCompute y = shY(i, THREAD_IDX);
            // Inner loop multiplies the entries of column i of H' - H'(:,i) (row i of conj(H) -> conj(H(i,:)))
            // with Y(i,:)
#pragma unroll
            for(uint32_t j = 0; j < N_COLS_H; ++j)
            {
                prod[j] = lwFma(lwConj(shH(i, j)), y, prod[j]);
            }
        }

        // Each thread computes one column of B
#pragma unroll
        for(uint32_t j = 0; j < N_COLS_H; ++j)
        {
            shB(j, THREAD_IDX) = prod[j];

#ifdef ENABLE_DEBUG
            printf("B[%d][%d] = %f+j%f\n", j, THREAD_IDX, shB(j, THREAD_IDX).x, shB(j, THREAD_IDX).y);
#endif
        }
    }

    // Compute G = H'*H + Noise_pwr*ilw(Rxx): N_LAYERS x N_LAYERS
#pragma unroll
    for(uint32_t i = 0; i < N_ITER_TO_COMPUTE_G; ++i)
    {
        uint32_t iCol = i * N_COLS_G_COMPUTE_PER_ITER + COL_IDX_G;
        // All threads may not participate in the last iteration
        if((ROW_IDX_G < N_ROWS_G) && (iCol < N_COLS_G))
        {
            TComplexCompute prod{};

            // Compute H'*H
            // @todo: since H'*H is hermtian symmetric, only numele(H)/2 threads need to be computed. For tiny
            // matrices this may not account to much work but worthwhile to, explore for large matrices
#pragma unroll
            for(uint32_t elem = 0; elem < N_ROWS_H; ++elem)
            {
                prod = lwFma(lwConj(shH(elem, ROW_IDX_G)), shH(elem, iCol), prod);
            }
            // Add Noise_pwr*ilw(Rxx) to the diagonal of H'*H, ilw(Rxx) is assumed to be unity
            if(ROW_IDX_G == iCol)
            {
                prod.x = lwReal(prod) + Noise_pwr;
            }
            shG(ROW_IDX_G, iCol) = prod;

#ifdef ENABLE_DEBUG
            printf("G[%d][%d] = %f+j%f\n", ROW_IDX_G, iCol, shG(ROW_IDX_G, iCol).x, shG(ROW_IDX_G, iCol).y);
#endif

            // Initialize diagonal matrix I*Noise_pwr (starting LU with I*Noise_pwr instead of I avoids an explicit
            // multiply of Noise_pwr with Gilw later i.e. Gilw = Ree. If LU is started with I then Ree = Gilw*Noise_pwr)
            // I*Noise_pwr and G have the same dimensions, safe to use iCol to index into I*Noise_pwr matrix as well
            TComplexCompute cval{};
            if(ROW_IDX_NOISE_PWR == iCol)
            {
                cval = lwGet<TComplexCompute>(Noise_pwr);
            }
            shNoisePwrMat(ROW_IDX_NOISE_PWR, iCol) = cval;
        }
    }

    thisThrdBlk.sync();

#ifdef ENABLE_DEBUG
    for(uint32_t i = 0; i < N_ROWS_A; ++i)
    {
        if(THREAD_IDX < N_COLS_A)
            printf("A[%d][%d] = %f+j%f\n", i, THREAD_IDX, shA(i, THREAD_IDX).x, shA(i, THREAD_IDX).y);
    }
#endif

    //--------------------------------------------------------------------------------------------------------
    // Stage3: Perform joint LU factorization
    // A = [ G | I*Noise_pwr | B ] -> [ U | Lilw*Noise_pwr | C ]
    // where C = L\B , Lilw = L\I

    // Iterate row by row of A applying Gaussian elimination. In each iteration Gaussian elimination
    // annihilates all elements of a column below main diagonal of G. In iteration k annihilate elements
    // G(k+1:n, k ). At the end of all iterations G is transformed to U
    // While transforming G to U, applying Gaussian elimination to other columns of A i.e. matrices Noise_pwr*I
    // and B produces matrices Lilw and C respetively which can then be used to compute Ree and DataEq via back
    // substitution
#pragma unroll
    for(uint32_t k = 0; k < N_ROWS_A - 1; ++k)
    {
        // Gaussian elimination on submatrix A(k,k), since we know that A(k+1:n,k) will be annihilated we directly
        // proceed to applying Gaussian elimination on submatrix A(k+1:n,k+1:n)

        // Complex multiplication by ilwerse of real number is cheaper instead of complex division
        // @todo: add a safety check on Akk to avoid divide by zero
        TCompute minus_one_over_Akk = lwGet<TCompute>(-1) / lwReal(shA(k, k));

#ifdef ENABLE_DEBUG
        printf("Iteration: %d, A[%d][%d] = %f+j%f, ilw = %f---------------\n", k, k, k, shA(k, k).x, shA(k, k).y, minus_one_over_Akk);
#endif

#pragma unroll
        for(uint32_t i = k + 1; i < N_ROWS_A; ++i)
        {
            // Compute multipliers needed for Gaussian elimination. For storage compactness the multipliers
            // (non-zero elements of Gauss vector/column of L) are stored in the annihilated zero location of
            // columns of U
#ifdef ENABLE_DEBUG
            printf("Before storing multiplier: A[%d][%d] = %f+j%f\n", i, k, shA(i, k).x, shA(i, k).y);
#endif
            // All threads compute multiplier Aik into a register and use it but only one thread stores it back
            TComplexCompute Aik = shA(i, k) * minus_one_over_Akk;

#ifdef ENABLE_DEBUG
            printf("After storing multiplier: A[%d][%d] = %f+j%f\n", i, k, Aik.x, Aik.y);
#endif
            // Perform Gaussian elimination:
            // linear combination of row k and row i starting from column element k+1:N_COLS_A
            if((THREAD_IDX > k) && (THREAD_IDX < N_COLS_A))
            {
                shA(i, THREAD_IDX) = lwFma(Aik, shA(k, THREAD_IDX), shA(i, THREAD_IDX));

#ifdef ENABLE_DEBUG
                printf("A[%d][%d] = %f+j%f\n", i, THREAD_IDX, shA(i, THREAD_IDX).x, shA(i, THREAD_IDX).y);
#endif
            }

            if(0 == thisThrdBlk.thread_rank())
            {
                shA(i, k) = Aik;
            }
        }

        // Wait for the entire submatrix update
        thisThrdBlk.sync();
    }

    //--------------------------------------------------------------------------------------------------------
    // Stage4: Solve by back substitution
    // Solve U*[ Ree | DataEq ] = [ Lilw*Noise_pwr | C ] for Gilw and DataEq where Gilw = U\(Lilw*Noise_pwr)
    // and DataEq = U\C

    // Perform back substitution on last row first
    TCompute one_over_Uii = lwGet<TCompute>(1) / lwReal(shU(N_ROWS_U - 1, N_ROWS_U - 1));
    if((THREAD_IDX >= START_COL_OFFSET_REE) && (THREAD_IDX < N_COLS_A))
    {
        shA(N_ROWS_U - 1, THREAD_IDX) *= one_over_Uii;
    }

    // Now perform back substitution beginning from the row above last row and working upwards upto the first
    // row. Each thread solves for one column of the result (which are the columns of Ree and DataEq)
#pragma unroll
    for(int32_t i = N_ROWS_U - 2; i >= 0; --i)
    {
        one_over_Uii = lwGet<TCompute>(1) / lwReal(shU(i, i));

        if((THREAD_IDX >= START_COL_OFFSET_REE) && (THREAD_IDX < N_COLS_A))
        {
            TComplexCompute sum = lwGet<TComplexCompute>(0);

#pragma unroll
            for(int32_t j = i + 1; j < N_ROWS_A; ++j)
            {
                sum = lwFma(shA(j, THREAD_IDX), shU(i, j), sum);
            }
            shA(i, THREAD_IDX) = (shA(i, THREAD_IDX) - sum) * one_over_Uii;
        }
    }

    thisThrdBlk.sync();

#ifdef WRITE_EQ_OUTPUT // retained for debug                                                                                      \
                       //-------------------------------------------------------------------------------------------------------- \
                       // Stage5: Write the results (Ree and DataEq) into device memory
#pragma unroll
    for(uint32_t i = 0; i < N_ITER_TO_WR_DATA_EQ; ++i)
    {
        uint32_t iCol = i * N_COLS_DATA_EQ_WR_PER_ITER + COL_IDX_DATA_EQ;
        // Not all threads would participate in the last iteration
        if(iCol < N_COLS_DATA_EQ)
        {
            tData_eq(ROW_IDX_DATA_EQ, FREQ_IDX, iCol) = type_colwert<TComplexStorageOut>(shDataEq(ROW_IDX_DATA_EQ, iCol));
#ifdef ENABLE_DEBUG
            if((1 == FREQ_IDX) || (2 == FREQ_IDX) || (3 == FREQ_IDX))
                printf("X[%d][%d][%d] = %f+j%f\n", ROW_IDX_DATA_EQ, FREQ_IDX, iCol, shDataEq(ROW_IDX_DATA_EQ, iCol).x, shDataEq(ROW_IDX_DATA_EQ, iCol).y);
#endif
        }
    }

    // The following assumes that blockDim.x >= N_COLS_REE
    if(THREAD_IDX < N_COLS_REE)
    {
        tRee_diag(THREAD_IDX, FREQ_IDX, NH_IDX) = type_colwert<TStorageOut>(shRee(THREAD_IDX, THREAD_IDX).x);
    }

    thisThrdBlk.sync();
#endif

    //--------------------------------------------------------------------------------------------------------
    // Stage5: Compute LLRs
    // soft_demap<TStorageIn,TStorageOut,TCompute,N_LAYERS,ND>(QAMEnumToTagMap<QAM>(),tData_eq, tRee_diag, tLLR);
    soft_demap<TStorageIn, TStorageOut, TCompute, N_LAYERS, ND>(QAMEnumToTagMap<QAM>(), shDataEq, shRee, tData_eq, tRee_diag, tLLR);
}

template <typename TStorageIn,
          typename TCompute,
          uint32_t N_ROWS_MAT,
          uint32_t N_COLS_MAT>
__device__ __forceinline__ void cmplxMatLoad(thread_block const&                                                                      thisThrdBlk,
                                             block_2D<const typename complex_from_scalar<TStorageIn>::type*, N_ROWS_MAT, N_COLS_MAT>& srcMat,
                                             block_2D<typename complex_from_scalar<TCompute>::type*, N_ROWS_MAT + 1, N_COLS_MAT>&     dstMat)
{
    typedef typename complex_from_scalar<TStorageIn>::type TComplexStorageIn;
    typedef typename complex_from_scalar<TCompute>::type   TComplexCompute;

    const uint32_t     N_THREADS               = thisThrdBlk.size();
    const uint32_t     THREAD_IDX              = thisThrdBlk.thread_rank();
    constexpr uint32_t N_MAT_ELEMS_TO_RD       = N_ROWS_MAT * N_COLS_MAT;
    const uint32_t     N_MAT_ELEMS_RD_PER_ITER = (N_MAT_ELEMS_TO_RD > N_THREADS) ? N_THREADS : N_MAT_ELEMS_TO_RD;
    const uint32_t     N_ITER_TO_RD_MAT        = div_round_up(N_MAT_ELEMS_TO_RD, N_MAT_ELEMS_RD_PER_ITER);

    for(uint32_t i = 0; i < N_ITER_TO_RD_MAT; ++i)
    {
        uint32_t matElemIdx = ((i * N_MAT_ELEMS_RD_PER_ITER) + THREAD_IDX);
        uint32_t iRow       = matElemIdx % N_ROWS_MAT;
        uint32_t iCol       = matElemIdx / N_ROWS_MAT;
        // Not all threads would participate in the last iteration
        if(iCol < N_COLS_MAT)
        {
            dstMat(iRow, iCol) =
                type_colwert<TComplexCompute>(srcMat(iRow, iCol));

#ifdef ENABLE_DEBUG
            printf("Mat[%d][%d] = %f+j%f\n", iRow, iCol, dstMat(iRow, iCol).x, dstMat(iRow, iCol).y);
#endif
        }
    }
}

// Inplace LU factorization of Matrix A - Iterative version (submatrix updates done one row at a time)
// Iterative version maybe used if the thread block size is around the length of a row (i.e. N_COLS_MAT) of
// the augmented matrix
template <typename TCompute,
          uint32_t N_ROWS_MAT,
          uint32_t N_COLS_MAT>
__device__ __forceinline__ void luFactorizeIter(thread_block const&                                                                  thisThrdBlk,
                                                block_2D<typename complex_from_scalar<TCompute>::type*, N_ROWS_MAT + 1, N_COLS_MAT>& matA)
{
    typedef typename complex_from_scalar<TCompute>::type TComplexCompute;

    const uint32_t THREAD_X_IDX = threadIdx.x;

    // Iterate row by row of A applying Gaussian elimination. In each iteration Gaussian elimination
    // annihilates all elements of a column below main diagonal of G. In iteration k annihilate elements
    // G(k+1:n, k ). At the end of all iterations G is transformed to U
    // While transforming G to U, applying Gaussian elimination to other columns of A i.e. matrices I and M
    // produces matrices Lilw and F respetively which can then be used to compute Ree and C via back
    // substitution
#pragma unroll
    for(uint32_t k = 0; k < N_ROWS_MAT - 1; ++k)
    {
        // Gaussian elimination on submatrix A(k,k), since we know that A(k+1:n,k) will be annihilated we directly
        // proceed to applying Gaussian elimination on submatrix A(k+1:n,k+1:n)

        // Complex multiplication by ilwerse of real number is cheaper instead of complex division
        // @todo: add a safety check on Akk to avoid divide by zero
        TCompute minus_one_over_Akk = lwGet<TCompute>(-1) / lwReal(matA(k, k));

#ifdef ENABLE_DEBUG
        printf("Iteration: %d, A[%d][%d] = %f+j%f, ilw = %f---------------\n", k, k, k, matA(k, k).x, matA(k, k).y, minus_one_over_Akk);
#endif

#pragma unroll
        for(uint32_t i = k + 1; i < N_ROWS_MAT; ++i)
        {
            // Compute multipliers needed for Gaussian elimination. For storage compactness the multipliers
            // (non-zero elements of Gauss vector/column of L) are stored in the annihilated zero location of
            // columns of U
#ifdef ENABLE_DEBUG
            printf("Before storing multiplier: A[%d][%d] = %f+j%f\n", i, k, matA(i, k).x, matA(i, k).y);
#endif
            // All threads compute multiplier Aik into a register and use it but only one thread stores it back
            TComplexCompute Aik = matA(i, k) * minus_one_over_Akk;

#ifdef ENABLE_DEBUG
            printf("After storing multiplier: A[%d][%d] = %f+j%f\n", i, k, Aik.x, Aik.y);
#endif
            // Perform Gaussian elimination:
            // linear combination of row k and row i starting from column element k+1:N_COLS_A
            if((THREAD_X_IDX > k) && (THREAD_X_IDX < N_COLS_MAT))
            {
                matA(i, THREAD_X_IDX) = lwFma(Aik, matA(k, THREAD_X_IDX), matA(i, THREAD_X_IDX));

#ifdef ENABLE_DEBUG
                printf("A[%d][%d] = %f+j%f\n", i, THREAD_X_IDX, matA(i, THREAD_X_IDX).x, matA(i, THREAD_X_IDX).y);
#endif
            }

            // Ensure all threads (which may extend across multiple warps for nColsA > 32) have read and use shA(i,k) before writing into it
            thisThrdBlk.sync();
            if(0 == (THREAD_X_IDX))
            {
                matA(i, k) = Aik;
            }
        }

        // Wait for the entire submatrix update
        thisThrdBlk.sync();
    }
}

// Inplace LU factorization of Matrix A - Parallel version (entire sub-matrix update done one in parallel)
// - Parallel version maybe used (over iterative version) if the thread block size is much larger than the
// length of a row (i.e. N_COLS_MAT) of the augmented matrix resulting in fewer inactive threads during LU factorization.
// - This parallel version (luFactorizeParallel_v1) maximizes the number of active threads during the 
// sub-matrix update by recomputing indices for each outerloop iteration and also reduces the number of inner 
// loop iterations needed to update the submatrix. The index recomputation cost is paid once per outer loop
// iteration.
template <typename TCompute,
          uint32_t N_ROWS_MAT,
          uint32_t N_COLS_MAT>
__device__ __forceinline__ void luFactorizeParallel_v1(thread_block const&                                                                  thisThrdBlk,
                                                       block_2D<typename complex_from_scalar<TCompute>::type*, N_ROWS_MAT + 1, N_COLS_MAT>& matA)
{
    typedef typename complex_from_scalar<TCompute>::type TComplexCompute;

    const uint32_t N_THREADS_X  = blockDim.x;
    const uint32_t THREAD_X_IDX = threadIdx.x;

    // Iterate row by row of A applying Gaussian elimination. In each iteration Gaussian elimination
    // annihilates all elements of a column below main diagonal of G. In iteration k annihilate elements
    // G(k+1:n, k ). At the end of all iterations G is transformed to U
    // While transforming G to U, applying Gaussian elimination to other columns of A i.e. matrices I and M
    // produces matrices Lilw and F respetively which can then be used to compute Ree and C via back
    // substitution
#pragma unroll
    for(uint32_t k = 0; k < N_ROWS_MAT - 1; ++k)
    {
        // Gaussian elimination on submatrix A(k,k), since we know that A(k+1:n,k) will be annihilated we directly
        // proceed to applying Gaussian elimination on submatrix A(k+1:n,k+1:n)

        // Complex multiplication by ilwerse of real number is cheaper instead of complex division
        // @todo: add a safety check on Akk to avoid divide by zero
        TCompute minus_one_over_Akk = lwGet<TCompute>(-1) / lwReal(matA(k, k));

#ifdef ENABLE_DEBUG
        printf("Iteration: %d, A[%d][%d] = %f+j%f, ilw = %f---------------\n", k, k, k, matA(k, k).x, matA(k, k).y, minus_one_over_Akk);
#endif

        // The entire sub-matrix can be updated in parallel (i.e. in addition to columns, the rows are also updated in parallel)
        // to extent permitted by parallelism (i.e. thread count) available in the thread block
        uint32_t subMatStartRowOffset = (k + 1);
        uint32_t subMatStartColOffset = (k + 1);
        uint32_t nRowsSubMat          = N_ROWS_MAT - subMatStartRowOffset;
        uint32_t nColsSubMat          = N_COLS_MAT - subMatStartColOffset;
        uint32_t subMatColIdx         = THREAD_X_IDX % nColsSubMat;
        uint32_t matColIdx            = subMatStartColOffset + subMatColIdx; // process columns > k, note: matrix is in column major layout

        // Ensure whole rows are updated at a time
        // Assumes N_THREADS_X >= nColsSubMat
        uint32_t nRowsSubMatPerIter = N_THREADS_X / nColsSubMat;
        bool     thrdEnable         = (THREAD_X_IDX < (nRowsSubMatPerIter * nColsSubMat)); // Disable threads which don't update full rows
        uint32_t nIterToProcSubMat  = div_round_up(nRowsSubMat, nRowsSubMatPerIter);
        for(uint32_t i = 0; i < nIterToProcSubMat; ++i)
        {
            uint32_t subMatRowIdx = (i * nRowsSubMatPerIter) + (THREAD_X_IDX / nColsSubMat);
            uint32_t matRowIdx    = subMatStartRowOffset + subMatRowIdx; // process rows > k

            TComplexCompute Aik = lwGet<TComplexCompute>(0);
            if(thrdEnable && (matRowIdx < N_ROWS_MAT))
            {
                // Compute multipliers needed for Gaussian elimination. For storage compactness the multipliers
                // (non-zero elements of Gauss vector/column of L) are stored in the annihilated zero location of
                // columns of U

#ifdef ENABLE_DEBUG
                printf("Before storing multiplier: A[%d][%d] = %f+j%f\n", matRowIdx, k, matA(matRowIdx, k).x, matA(matRowIdx, k).y);
#endif
                // All threads compute multiplier Aik into a register and use it but only one thread stores it back
                Aik = matA(matRowIdx, k) * minus_one_over_Akk;

#ifdef ENABLE_DEBUG
                printf("After storing multiplier: A[%d][%d] = %f+j%f\n", matRowIdx, k, Aik.x, Aik.y);
#endif
                // Perform Gaussian elimination:
                // linear combination of row k and row i starting from column element k+1:N_COLS_A
                // if((THREAD_X_IDX > k) && (THREAD_X_IDX < N_COLS_MAT))
                if(matColIdx < N_COLS_MAT)
                {
                    matA(matRowIdx, matColIdx) = lwFma(Aik, matA(k, matColIdx), matA(matRowIdx, matColIdx));

#ifdef ENABLE_DEBUG
                    printf("A[%d][%d] = %f+j%f\n", matRowIdx, matColIdx, matA(matRowIdx, matColIdx).x, matA(matRowIdx, matColIdx).y);
#endif
                }
            }

            // Ensure all threads (which may extend across multiple warps for nColsA > 32) have read and use shA(i,k) before writing into it
            thisThrdBlk.sync();
            if(thrdEnable && (matRowIdx < N_ROWS_MAT) && (subMatStartColOffset == matColIdx))
            {
                matA(matRowIdx, k) = Aik;
            }
        }

        // Wait for the entire submatrix update
        thisThrdBlk.sync();
    }
}

// Inplace LU factorization of Matrix - Parallel version (entire sub-matrix update done one in parallel)
// - Parallel version maybe used (over iterative version) if the thread block size is much larger than the
// length of a row (i.e. N_COLS_MAT) of the augmented matrix resulting in fewer inactive threads during LU factorization.
// - This parallel version computes indices used in sub-matrix update once before the outerloop eliminating
// per outer loop index recomputation cost. Consequently (unlike luFactorizeParallel_v1) the smaller sub-matrix
// updates do not utilize all the availalbe threads (inactive thread count increases with smaller sub-matrices)
// while the number of inner loop iterations does not decrease with decrease in sub-matrix dimension.
template <typename TCompute,
          uint32_t N_ROWS_MAT,
          uint32_t N_COLS_MAT>
__device__ __forceinline__ void luFactorizeParallel_v2(thread_block const&                                                                  thisThrdBlk,
                                                       block_2D<typename complex_from_scalar<TCompute>::type*, N_ROWS_MAT + 1, N_COLS_MAT>& matA)
{
    typedef typename complex_from_scalar<TCompute>::type TComplexCompute;

    const uint32_t N_THREADS_X  = blockDim.x;
    const uint32_t THREAD_X_IDX = threadIdx.x;

    // Ensure whole rows are updated at a time
    // Assumes N_THREADS_X >= nColsSubMat
    uint32_t nRowsMatPerIter = N_THREADS_X / N_COLS_MAT;
    bool     thrdEnableMain  = (THREAD_X_IDX < (nRowsMatPerIter * N_COLS_MAT)); // Disable threads which don't update full rows
    uint32_t nIterToProcMat  = div_round_up(N_ROWS_MAT, nRowsMatPerIter);
    uint32_t matColIdx       = THREAD_X_IDX % N_COLS_MAT;
    uint32_t matRowOffset    = THREAD_X_IDX / N_COLS_MAT;

    // Iterate row by row of A applying Gaussian elimination. In each iteration Gaussian elimination
    // annihilates all elements of a column below main diagonal of G. In iteration k annihilate elements
    // G(k+1:n, k ). At the end of all iterations G is transformed to U
    // While transforming G to U, applying Gaussian elimination to other columns of A i.e. matrices I and M
    // produces matrices Lilw and F respetively which can then be used to compute Ree and C via back
    // substitution
#pragma unroll
    for(uint32_t k = 0; k < N_ROWS_MAT - 1; ++k)
    {
        // Gaussian elimination on submatrix A(k,k), since we know that A(k+1:n,k) will be annihilated we directly
        // proceed to applying Gaussian elimination on submatrix A(k+1:n,k+1:n)

        // Complex multiplication by ilwerse of real number is cheaper instead of complex division
        // @todo: add a safety check on Akk to avoid divide by zero
        TCompute minus_one_over_Akk = lwGet<TCompute>(-1) / lwReal(matA(k, k));

#ifdef ENABLE_DEBUG
        printf("Iteration: %d, A[%d][%d] = %f+j%f, ilw = %f---------------\n", k, k, k, matA(k, k).x, matA(k, k).y, minus_one_over_Akk);
#endif

        // The entire sub-matrix can be updated in parallel (i.e. in addition to columns, the rows are also updated in parallel)
        // to extent permitted by parallelism (i.e. thread count) available in the thread block
        for(uint32_t i = 0; i < nIterToProcMat; ++i)
        {
            uint32_t matRowIdx = (i * nRowsMatPerIter) + matRowOffset;
            // process rows > k and process columns > k
            bool thrdEnable = thrdEnableMain && ((matRowIdx > k) && (matRowIdx < N_ROWS_MAT) &&
                                                 (matColIdx > k));

            TComplexCompute Aik = lwGet<TComplexCompute>(0);
            if(thrdEnable)
            {
                // Compute multipliers needed for Gaussian elimination. For storage compactness the multipliers
                // (non-zero elements of Gauss vector/column of L) are stored in the annihilated zero location of
                // columns of U

#ifdef ENABLE_DEBUG
                printf("Before storing multiplier: A[%d][%d] = %f+j%f\n", matRowIdx, k, matA(matRowIdx, k).x, matA(matRowIdx, k).y);
#endif
                // All threads compute multiplier Aik into a register and use it but only one thread stores it back
                Aik = matA(matRowIdx, k) * minus_one_over_Akk;

#ifdef ENABLE_DEBUG
                printf("After storing multiplier: A[%d][%d] = %f+j%f\n", matRowIdx, k, Aik.x, Aik.y);
#endif
                // Perform Gaussian elimination:
                // linear combination of row k and row i starting from column element k+1:N_COLS_A
                matA(matRowIdx, matColIdx) = lwFma(Aik, matA(k, matColIdx), matA(matRowIdx, matColIdx));

#ifdef ENABLE_DEBUG
                printf("A[%d][%d] = %f+j%f\n", matRowIdx, matColIdx, matA(matRowIdx, matColIdx).x, matA(matRowIdx, matColIdx).y);
#endif
            }

            // Ensure all threads (which may extend across multiple warps for nColsA > 32) have read and use shA(i,k) before writing into it
            thisThrdBlk.sync();
            if(thrdEnable && ((k + 1) == matColIdx))
            {
                matA(matRowIdx, k) = Aik;
            }
        }

        // Wait for the entire submatrix update
        thisThrdBlk.sync();
    }
}

template <typename TCompute,
          uint32_t N_ROWS_U,
          uint32_t N_COLS_U,
          uint32_t N_ROWS_A,
          uint32_t N_COLS_A>
__device__ __forceinline__ void backSub(thread_block const&                                                              thisThrdBlk,
                                        uint32_t                                                                         startCol,
                                        block_2D<typename complex_from_scalar<TCompute>::type*, N_ROWS_U + 1, N_COLS_U>& matU,
                                        block_2D<typename complex_from_scalar<TCompute>::type*, N_ROWS_A + 1, N_COLS_A>& matA)
{
    typedef typename complex_from_scalar<TCompute>::type TComplexCompute;

    const uint32_t THREAD_X_IDX = threadIdx.x;

    // Perform back substitution on last row first
    TCompute one_over_Uii = lwGet<TCompute>(1) / lwReal(matU(N_ROWS_U - 1, N_ROWS_U - 1));
    if((THREAD_X_IDX >= startCol) && (THREAD_X_IDX < N_COLS_A))
    {
        matA(N_ROWS_U - 1, THREAD_X_IDX) *= one_over_Uii;
    }

    // Now perform back substitution beginning from the row above last row and working upwards upto the first
    // row. Each thread solves for one column of the result (which are the columns of Ree and C)
#pragma unroll
    for(int32_t i = N_ROWS_U - 2; i >= 0; --i)
    {
        one_over_Uii = lwGet<TCompute>(1) / lwReal(matU(i, i));

        if((THREAD_X_IDX >= startCol) && (THREAD_X_IDX < N_COLS_A))
        {
            TComplexCompute sum = lwGet<TComplexCompute>(0);

#pragma unroll
            for(int32_t j = i + 1; j < N_ROWS_A; ++j)
            {
                sum = lwFma(matA(j, THREAD_X_IDX), matU(i, j), sum);
            }
            matA(i, THREAD_X_IDX) = (matA(i, THREAD_X_IDX) - sum) * one_over_Uii;
        }
    }

    thisThrdBlk.sync();
}

// Equalization kernel for high order MIMO and per PRB
// {N_LAYERS, N_BS_ANTS} = {8,16}, {16,16}
// Inputs and outputs assumed to be column major
// dimBlock: (N_BS_ANTS*N_LAYERS, N_TONES_PER_ITER)
// dimGrid : ((LWPHY_N_TONES_PER_PRB/N_THRD_BLK_TONES), Nprb)
template <typename TStorageIn,
          typename TStorageOut,
          typename TCompute,
          uint32_t N_THRD_BLK_TONES, // # of frequency bins processed by a thread block
          uint32_t N_TONES_PER_ITER, // # of frequency bins processed in 1 iteration by the thread block
          uint32_t N_BS_ANTS,        // # of BS antenna (# of rows in H matrix)
          uint32_t N_LAYERS,         // # of layers (# of cols in H matrix)
          uint32_t NH>               // # of estimates of H in time
__global__ void
eq_mmse_coef_comp_high_mimo_kernel_v1(tensor_ref<const typename complex_from_scalar<TStorageIn>::type, 4> tH,          // (N_BS_ANTS, N_LAYERS, NF, NH)
                                      tensor_ref<const typename complex_from_scalar<TStorageIn>::type, 4> tRwwIlw,     // (N_BS_ANTS, N_BS_ANTS, N_PRB, NH)
                                      tensor_ref<TStorageOut, 3>                                          tReeDiagIlw, // (N_SC, N_LAYERS, N_PRB, NH)   // (N_LAYERS, NF, NH)
                                      tensor_ref<typename complex_from_scalar<TStorageOut>::type, 4>      tCoef,       // (N_SC, N_LAYERS, N_BS_ANTS, N_PRB, NH) // (N_LAYERS, N_BS_ANTS, NF, NH)
                                      tensor_ref<typename complex_from_scalar<TStorageOut>::type, 4>      tDbg)
{
    // H is channel matrix
    // G is the enhanced Gram matrix
    // A is the augmented matrix, A = [ G | I | M ]

    //--------------------------------------------------------------------------------------------------------
    // Dimensions

    // H  : Channel matrix
    constexpr uint32_t N_ROWS_H = N_BS_ANTS;
    constexpr uint32_t N_COLS_H = N_LAYERS;

    // Rww: Noise covariance matrix
    constexpr uint32_t N_ROWS_RWW = N_BS_ANTS;
    constexpr uint32_t N_COLS_RWW = N_BS_ANTS;

    // M  : Intermediate matrix, M = H'*RwwIlw
    constexpr uint32_t N_ROWS_M = N_COLS_H;   // N_LAYERS
    constexpr uint32_t N_COLS_M = N_COLS_RWW; // N_BS_ANTS

    // G  : Enhanced Gram matrix, G = H'*RwwIlw*H + ilw(Rxx)
    constexpr uint32_t N_ROWS_G = N_LAYERS;
    constexpr uint32_t N_COLS_G = N_LAYERS;

    // U  : Upper triangular matrix
    constexpr uint32_t N_ROWS_U = N_ROWS_G;
    constexpr uint32_t N_COLS_U = N_COLS_G;

    // I  : Identity matrix, I = G*Gilw
    constexpr uint32_t N_ROWS_I = N_ROWS_G;
    constexpr uint32_t N_COLS_I = N_COLS_G;

    // Ree: Residual error covariance matrix, Ree = Gilw
    constexpr uint32_t N_ROWS_REE = N_ROWS_G;
    // constexpr uint32_t N_COLS_REE = N_COLS_G;

    // C  : MMSE coefficient matrix, C = Ree*H'*RwwIlw = Ree*M
    constexpr uint32_t N_ROWS_C = N_ROWS_REE;
    constexpr uint32_t N_COLS_C = N_COLS_M;

    // A  : Augmented result matrix, A = [ G | I | M ] -> [ U | Lilw | F ] -> [ U | Gilw | C ]
    constexpr uint32_t N_ROWS_A = N_ROWS_G;
    constexpr uint32_t N_COLS_A = N_COLS_G + N_COLS_I + N_COLS_M;

    // Column offsets to Ree (which is followed by C) matrix within the augmented matrix
    // which form the RHS of the back substition (and after back substitution the RHS is overwritten by the
    // unknowns being solved for)
    constexpr uint32_t START_COL_OFFSET_REE = N_COLS_U;

    constexpr int32_t N_ITER = N_THRD_BLK_TONES / N_TONES_PER_ITER;

    constexpr uint32_t N_INST = (1 == N_ITER) ? 1 : 2; // double buffering for pipelining

    // const uint32_t N_THREADS_X = blockDim.x; // Number of threads needed to process one frequency bin
    // const uint32_t N_THREADS = blockDim.x * blockDim.y; // N_TONES_PER_ITER == blockDim.y

    // Reciprocal of symbol energy
    const TCompute ES_ILW = lwGet<TCompute>(1);

    //--------------------------------------------------------------------------------------------------------
    // Compute indices used for element access
    const uint32_t THREAD_X_IDX = threadIdx.x;
    // const uint32_t THREAD_IDX   = (threadIdx.y * blockDim.x) + threadIdx.x;

    // There are N_TONES_PER_ITER groups of threads with each group containing N_THREADS_X per group
    const uint32_t PRB_IDX                     = blockIdx.y;
    const uint32_t THRD_GRP_FREQ_OFFSET        = threadIdx.y;
    const uint32_t THRD_BLK_START_FREQ_OFFSET  = (blockIdx.x * N_THRD_BLK_TONES);
    const uint32_t THRD_GRP_START_FREQ_OFFSET  = THRD_BLK_START_FREQ_OFFSET + THRD_GRP_FREQ_OFFSET;
    const uint32_t THRD_BLK_ABS_START_FREQ_IDX = PRB_IDX * LWPHY_N_TONES_PER_PRB;

    const uint32_t NH_IDX = 0; // @todo: add high mobility support i.e. handle multiple estimates of H in time

    // const uint32_t ROW_IDX_RWW = THREAD_IDX % N_ROWS_RWW;
    // const uint32_t COL_IDX_RWW = THREAD_IDX / N_ROWS_RWW; // COL_IDX_RWW needs a bounds check (since N_THREADS > # of Rww elements)

    const uint32_t ROW_IDX_H = THREAD_X_IDX % N_ROWS_H;
    const uint32_t COL_IDX_H = THREAD_X_IDX / N_ROWS_H;

    const uint32_t ROW_IDX_I = THREAD_X_IDX % N_ROWS_I;
    const uint32_t COL_IDX_I = THREAD_X_IDX / N_ROWS_I;

    const uint32_t ROW_IDX_M = THREAD_X_IDX % N_ROWS_M;
    const uint32_t COL_IDX_M = THREAD_X_IDX / N_ROWS_M;

    const uint32_t ROW_IDX_G = THREAD_X_IDX % N_ROWS_G;
    const uint32_t COL_IDX_G = THREAD_X_IDX / N_ROWS_G; // COL_IDX_G needs a bounds check (since N_THREADS_X > # of G elements)

    // const uint32_t ROW_IDX_REE = THREAD_X_IDX % N_ROWS_REE;
    // const uint32_t COL_IDX_REE = THREAD_X_IDX / N_ROWS_REE; // COL_IDX_REE needs a bounds check (since N_THREADS_X > # of Ree elements)

    const uint32_t ROW_IDX_C = THREAD_X_IDX % N_ROWS_C;
    const uint32_t COL_IDX_C = THREAD_X_IDX / N_ROWS_C;

    //--------------------------------------------------------------------------------------------------------
    // Shared memory allocation
    // H[N_TONES_PER_ITER*N_INST]

    // Shared memory contents as processing progresses:
    // A = [ G | I | M ] -> [ U | Lilw | F ] -> [ U | Ree | C ]

    constexpr uint32_t N_SMEM_ELEMS =
        (((N_ROWS_H + 1) * N_COLS_H * N_INST) + // (N_ROWS_H + 1) for SMEM padding to avoid bank conflicts
         (N_ROWS_A + 1) * N_COLS_A) *
            N_TONES_PER_ITER +
        (N_ROWS_RWW + 1) * N_COLS_RWW;

    typedef typename complex_from_scalar<TCompute>::type    TComplexCompute;
    typedef typename complex_from_scalar<TStorageIn>::type  TComplexStorageIn;
    typedef typename complex_from_scalar<TStorageOut>::type TComplexStorageOut;

    __shared__ TComplexCompute smemBlk[N_SMEM_ELEMS];

    constexpr uint32_t                                     SMEM_START_OFFSET_RWW = 0;
    block_2D<TComplexCompute*, N_ROWS_RWW + 1, N_COLS_RWW> shRwwIlw(&smemBlk[SMEM_START_OFFSET_RWW]);

    constexpr uint32_t                                         SMEM_START_OFFSET_H_BLK = SMEM_START_OFFSET_RWW + shRwwIlw.num_elem();
    const uint32_t                                             SMEM_START_OFFSET_H     = SMEM_START_OFFSET_H_BLK + THRD_GRP_FREQ_OFFSET * (N_ROWS_H + 1) * N_COLS_H * N_INST;
    block_3D<TComplexCompute*, N_ROWS_H + 1, N_COLS_H, N_INST> shH(&smemBlk[SMEM_START_OFFSET_H]);

    constexpr uint32_t                                 SMEM_START_OFFSET_A_BLK = SMEM_START_OFFSET_H_BLK + N_TONES_PER_ITER * shH.num_elem();
    const uint32_t                                     SMEM_START_OFFSET_A     = SMEM_START_OFFSET_A_BLK + THRD_GRP_FREQ_OFFSET * (N_ROWS_A + 1) * N_COLS_A;
    block_2D<TComplexCompute*, N_ROWS_A + 1, N_COLS_A> shA(&smemBlk[SMEM_START_OFFSET_A]);

    // SMEM overlay: A with [ G | I | M ]
    const uint32_t                                     SMEM_START_OFFSET_G = SMEM_START_OFFSET_A;
    block_2D<TComplexCompute*, N_ROWS_G + 1, N_COLS_G> shG(&smemBlk[SMEM_START_OFFSET_G]);

    const uint32_t                                     SMEM_START_OFFSET_I = SMEM_START_OFFSET_G + shG.num_elem();
    block_2D<TComplexCompute*, N_ROWS_I + 1, N_COLS_I> shI(&smemBlk[SMEM_START_OFFSET_I]);

    const uint32_t                                     SMEM_START_OFFSET_M = SMEM_START_OFFSET_I + shI.num_elem();
    block_2D<TComplexCompute*, N_ROWS_M + 1, N_COLS_M> shM(&smemBlk[SMEM_START_OFFSET_M]);

    // SMEM overlay: after LU - U replaces G, Lilw replaces I and F replaces M
    auto& shU    = shG;
    auto& shLilw = shI;
    auto& shF    = shM;

    // SMEM overlay: after back substitution - Ree replaces Lilw and C replaces F
    // (i.e. results are stored inplace)
    auto& shRee = shLilw;
    auto& shC   = shF;

    //--------------------------------------------------------------------------------------------------------
    // Stage1: Load inputs
    thread_block const& thisThrdBlk = this_thread_block();

#ifdef ENABLE_DEBUG
    if(0 != blockIdx.x) return;
#endif

    // Read RwwIlw (RwwIlw is used by all subcarriers within the PRB)
    block_2D<const typename complex_from_scalar<TStorageIn>::type*, N_ROWS_RWW, N_COLS_RWW> srcRwwIlw(tRwwIlw.addr + tRwwIlw.offset(0, 0, PRB_IDX, NH_IDX));
    cmplxMatLoad<TStorageIn, TCompute, N_ROWS_RWW, N_COLS_RWW>(thisThrdBlk, srcRwwIlw, shRwwIlw);

    // Prologue
    // Prefetch H for first iteration
    uint32_t f       = 0;
    uint32_t lwrrIdx = 0;
    if(COL_IDX_H < N_COLS_H)
    {
        const uint32_t FREQ_IDX     = THRD_GRP_START_FREQ_OFFSET + (f * N_TONES_PER_ITER);
        const uint32_t ABS_FREQ_IDX = THRD_BLK_ABS_START_FREQ_IDX + FREQ_IDX;

        shH(ROW_IDX_H, COL_IDX_H, lwrrIdx) =
            type_colwert<TComplexCompute>(tH(ROW_IDX_H, COL_IDX_H, ABS_FREQ_IDX, NH_IDX));

        // tDbg(ROW_IDX_H,COL_IDX_H,ABS_FREQ_IDX, NH_IDX) = shH(ROW_IDX_H,COL_IDX_H);
#ifdef ENABLE_DEBUG
        printf("H[%d][%d][%d] = %f+j%f\n", ABS_FREQ_IDX, ROW_IDX_H, COL_IDX_H, shH(ROW_IDX_H, COL_IDX_H, FREQ_IDX).x, shH(ROW_IDX_H, COL_IDX_H, FREQ_IDX).y);
#endif
    }

    for(int32_t f = 0; f < N_ITER; ++f)
    {
        const uint32_t FREQ_IDX     = THRD_GRP_START_FREQ_OFFSET + (f * N_TONES_PER_ITER);
        const uint32_t ABS_FREQ_IDX = THRD_BLK_ABS_START_FREQ_IDX + FREQ_IDX;

        // A thread can process elements different from what it loaded, so wait for all threads in the thread block
        // to complete
        thisThrdBlk.sync();

        // Prefetch H for next iteration
        if((COL_IDX_H < N_COLS_H) && (f < (N_ITER - 1)))
        {
            const uint32_t NXT_FREQ_IDX     = THRD_GRP_START_FREQ_OFFSET + ((f + 1) * N_TONES_PER_ITER);
            const uint32_t NXT_ABS_FREQ_IDX = THRD_BLK_ABS_START_FREQ_IDX + NXT_FREQ_IDX;
            shH(ROW_IDX_H, COL_IDX_H, lwrrIdx ^ 1) =
                type_colwert<TComplexCompute>(tH(ROW_IDX_H, COL_IDX_H, NXT_ABS_FREQ_IDX, NH_IDX));

            // tDbg(ROW_IDX_H,COL_IDX_H,NXT_ABS_FREQ_IDX, NH_IDX) = shH(ROW_IDX_H,COL_IDX_H);
#ifdef ENABLE_DEBUG
            printf("H[%d][%d][%d] = %f+j%f\n", NXT_ABS_FREQ_IDX, ROW_IDX_H, COL_IDX_H, shH(ROW_IDX_H, COL_IDX_H, NXT_FREQ_IDX).x, shH(ROW_IDX_H, COL_IDX_H, NXT_FREQ_IDX).y);
#endif
        }

        //---------------------------------------------------------------------------------------------------
        // Stage2: Compute the enhanced Gram matrix: M = H'*ilw(Rww) and G = (H'*ilw(Rww)*H + ilw(Rxx))

        // Compute M = H'*ilw(Rww): N_COLS_H x N_COLS_RWW = N_LAYERS x N_BS_ANTS
        if(COL_IDX_M < N_COLS_M)
        {
            TComplexCompute M = lwGet<TComplexCompute>(0);

#pragma unroll
            for(uint32_t i = 0; i < N_ROWS_RWW; ++i)
            {
                M = lwFma(lwConj(shH(i, ROW_IDX_M, lwrrIdx)), shRwwIlw(i, COL_IDX_M), M);
#ifdef ENABLE_DEBUG
                if(1 == FREQ_IDX) printf("Iter %d: M[%d][%d][%d] = %f+j%f conjH[%d][%d] = %f+j%f RwwIlw[%d][%d] = %f+j%f\n", i, FREQ_IDX, ROW_IDX_M, COL_IDX_M, shM(ROW_IDX_M, COL_IDX_M).x, shM(ROW_IDX_M, COL_IDX_M).y, ROW_IDX_M, i, lwReal(lwConj(shH(i, ROW_IDX_M, lwrrIdx))), lwImag(lwConj(shH(i, ROW_IDX_M, lwrrIdx))), i, COL_IDX_M, shRwwIlw(i, COL_IDX_M).x, shRwwIlw(i, COL_IDX_M).y);
#endif
            }
            shM(ROW_IDX_M, COL_IDX_M) = M;
        }

        // Wait for matrix M computation to finish, before using it in computation of G
        thisThrdBlk.sync();

        // if((1 == f) && (COL_IDX_RWW < N_COLS_RWW)) tDbg(ROW_IDX_RWW,COL_IDX_RWW,PRB_IDX, NH_IDX) = shRwwIlw(ROW_IDX_RWW,COL_IDX_RWW);
        // tDbg(ROW_IDX_H,COL_IDX_H,THRD_BLK_ABS_START_FREQ_IDX+FREQ_IDX, NH_IDX) = shH(ROW_IDX_H,COL_IDX_H,lwrrIdx);
        // tDbg(ROW_IDX_M,COL_IDX_M,THRD_BLK_ABS_START_FREQ_IDX+FREQ_IDX, NH_IDX) = shM(ROW_IDX_M,COL_IDX_M);

        // Compute G = (M*H + RxxIlw)
        if(COL_IDX_G < N_COLS_G)
        {
            TComplexCompute G = lwGet<TComplexCompute>(0);

#pragma unroll
            for(uint32_t i = 0; i < N_ROWS_H; ++i)
            {
                G = lwFma(shM(ROW_IDX_G, i), shH(i, COL_IDX_G, lwrrIdx), G);
            }

            if(ROW_IDX_G == COL_IDX_G)
            {
                G += ES_ILW;
            }
            shG(ROW_IDX_G, COL_IDX_G) = G;

#ifdef ENABLE_DEBUG
            printf("After: M[%d][%d][%d] = %f+j%f\n", FREQ_IDX, ROW_IDX_M, COL_IDX_M, shM(ROW_IDX_M, COL_IDX_M).x, shM(ROW_IDX_M, COL_IDX_M).y);
            printf("G[%d][%d][%d] = %f+j%f\n", FREQ_IDX, ROW_IDX_G, COL_IDX_G, shG(ROW_IDX_G, COL_IDX_G).x, shG(ROW_IDX_G, COL_IDX_G).y);
#endif

            // tDbg(ROW_IDX_G,COL_IDX_G,THRD_BLK_ABS_START_FREQ_IDX+FREQ_IDX, NH_IDX) = type_colwert<TComplexStorageOut>(shG(ROW_IDX_G,COL_IDX_G));
        }

        // Initialize matrix I
        if(COL_IDX_I < N_COLS_I)
        {
            shI(ROW_IDX_I, COL_IDX_I) =
                (ROW_IDX_I != COL_IDX_I) ? lwGet<TComplexCompute>(0) : lwGet<TComplexCompute>(1);
        }

        // Wait for matrix G computation to finish, before using it in computation of Gilw
        thisThrdBlk.sync();

#ifdef ENABLE_DEBUG
        // A0
        for(uint32_t i = 0; i < N_ROWS_A; ++i)
        {
            if(THREAD_X_IDX < N_COLS_A)
                tDbg(i, THREAD_X_IDX, THRD_BLK_ABS_START_FREQ_IDX + FREQ_IDX, NH_IDX) = type_colwert<TComplexStorageOut>(shA(i, THREAD_X_IDX));
        }
#endif

        //---------------------------------------------------------------------------------------------------
        // Stage3: Perform joint LU factorization
        // A = [ G | I | M ] -> [ U | Lilw | F ]
        // where U = L\G, Lilw = L\I, F = L\M

        // eq_mmse_coef_comp_high_mimo_kernel_v1: thread block size >> # of columns of augmented matrix
        // (i.e. (N_LAYERS * N_LAYERS) >> (2*N_LAYERS + N_BS_ANTS)). Thus use parallel version of the
        // factorization algorithm to cut down iteration count and increase active threads during sub-matrix
        // updates
        // luFactorizeIter<TCompute, N_ROWS_A, N_COLS_A>(thisThrdBlk, shA);
        // luFactorizeParallel_v1<TCompute, N_ROWS_A, N_COLS_A>(thisThrdBlk, shA);
        luFactorizeParallel_v2<TCompute, N_ROWS_A, N_COLS_A>(thisThrdBlk, shA);

#ifdef ENABLE_DEBUG
        // A1
        for(uint32_t i = 0; i < N_ROWS_A; ++i)
        {
            if(THREAD_X_IDX < N_COLS_A)
                tDbg(i, THREAD_X_IDX, THRD_BLK_ABS_START_FREQ_IDX + FREQ_IDX, NH_IDX) = type_colwert<TComplexStorageOut>(shA(i, THREAD_X_IDX));
        }
#endif
        //---------------------------------------------------------------------------------------------------
        // Stage4: Solve by back substitution, compute residual error covariance matrix Ree as the ilwerse
        // of Gram matrix: Gilw = Ree and the MMSE coefficient matrix C
        // Solve U*[ Ree | C ] = [ Lilw | F ] for Gilw and C where Gilw = U\(Lilw) and C = U\F

        backSub<TCompute, N_ROWS_U, N_COLS_U, N_ROWS_A, N_COLS_A>(thisThrdBlk, START_COL_OFFSET_REE, shU, shA);

#ifdef ENABLE_DEBUG
        // A2
        for(uint32_t i = 0; i < N_ROWS_A; ++i)
        {
            if(THREAD_X_IDX < N_COLS_A)
                tDbg(i, THREAD_X_IDX, THRD_BLK_ABS_START_FREQ_IDX + FREQ_IDX, NH_IDX) = type_colwert<TComplexStorageOut>(shA(i, THREAD_X_IDX));
        }
#endif
        //--------------------------------------------------------------------------------------------------------
        // Stage5: Write the results (Ree and C) into device memory
        if(COL_IDX_C < N_COLS_C)
        {
            // Compute bias correction factor lambda and apply to coefficients
            TCompute lambda = lwGet<TCompute>(1) / (lwGet<TCompute>(1) - lwReal(shRee(ROW_IDX_C, ROW_IDX_C)));

            tCoef(COL_IDX_C, FREQ_IDX, ROW_IDX_C, PRB_IDX) =
                type_colwert<TComplexStorageOut>(shC(ROW_IDX_C, COL_IDX_C) * lambda);
        }

        if(THREAD_X_IDX < N_ROWS_REE)
        {
            // Compute ReeIlw while applying bias correction
            // TCompute reeIlw = (lwGet<TCompute>(1)/shMemBlkReeDiag(THREAD_X_IDX, FREQ_IDX)) - lwGet<TCompute>(1);
            TCompute ree    = lwReal(shRee(THREAD_X_IDX, THREAD_X_IDX));
            TCompute reeIlw = (lwGet<TCompute>(1) - ree) / ree;

            tReeDiagIlw(FREQ_IDX, THREAD_X_IDX, PRB_IDX) = type_colwert<TStorageOut>(reeIlw);
        }

#ifdef ENABLE_DEBUG
        printf("C[%d][%d][%d] = %f+j%f\n", ABS_FREQ_IDX, ROW_IDX_C, COL_IDX_C, shC(ROW_IDX_C, COL_IDX_C).x, shC(ROW_IDX_C, COL_IDX_C).y);
#endif
        // thisThrdBlk.sync();
        lwrrIdx ^= 1;
    }
}

// Equalization kernel for high order MIMO and per PRB
// This flavor uses fewer threads (as many needed as solve the joint LU/back substitution processing)
// {N_LAYERS, N_BS_ANTS} = {8,16}, {16,16}
// Inputs and outputs assumed to be column major
// dimBlock: (N_BS_ANTS*N_LAYERS, N_TONES_PER_ITER)
// dimGrid : ((LWPHY_N_TONES_PER_PRB/N_THRD_BLK_TONES), Nprb)
template <typename TStorageIn,
          typename TStorageOut,
          typename TCompute,
          uint32_t N_THRD_BLK_TONES, // # of frequency bins processed by a thread block
          uint32_t N_TONES_PER_ITER, // # of frequency bins processed in 1 iteration by the thread block
          uint32_t N_BS_ANTS,        // # of BS antenna (# of rows in H matrix)
          uint32_t N_LAYERS,         // # of layers (# of cols in H matrix)
          uint32_t NH>               // # of estimates of H in time
__global__ void
eq_mmse_coef_comp_high_mimo_kernel_v2(tensor_ref<const typename complex_from_scalar<TStorageIn>::type, 4> tH,          // (N_BS_ANTS, N_LAYERS, NF, NH)
                                      tensor_ref<const typename complex_from_scalar<TStorageIn>::type, 4> tRwwIlw,     // (N_BS_ANTS, N_BS_ANTS, N_PRB, NH)
                                      tensor_ref<TStorageOut, 3>                                          tReeDiagIlw, // (N_SC, N_LAYERS, N_PRB, NH)   // (N_LAYERS, NF, NH)
                                      tensor_ref<typename complex_from_scalar<TStorageOut>::type, 4>      tCoef,       // (N_SC, N_LAYERS, N_BS_ANTS, N_PRB, NH) // (N_LAYERS, N_BS_ANTS, NF, NH)
                                      tensor_ref<typename complex_from_scalar<TStorageOut>::type, 4>      tDbg)
{
    // H is channel matrix
    // G is the enhanced Gram matrix
    // A is the augmented matrix, A = [ G | I | M ]

    //--------------------------------------------------------------------------------------------------------
    // Dimensions

    // H  : Channel matrix
    constexpr uint32_t N_ROWS_H = N_BS_ANTS;
    constexpr uint32_t N_COLS_H = N_LAYERS;

    // Rww: Noise covariance matrix
    constexpr uint32_t N_ROWS_RWW = N_BS_ANTS;
    constexpr uint32_t N_COLS_RWW = N_BS_ANTS;

    // M  : Intermediate matrix, M = H'*RwwIlw
    constexpr uint32_t N_ROWS_M = N_COLS_H;   // N_LAYERS
    constexpr uint32_t N_COLS_M = N_COLS_RWW; // N_BS_ANTS

    // G  : Enhanced Gram matrix, G = H'*RwwIlw*H + ilw(Rxx)
    constexpr uint32_t N_ROWS_G = N_LAYERS;
    constexpr uint32_t N_COLS_G = N_LAYERS;

    // U  : Upper triangular matrix
    constexpr uint32_t N_ROWS_U = N_ROWS_G;
    constexpr uint32_t N_COLS_U = N_COLS_G;

    // I  : Identity matrix, I = G*Gilw
    constexpr uint32_t N_ROWS_I = N_ROWS_G;
    constexpr uint32_t N_COLS_I = N_COLS_G;

    // Ree: Residual error covariance matrix, Ree = Gilw
    constexpr uint32_t N_ROWS_REE = N_ROWS_G;
    // constexpr uint32_t N_COLS_REE = N_COLS_G;

    // C  : MMSE coefficient matrix, C = Ree*H'*RwwIlw = Ree*M
    constexpr uint32_t N_ROWS_C = N_ROWS_REE;
    constexpr uint32_t N_COLS_C = N_COLS_M;

    // A  : Augmented result matrix, A = [ G | I | M ] -> [ U | Lilw | F ] -> [ U | Gilw | C ]
    constexpr uint32_t N_ROWS_A = N_ROWS_G;
    constexpr uint32_t N_COLS_A = N_COLS_G + N_COLS_I + N_COLS_M;

    // Column offsets to Ree (which is followed by C) matrix within the augmented matrix
    // which form the RHS of the back substition (and after back substitution the RHS is overwritten by the
    // unknowns being solved for)
    constexpr uint32_t START_COL_OFFSET_REE = N_COLS_U;

    constexpr int32_t N_ITER = N_THRD_BLK_TONES / N_TONES_PER_ITER;

    constexpr uint32_t N_INST = 2; // double buffering for pipelining

    const uint32_t N_THREADS_X = blockDim.x; // Number of threads needed to process one frequency bin
    // const uint32_t N_THREADS   = blockDim.x * blockDim.y; // N_TONES_PER_ITER == blockDim.y

    // Reciprocal of symbol energy
    const TCompute ES_ILW = lwGet<TCompute>(1);

    //--------------------------------------------------------------------------------------------------------
    // Compute indices used for element access
    const uint32_t THREAD_X_IDX = threadIdx.x;
    // const uint32_t THREAD_IDX   = (threadIdx.y * blockDim.x) + threadIdx.x;

    // There are N_TONES_PER_ITER groups of threads with each group containing N_THREADS_X per group
    const uint32_t PRB_IDX                     = blockIdx.y;
    const uint32_t THRD_GRP_FREQ_OFFSET        = threadIdx.y;
    const uint32_t THRD_BLK_START_FREQ_OFFSET  = (blockIdx.x * N_THRD_BLK_TONES);
    const uint32_t THRD_GRP_START_FREQ_OFFSET  = THRD_BLK_START_FREQ_OFFSET + THRD_GRP_FREQ_OFFSET;
    const uint32_t THRD_BLK_ABS_START_FREQ_IDX = PRB_IDX * LWPHY_N_TONES_PER_PRB;

    const uint32_t NH_IDX = 0; // @todo: add high mobility support i.e. handle multiple estimates of H in time

    // const uint32_t ROW_IDX_RWW = THREAD_IDX % N_ROWS_RWW;
    // const uint32_t COL_IDX_RWW = THREAD_IDX / N_ROWS_RWW; // COL_IDX_RWW needs a bounds check (since N_THREADS > # of Rww elements)

    const uint32_t ROW_IDX_H = THREAD_X_IDX % N_ROWS_H;
    const uint32_t COL_IDX_H = THREAD_X_IDX / N_ROWS_H;

    const uint32_t ROW_IDX_I = THREAD_X_IDX % N_ROWS_I;
    // const uint32_t COL_IDX_I = THREAD_X_IDX / N_ROWS_I;

    // const uint32_t ROW_IDX_M = THREAD_X_IDX % N_ROWS_M;
    // const uint32_t COL_IDX_M = THREAD_X_IDX / N_ROWS_M;

    const uint32_t ROW_IDX_G = THREAD_X_IDX % N_ROWS_G;
    const uint32_t COL_IDX_G = THREAD_X_IDX / N_ROWS_G; // COL_IDX_G needs a bounds check (since N_THREADS_X > # of G elements)

    // const uint32_t ROW_IDX_REE = THREAD_X_IDX % N_ROWS_REE;
    // const uint32_t COL_IDX_REE = THREAD_X_IDX / N_ROWS_REE; // COL_IDX_REE needs a bounds check (since N_THREADS_X > # of Ree elements)

    const uint32_t ROW_IDX_C = THREAD_X_IDX % N_ROWS_C;
    const uint32_t COL_IDX_C = THREAD_X_IDX / N_ROWS_C;

    const uint32_t N_COLS_H_RD_PER_ITER = N_THREADS_X / N_ROWS_H;
    const uint32_t N_ITER_TO_RD_H       = div_round_up(N_COLS_H, N_COLS_H_RD_PER_ITER);

    const uint32_t N_COLS_G_COMPUTE_PER_ITER = N_THREADS_X / N_ROWS_G;
    const uint32_t N_ITER_TO_COMPUTE_G       = div_round_up(N_COLS_G, N_COLS_G_COMPUTE_PER_ITER);

    const uint32_t N_COLS_C_WR_PER_ITER = N_THREADS_X / N_ROWS_C;
    const uint32_t N_ITER_TO_WR_C       = div_round_up(N_COLS_C, N_COLS_C_WR_PER_ITER);

    //--------------------------------------------------------------------------------------------------------
    // Shared memory allocation
    // H[N_TONES_PER_ITER*N_INST]

    // Shared memory contents as processing progresses:
    // A = [ G | I | M ] -> [ U | Lilw | F ] -> [ U | Ree | C ]

    constexpr uint32_t N_SMEM_ELEMS =
        (((N_ROWS_H + 1) * N_COLS_H * N_INST) + // (N_ROWS_H + 1) for SMEM padding to avoid bank conflicts
         (N_ROWS_A + 1) * N_COLS_A) *
            N_TONES_PER_ITER +
        (N_ROWS_RWW + 1) * N_COLS_RWW;

    typedef typename complex_from_scalar<TCompute>::type    TComplexCompute;
    typedef typename complex_from_scalar<TStorageIn>::type  TComplexStorageIn;
    typedef typename complex_from_scalar<TStorageOut>::type TComplexStorageOut;

    __shared__ TComplexCompute smemBlk[N_SMEM_ELEMS];

    constexpr uint32_t                                     SMEM_START_OFFSET_RWW = 0;
    block_2D<TComplexCompute*, N_ROWS_RWW + 1, N_COLS_RWW> shRwwIlw(&smemBlk[SMEM_START_OFFSET_RWW]);

    constexpr uint32_t                                         SMEM_START_OFFSET_H_BLK = SMEM_START_OFFSET_RWW + shRwwIlw.num_elem();
    const uint32_t                                             SMEM_START_OFFSET_H     = SMEM_START_OFFSET_H_BLK + THRD_GRP_FREQ_OFFSET * (N_ROWS_H + 1) * N_COLS_H * N_INST;
    block_3D<TComplexCompute*, N_ROWS_H + 1, N_COLS_H, N_INST> shH(&smemBlk[SMEM_START_OFFSET_H]);

    constexpr uint32_t                                 SMEM_START_OFFSET_A_BLK = SMEM_START_OFFSET_H_BLK + N_TONES_PER_ITER * shH.num_elem();
    const uint32_t                                     SMEM_START_OFFSET_A     = SMEM_START_OFFSET_A_BLK + THRD_GRP_FREQ_OFFSET * (N_ROWS_A + 1) * N_COLS_A;
    block_2D<TComplexCompute*, N_ROWS_A + 1, N_COLS_A> shA(&smemBlk[SMEM_START_OFFSET_A]);

    // SMEM overlay: A with [ G | I | M ]
    const uint32_t                                     SMEM_START_OFFSET_G = SMEM_START_OFFSET_A;
    block_2D<TComplexCompute*, N_ROWS_G + 1, N_COLS_G> shG(&smemBlk[SMEM_START_OFFSET_G]);

    const uint32_t                                     SMEM_START_OFFSET_I = SMEM_START_OFFSET_G + shG.num_elem();
    block_2D<TComplexCompute*, N_ROWS_I + 1, N_COLS_I> shI(&smemBlk[SMEM_START_OFFSET_I]);

    const uint32_t                                     SMEM_START_OFFSET_M = SMEM_START_OFFSET_I + shI.num_elem();
    block_2D<TComplexCompute*, N_ROWS_M + 1, N_COLS_M> shM(&smemBlk[SMEM_START_OFFSET_M]);

    // SMEM overlay: after LU - U replaces G, Lilw replaces I and F replaces M
    auto& shU    = shG;
    auto& shLilw = shI;
    auto& shF    = shM;

    // SMEM overlay: after back substitution - Ree replaces Lilw and C replaces F
    // (i.e. results are stored inplace)
    auto& shRee = shLilw;
    auto& shC   = shF;

    //--------------------------------------------------------------------------------------------------------
    // Stage1: Load inputs
    thread_block const& thisThrdBlk = this_thread_block();

    // Read RwwIlw (RwwIlw is used by all subcarriers within the PRB)
    block_2D<const typename complex_from_scalar<TStorageIn>::type*, N_ROWS_RWW, N_COLS_RWW> srcRwwIlw(tRwwIlw.addr + tRwwIlw.offset(0, 0, PRB_IDX, NH_IDX));
    cmplxMatLoad<TStorageIn, TCompute, N_ROWS_RWW, N_COLS_RWW>(thisThrdBlk, srcRwwIlw, shRwwIlw);

    // Prologue
    // Prefetch H into shared memory
    uint32_t       f            = 0;
    uint32_t       lwrrIdx      = 0;
    const uint32_t FREQ_IDX     = THRD_GRP_START_FREQ_OFFSET + (f * N_TONES_PER_ITER);
    const uint32_t ABS_FREQ_IDX = THRD_BLK_ABS_START_FREQ_IDX + FREQ_IDX;

#pragma unroll
    for(uint32_t i = 0; i < N_ITER_TO_RD_H; ++i)
    {
        uint32_t iCol = i * N_COLS_H_RD_PER_ITER + COL_IDX_H;
        // All threads may not participate in the last iteration
        if(iCol < N_COLS_H)
        {
            shH(ROW_IDX_H, iCol, lwrrIdx) = type_colwert<TComplexCompute>(tH(ROW_IDX_H, iCol, ABS_FREQ_IDX, NH_IDX));

#ifdef ENABLE_DEBUG
            printf("H[%d][%d] = %f+j%f\n", ROW_IDX_H, iCol, shH(ROW_IDX_H, iCol, lwrrIdx).x, shH(ROW_IDX_H, iCol, lwrrIdx).y);
#endif
        }
    }

    for(int32_t f = 0; f < N_ITER; ++f)
    {
        const uint32_t FREQ_IDX     = THRD_GRP_START_FREQ_OFFSET + (f * N_TONES_PER_ITER);
        const uint32_t ABS_FREQ_IDX = THRD_BLK_ABS_START_FREQ_IDX + FREQ_IDX;

        // A thread can process elements different from what it loaded, so wait for all threads in the thread block
        // to complete
        thisThrdBlk.sync();

        // Prefetch H for next iteration
        if(f < (N_ITER - 1))
        {
            const uint32_t NXT_FREQ_IDX     = THRD_GRP_START_FREQ_OFFSET + ((f + 1) * N_TONES_PER_ITER);
            const uint32_t NXT_ABS_FREQ_IDX = THRD_BLK_ABS_START_FREQ_IDX + NXT_FREQ_IDX;
#pragma unroll
            for(uint32_t i = 0; i < N_ITER_TO_RD_H; ++i)
            {
                uint32_t iCol = i * N_COLS_H_RD_PER_ITER + COL_IDX_H;
                // All threads may not participate in the last iteration
                if(iCol < N_COLS_H)
                {
                    shH(ROW_IDX_H, iCol, lwrrIdx ^ 1) = type_colwert<TComplexCompute>(tH(ROW_IDX_H, iCol, NXT_ABS_FREQ_IDX, NH_IDX));

#ifdef ENABLE_DEBUG
                    printf("H[%d][%d] = %f+j%f\n", ROW_IDX_H, iCol, shH(ROW_IDX_H, iCol, lwrrIdx ^ 1).x, shH(ROW_IDX_H, iCol, lwrrIdx ^ 1).y);
#endif
                }
            }
        }

        //---------------------------------------------------------------------------------------------------
        // Stage2: Compute the enhanced Gram matrix: M = H'*ilw(Rww) and G = (H'*ilw(Rww)*H + ilw(Rxx))

        // Compute M = H'*ilw(Rww): N_COLS_H x N_COLS_RWW = N_LAYERS x N_BS_ANTS
        // Select a subset of the threads in the warp to access the columns of RwwIlw and each thread computes one
        // column of M
        if(THREAD_X_IDX < N_COLS_RWW)
        {
            TComplexCompute M[N_COLS_H]{}; // Note columns of H are rows of H'

            // Each iteration of outer loop walks through a row of RwwIlw (threads in the warp accessing all columns
            // of RwwIlw) i.e. RwwIlw(i,:)
#pragma unroll
            for(uint32_t i = 0; i < N_ROWS_RWW; ++i)
            {
                TComplexCompute rwwIlw = shRwwIlw(i, THREAD_X_IDX);
                // Inner loop multiplies the entries of column i of H' - H'(:,i) (row i of conj(H) -> conj(H(i,:)))
                // with Y(i,:)
#pragma unroll
                for(uint32_t j = 0; j < N_COLS_H; ++j)
                {
                    M[j] = lwFma(lwConj(shH(i, j, lwrrIdx)), rwwIlw, M[j]);
                }
            }

            // Each thread computes one column of M
#pragma unroll
            for(uint32_t j = 0; j < N_COLS_H; ++j)
            {
                shM(j, THREAD_X_IDX) = M[j];

#ifdef ENABLE_DEBUG
                printf("M[%d][%d] = %f+j%f\n", j, THREAD_X_IDX, shM(j, THREAD_X_IDX).x, shM(j, THREAD_X_IDX).y);
#endif
                // tDbg(j,THREAD_X_IDX,ABS_FREQ_IDX,NH_IDX) =
                //    type_colwert<TComplexStorageOut>(shM(j,THREAD_X_IDX));
            }
        }

        // Wait for matrix M computation to finish, before using it in computation of G
        thisThrdBlk.sync();

        // Compute G = (M*H + RxxIlw): N_LAYERS x N_LAYERS
#pragma unroll
        for(uint32_t i = 0; i < N_ITER_TO_COMPUTE_G; ++i)
        {
            uint32_t iCol = i * N_COLS_G_COMPUTE_PER_ITER + COL_IDX_G;

            // All threads may not participate in the last iteration
            if((ROW_IDX_G < N_ROWS_G) && (iCol < N_COLS_G))
            {
                TComplexCompute G{};

#pragma unroll
                for(uint32_t elem = 0; elem < N_COLS_RWW; ++elem)
                {
                    G = lwFma(shM(ROW_IDX_G, elem), shH(elem, iCol, lwrrIdx), G);
                }

                // Add Noise_pwr*ilw(Rxx) to the diagonal of H'*H, ilw(Rxx) is assumed to be unity
                if(ROW_IDX_G == iCol)
                {
                    G += ES_ILW;
                }
                shG(ROW_IDX_G, iCol) = G;

#ifdef ENABLE_DEBUG
                printf("G[%d][%d] = %f+j%f\n", ROW_IDX_G, iCol, shG(ROW_IDX_G, iCol).x, shG(ROW_IDX_G, iCol).y);
#endif

                // tDbg(ROW_IDX_G,iCol,ABS_FREQ_IDX,NH_IDX) =
                //   type_colwert<TComplexStorageOut>(shG(ROW_IDX_G,iCol));
            }

            // Initialize matrix I (which has the same dimenison as G)
            if(iCol < N_COLS_I)
            {
                shI(ROW_IDX_I, iCol) =
                    (ROW_IDX_I != iCol) ? lwGet<TComplexCompute>(0) : lwGet<TComplexCompute>(1);
            }
        }

        // if((1 == f) && (COL_IDX_RWW < N_COLS_RWW)) tDbg(ROW_IDX_RWW,COL_IDX_RWW,PRB_IDX, NH_IDX) = shRwwIlw(ROW_IDX_RWW,COL_IDX_RWW);
        // tDbg(ROW_IDX_H,COL_IDX_H,THRD_BLK_ABS_START_FREQ_IDX+FREQ_IDX, NH_IDX) = shH(ROW_IDX_H,COL_IDX_H,lwrrIdx);
        // tDbg(ROW_IDX_M,COL_IDX_M,THRD_BLK_ABS_START_FREQ_IDX+FREQ_IDX, NH_IDX) = shM(ROW_IDX_M,COL_IDX_M);

        // Wait for matrix G computation to finish, before using it in computation of Gilw
        thisThrdBlk.sync();

        //---------------------------------------------------------------------------------------------------
        // Stage3: Perform joint LU factorization
        // A = [ G | I | M ] -> [ U | Lilw | F ]
        // where U = L\G, Lilw = L\I, F = L\M

        // eq_mmse_coef_comp_high_mimo_kernel_v2: thread block size == # of columns of augmented matrix
        // (thread block size = row length of augmented matrix = (2*N_LAYERS + N_BS_ANTS)). 
        // Thus no benefit in using the parallel version of factorization algorithm (for sub-matrix updates)
        luFactorizeIter<TCompute, N_ROWS_A, N_COLS_A>(thisThrdBlk, shA);
        // luFactorizeParallel_v1<TCompute, N_ROWS_A, N_COLS_A>(thisThrdBlk, shA);
        // luFactorizeParallel_v2<TCompute, N_ROWS_A, N_COLS_A>(thisThrdBlk, shA);

        //---------------------------------------------------------------------------------------------------
        // Stage4: Solve by back substitution, compute residual error covariance matrix Ree as the ilwerse
        // of Gram matrix: Gilw = Ree and the MMSE coefficient matrix C
        // Solve U*[ Ree | C ] = [ Lilw | F ] for Gilw and C where Gilw = U\(Lilw) and C = U\F

        backSub<TCompute, N_ROWS_U, N_COLS_U, N_ROWS_A, N_COLS_A>(thisThrdBlk, START_COL_OFFSET_REE, shU, shA);

        //--------------------------------------------------------------------------------------------------------
        // Stage5: Write the results (Ree and C) into device memory

#pragma unroll
        for(uint32_t i = 0; i < N_ITER_TO_WR_C; ++i)
        {
            uint32_t iCol = i * N_COLS_C_WR_PER_ITER + COL_IDX_C;

            // All threads may not participate in the last iteration
            if(iCol < N_COLS_C)
            {
                // Compute bias correction factor lambda and apply to coefficients
                TCompute lambda = lwGet<TCompute>(1) / (lwGet<TCompute>(1) - lwReal(shRee(ROW_IDX_C, ROW_IDX_C)));

                tCoef(iCol, FREQ_IDX, ROW_IDX_C, PRB_IDX) =
                    type_colwert<TComplexStorageOut>(shC(ROW_IDX_C, iCol) * lambda);
            }
        }

        if(THREAD_X_IDX < N_ROWS_REE)
        {
            // Compute ReeIlw while applying bias correction
            // TCompute reeIlw = (lwGet<TCompute>(1)/shMemBlkReeDiag(THREAD_X_IDX, FREQ_IDX)) - lwGet<TCompute>(1);
            TCompute ree    = lwReal(shRee(THREAD_X_IDX, THREAD_X_IDX));
            TCompute reeIlw = (lwGet<TCompute>(1) - ree) / ree;

            tReeDiagIlw(FREQ_IDX, THREAD_X_IDX, PRB_IDX) = type_colwert<TStorageOut>(reeIlw);
        }

#ifdef ENABLE_DEBUG
        printf("C[%d][%d][%d] = %f+j%f\n", ABS_FREQ_IDX, ROW_IDX_C, COL_IDX_C, shC(ROW_IDX_C, COL_IDX_C).x, shC(ROW_IDX_C, COL_IDX_C).y);
#endif
        // thisThrdBlk.sync();
        lwrrIdx ^= 1;
    }
}

// Equalization kernel for massive MIMO 
// {N_LAYERS, N_BS_ANTS} = {16,64}
// Inputs and outputs assumed to be column major
// dimBlock: (N_BS_ANTS*N_LAYERS, N_TONES_PER_ITER)
// dimGrid : ((LWPHY_N_TONES_PER_PRB/N_THRD_BLK_TONES), Nprb)
template <typename TStorageIn,
          typename TStorageOut,
          typename TCompute,
          uint32_t N_THRD_BLK_TONES, // # of frequency bins processed by a thread block
          uint32_t N_TONES_PER_ITER, // # of frequency bins processed in 1 iteration by the thread block
          uint32_t N_BS_ANTS,        // # of BS antenna (# of rows in H matrix)
          uint32_t N_LAYERS,         // # of layers (# of cols in H matrix)
          uint32_t NH>               // # of estimates of H in time
__global__ void
eq_mmse_coef_comp_massive_mimo_kernel_v1(tensor_ref<const typename complex_from_scalar<TStorageIn>::type, 4> tH,          // (N_BS_ANTS, N_LAYERS, NF, NH)
                                         tensor_ref<const typename complex_from_scalar<TStorageIn>::type, 4> tRwwIlw,     // (N_BS_ANTS, N_BS_ANTS, N_PRB, NH)
                                         tensor_ref<TStorageOut, 3>                                          tReeDiagIlw, // (N_SC, N_LAYERS, N_PRB, NH)   // (N_LAYERS, NF, NH)
                                         tensor_ref<typename complex_from_scalar<TStorageOut>::type, 4>      tCoef,       // (N_SC, N_LAYERS, N_BS_ANTS, N_PRB, NH) // (N_LAYERS, N_BS_ANTS, NF, NH)
                                         tensor_ref<typename complex_from_scalar<TStorageOut>::type, 4>      tDbg)
{
    // H is channel matrix
    // G is the enhanced Gram matrix
    // A is the augmented matrix, A = [ G | I | M ]

    //--------------------------------------------------------------------------------------------------------
    // Dimensions

    // H  : Channel matrix
    constexpr uint32_t N_ROWS_H = N_BS_ANTS;
    constexpr uint32_t N_COLS_H = N_LAYERS;

    // Rww: Noise covariance matrix
    constexpr uint32_t N_ROWS_RWW = N_BS_ANTS;
    constexpr uint32_t N_COLS_RWW = N_BS_ANTS;

    // M  : Intermediate matrix, M = H'*RwwIlw
    constexpr uint32_t N_ROWS_M = N_COLS_H;   // N_LAYERS
    constexpr uint32_t N_COLS_M = N_COLS_RWW; // N_BS_ANTS

    // G  : Enhanced Gram matrix, G = H'*RwwIlw*H + ilw(Rxx)
    constexpr uint32_t N_ROWS_G = N_LAYERS;
    constexpr uint32_t N_COLS_G = N_LAYERS;

    // U  : Upper triangular matrix
    constexpr uint32_t N_ROWS_U = N_ROWS_G;
    constexpr uint32_t N_COLS_U = N_COLS_G;

    // I  : Identity matrix, I = G*Gilw
    constexpr uint32_t N_ROWS_I = N_ROWS_G;
    constexpr uint32_t N_COLS_I = N_COLS_G;

    // Ree: Residual error covariance matrix, Ree = Gilw
    constexpr uint32_t N_ROWS_REE = N_ROWS_G;
    // constexpr uint32_t N_COLS_REE = N_COLS_G;

    // C  : MMSE coefficient matrix, C = Ree*H'*RwwIlw = Ree*M
    constexpr uint32_t N_ROWS_C = N_ROWS_REE;
    constexpr uint32_t N_COLS_C = N_COLS_M;

    // A  : Augmented result matrix, A = [ G | I | M ] -> [ U | Lilw | F ] -> [ U | Gilw | C ]
    constexpr uint32_t N_ROWS_A = N_ROWS_G;
    constexpr uint32_t N_COLS_A = N_COLS_G + N_COLS_I + N_COLS_M;

    // Column offsets to Ree (which is followed by C) matrix within the augmented matrix
    // which form the RHS of the back substition (and after back substitution the RHS is overwritten by the
    // unknowns being solved for)
    constexpr uint32_t START_COL_OFFSET_REE = N_COLS_U;

    constexpr int32_t N_ITER = N_THRD_BLK_TONES / N_TONES_PER_ITER;

    constexpr uint32_t N_INST = 1; // buffering for pipelining

    const uint32_t N_THREADS_X = blockDim.x; // Number of threads needed to process one frequency bin
    // const uint32_t N_THREADS   = blockDim.x * blockDim.y; // N_TONES_PER_ITER == blockDim.y

    const uint32_t N_COLS_M_COMPUTE_PER_ITER = N_THREADS_X / N_ROWS_M;
    const uint32_t N_ITER_TO_COMPUTE_M       = div_round_up(N_COLS_M, N_COLS_M_COMPUTE_PER_ITER);

    const uint32_t N_COLS_G_COMPUTE_PER_ITER = N_THREADS_X / N_ROWS_G;
    const uint32_t N_ITER_TO_COMPUTE_G       = div_round_up(N_COLS_G, N_COLS_G_COMPUTE_PER_ITER);

    const uint32_t N_COLS_C_WR_PER_ITER = N_THREADS_X / N_ROWS_C;
    const uint32_t N_ITER_TO_WR_C       = div_round_up(N_COLS_C, N_COLS_C_WR_PER_ITER);

    // Reciprocal of symbol energy
    const TCompute ES_ILW = lwGet<TCompute>(1);

    //--------------------------------------------------------------------------------------------------------
    // Compute indices used for element access
    const uint32_t THREAD_X_IDX = threadIdx.x;
    // const uint32_t THREAD_IDX   = (threadIdx.y * blockDim.x) + threadIdx.x;

    // There are N_TONES_PER_ITER groups of threads with each group containing N_THREADS_X per group
    const uint32_t PRB_IDX                     = blockIdx.y;
    const uint32_t THRD_GRP_FREQ_OFFSET        = threadIdx.y;
    const uint32_t THRD_BLK_START_FREQ_OFFSET  = (blockIdx.x * N_THRD_BLK_TONES);
    const uint32_t THRD_GRP_START_FREQ_OFFSET  = THRD_BLK_START_FREQ_OFFSET + THRD_GRP_FREQ_OFFSET;
    const uint32_t THRD_BLK_ABS_START_FREQ_IDX = PRB_IDX * LWPHY_N_TONES_PER_PRB;

    const uint32_t NH_IDX = 0; // @todo: add high mobility support i.e. handle multiple estimates of H in time

    // const uint32_t ROW_IDX_RWW = THREAD_IDX % N_ROWS_RWW;
    // const uint32_t COL_IDX_RWW = THREAD_IDX / N_ROWS_RWW; // COL_IDX_RWW needs a bounds check (since N_THREADS > # of Rww elements)

    // const uint32_t ROW_IDX_H = THREAD_X_IDX % N_ROWS_H;
    // const uint32_t COL_IDX_H = THREAD_X_IDX / N_ROWS_H;

    const uint32_t ROW_IDX_I = THREAD_X_IDX % N_ROWS_I;
    // const uint32_t COL_IDX_I = THREAD_X_IDX / N_ROWS_I;

    const uint32_t ROW_IDX_M = THREAD_X_IDX % N_ROWS_M;
    const uint32_t COL_IDX_M = THREAD_X_IDX / N_ROWS_M;

    const uint32_t ROW_IDX_G = THREAD_X_IDX % N_ROWS_G;
    const uint32_t COL_IDX_G = THREAD_X_IDX / N_ROWS_G; // COL_IDX_G needs a bounds check (since N_THREADS_X > # of G elements)

    // const uint32_t ROW_IDX_REE = THREAD_X_IDX % N_ROWS_REE;
    // const uint32_t COL_IDX_REE = THREAD_X_IDX / N_ROWS_REE; // COL_IDX_REE needs a bounds check (since N_THREADS_X > # of Ree elements)

    const uint32_t ROW_IDX_C = THREAD_X_IDX % N_ROWS_C;
    const uint32_t COL_IDX_C = THREAD_X_IDX / N_ROWS_C;

    //--------------------------------------------------------------------------------------------------------
    // Shared memory allocation
    // H[N_TONES_PER_ITER*N_INST]

    // Shared memory contents as processing progresses:
    // A = [ G | I | M ] -> [ U | Lilw | F ] -> [ U | Ree | C ]

    constexpr uint32_t N_SMEM_ELEMS =
        (((N_ROWS_H + 1) * N_COLS_H * N_INST) + // (N_ROWS_H + 1) for SMEM padding to avoid bank conflicts
         (N_ROWS_A + 1) * N_COLS_A) *
        N_TONES_PER_ITER;

    typedef typename complex_from_scalar<TCompute>::type    TComplexCompute;
    typedef typename complex_from_scalar<TStorageIn>::type  TComplexStorageIn;
    typedef typename complex_from_scalar<TStorageOut>::type TComplexStorageOut;

    __shared__ TComplexCompute smemBlk[N_SMEM_ELEMS];
    // extern __shared__ __align__(sizeof(TComplexCompute)) TComplexCompute smemBlk[];

#if 0
    constexpr uint32_t SMEM_START_OFFSET_RWW = 0;

    block_2D<TComplexCompute*, N_ROWS_RWW + 1, N_COLS_RWW> shRwwIlw(&smemBlk[SMEM_START_OFFSET_RWW]);

    constexpr uint32_t SMEM_START_OFFSET_H_BLK = SMEM_START_OFFSET_RWW + shRwwIlw.num_elem();
    const uint32_t     SMEM_START_OFFSET_H     = SMEM_START_OFFSET_H_BLK + THRD_GRP_FREQ_OFFSET * (N_ROWS_H + 1) * N_COLS_H * N_INST;
#endif
    constexpr uint32_t SMEM_START_OFFSET_H_BLK = 0;
    const uint32_t     SMEM_START_OFFSET_H     = SMEM_START_OFFSET_H_BLK + THRD_GRP_FREQ_OFFSET * (N_ROWS_H + 1) * N_COLS_H * N_INST;

    block_3D<TComplexCompute*, N_ROWS_H + 1, N_COLS_H, N_INST> shH(&smemBlk[SMEM_START_OFFSET_H]);

    constexpr uint32_t SMEM_START_OFFSET_A_BLK = SMEM_START_OFFSET_H_BLK + N_TONES_PER_ITER * shH.num_elem();
    const uint32_t     SMEM_START_OFFSET_A     = SMEM_START_OFFSET_A_BLK + THRD_GRP_FREQ_OFFSET * (N_ROWS_A + 1) * N_COLS_A;

    block_2D<TComplexCompute*, N_ROWS_A + 1, N_COLS_A> shA(&smemBlk[SMEM_START_OFFSET_A]);

    // SMEM overlay: A with [ G | I | M ]
    const uint32_t SMEM_START_OFFSET_G = SMEM_START_OFFSET_A;

    block_2D<TComplexCompute*, N_ROWS_G + 1, N_COLS_G> shG(&smemBlk[SMEM_START_OFFSET_G]);

    const uint32_t SMEM_START_OFFSET_I = SMEM_START_OFFSET_G + shG.num_elem();

    block_2D<TComplexCompute*, N_ROWS_I + 1, N_COLS_I>
        shI(&smemBlk[SMEM_START_OFFSET_I]);

    const uint32_t SMEM_START_OFFSET_M = SMEM_START_OFFSET_I + shI.num_elem();

    block_2D<TComplexCompute*, N_ROWS_M + 1, N_COLS_M>
        shM(&smemBlk[SMEM_START_OFFSET_M]);

    // SMEM overlay: after LU - U replaces G, Lilw replaces I and F replaces M
    auto& shU    = shG;
    auto& shLilw = shI;
    auto& shF    = shM;

    // SMEM overlay: after back substitution - Ree replaces Lilw and C replaces F
    // (i.e. results are stored inplace)
    auto& shRee = shLilw;
    auto& shC   = shF;

    //--------------------------------------------------------------------------------------------------------
    // Stage1: Load inputs
    thread_block const& thisThrdBlk = this_thread_block();

#ifdef ENABLE_DEBUG
    if(0 != blockIdx.x) return;
#endif

    // Prologue
    // Prefetch H for first iteration
    uint32_t f       = 0;
    uint32_t lwrrIdx = 0;

    const uint32_t FREQ_IDX     = THRD_GRP_START_FREQ_OFFSET + (f * N_TONES_PER_ITER);
    const uint32_t ABS_FREQ_IDX = THRD_BLK_ABS_START_FREQ_IDX + FREQ_IDX;

    block_2D<const typename complex_from_scalar<TStorageIn>::type*, N_ROWS_H, N_COLS_H> srcH(tH.addr + tH.offset(0, 0, ABS_FREQ_IDX, NH_IDX));
    block_2D<TComplexCompute*, N_ROWS_H + 1, N_COLS_H>                                  dstH(&shH(0, 0, lwrrIdx));
    cmplxMatLoad<TStorageIn, TCompute, N_ROWS_H, N_COLS_H>(thisThrdBlk, srcH, dstH);

    for(int32_t f = 0; f < N_ITER; ++f)
    {
        const uint32_t FREQ_IDX     = THRD_GRP_START_FREQ_OFFSET + (f * N_TONES_PER_ITER);
        const uint32_t ABS_FREQ_IDX = THRD_BLK_ABS_START_FREQ_IDX + FREQ_IDX;

        // A thread can process elements different from what it loaded, so wait for all threads in the thread block
        // to complete
        thisThrdBlk.sync();

        // Prefetch H for next iteration
        if(f < (N_ITER - 1))
        {
            const uint32_t NXT_FREQ_IDX     = THRD_GRP_START_FREQ_OFFSET + ((f + 1) * N_TONES_PER_ITER);
            const uint32_t NXT_ABS_FREQ_IDX = THRD_BLK_ABS_START_FREQ_IDX + NXT_FREQ_IDX;

            block_2D<const typename complex_from_scalar<TStorageIn>::type*, N_ROWS_H, N_COLS_H> srcH(tH.addr + tH.offset(0, 0, NXT_ABS_FREQ_IDX, NH_IDX));
            block_2D<TComplexCompute*, N_ROWS_H + 1, N_COLS_H>                                  dstH(&shH(0, 0, lwrrIdx ^ 1));
            cmplxMatLoad<TStorageIn, TCompute, N_ROWS_H, N_COLS_H>(thisThrdBlk, srcH, dstH);
        }

        //---------------------------------------------------------------------------------------------------
        // Stage2: Compute the enhanced Gram matrix: M = H'*ilw(Rww) and G = (H'*ilw(Rww)*H + ilw(Rxx))

        // Compute M = H'*ilw(Rww): N_COLS_H x N_COLS_RWW = N_LAYERS x N_BS_ANTS
#pragma unroll
        for(uint32_t i = 0; i < N_ITER_TO_COMPUTE_M; ++i)
        {
            uint32_t iCol = i * N_COLS_M_COMPUTE_PER_ITER + COL_IDX_M;
            // All threads may not participate in the last iteration
            if((ROW_IDX_M < N_ROWS_M) && (iCol < N_COLS_M))
            {
                TComplexCompute prod{};

                // Compute H'*ilw(Rww)
#pragma unroll
                for(uint32_t elem = 0; elem < N_ROWS_RWW; ++elem)
                {
                    TComplexCompute rwwIlw = type_colwert<TComplexCompute>(tRwwIlw(elem, iCol, PRB_IDX, NH_IDX));
                    prod                   = lwFma(lwConj(shH(elem, ROW_IDX_M, lwrrIdx)), rwwIlw, prod);
                    // prod = lwFma(lwConj(shH(elem, ROW_IDX_M, lwrrIdx)), shRwwIlw, prod);
                }
                shM(ROW_IDX_M, iCol) = prod;

#ifdef ENABLE_DEBUG
                printf("M[%d][%d] = %f+j%f\n", ROW_IDX_M, iCol, shM(ROW_IDX_M, iCol).x, shM(ROW_IDX_M, iCol).y);
#endif
            }
        }

        // Wait for matrix M computation to finish, before using it in computation of G
        thisThrdBlk.sync();

        // if((1 == f) && (COL_IDX_RWW < N_COLS_RWW)) tDbg(ROW_IDX_RWW,COL_IDX_RWW,PRB_IDX, NH_IDX) = shRwwIlw(ROW_IDX_RWW,COL_IDX_RWW);
        // tDbg(ROW_IDX_H,COL_IDX_H,THRD_BLK_ABS_START_FREQ_IDX+FREQ_IDX, NH_IDX) = shH(ROW_IDX_H,COL_IDX_H,lwrrIdx);
        // tDbg(ROW_IDX_M,COL_IDX_M,THRD_BLK_ABS_START_FREQ_IDX+FREQ_IDX, NH_IDX) = shM(ROW_IDX_M,COL_IDX_M);

        // Compute G = (M*H + RxxIlw): N_LAYERS x N_LAYERS
#pragma unroll
        for(uint32_t i = 0; i < N_ITER_TO_COMPUTE_G; ++i)
        {
            uint32_t iCol = i * N_COLS_G_COMPUTE_PER_ITER + COL_IDX_G;

            // All threads may not participate in the last iteration
            if((ROW_IDX_G < N_ROWS_G) && (iCol < N_COLS_G))
            {
                TComplexCompute G{};

#pragma unroll
                for(uint32_t elem = 0; elem < N_COLS_RWW; ++elem)
                {
                    G = lwFma(shM(ROW_IDX_G, elem), shH(elem, iCol, lwrrIdx), G);
                }

                // Add Noise_pwr*ilw(Rxx) to the diagonal of H'*H, ilw(Rxx) is assumed to be unity
                if(ROW_IDX_G == iCol)
                {
                    G += ES_ILW;
                }
                shG(ROW_IDX_G, iCol) = G;

#ifdef ENABLE_DEBUG
                printf("G[%d][%d] = %f+j%f\n", ROW_IDX_G, iCol, shG(ROW_IDX_G, iCol).x, shG(ROW_IDX_G, iCol).y);
#endif

                // tDbg(ROW_IDX_G,iCol,ABS_FREQ_IDX,NH_IDX) =
                //   type_colwert<TComplexStorageOut>(shG(ROW_IDX_G,iCol));
            }

            // Initialize matrix I (which has the same dimenison as G)
            if(iCol < N_COLS_I)
            {
                shI(ROW_IDX_I, iCol) =
                    (ROW_IDX_I != iCol) ? lwGet<TComplexCompute>(0) : lwGet<TComplexCompute>(1);
            }
        }

        // Wait for matrix G computation to finish, before using it in computation of Gilw
        thisThrdBlk.sync();

#ifdef ENABLE_DEBUG
        // A0
        for(uint32_t i = 0; i < N_ROWS_A; ++i)
        {
            if(THREAD_X_IDX < N_COLS_A)
                tDbg(i, THREAD_X_IDX, THRD_BLK_ABS_START_FREQ_IDX + FREQ_IDX, NH_IDX) = type_colwert<TComplexStorageOut>(shA(i, THREAD_X_IDX));
        }
#endif

        //---------------------------------------------------------------------------------------------------
        // Stage3: Perform joint LU factorization
        // A = [ G | I | M ] -> [ U | Lilw | F ]
        // where U = L\G, Lilw = L\I, F = L\M

        // eq_mmse_coef_comp_massive_mimo_kernel_v1: thread block size >> # of columns of augmented matrix
        // (i.e. (N_LAYERS * N_LAYERS) >> (2*N_LAYERS + N_BS_ANTS)). Thus use parallel version of the
        // factorization algorithm to cut down iteration count and increase active threads during sub-matrix
        // updates
        // luFactorizeIter<TCompute, N_ROWS_A, N_COLS_A>(thisThrdBlk, shA);
        luFactorizeParallel_v2<TCompute, N_ROWS_A, N_COLS_A>(thisThrdBlk, shA);
#ifdef ENABLE_DEBUG
        // A1
        for(uint32_t i = 0; i < N_ROWS_A; ++i)
        {
            if(THREAD_X_IDX < N_COLS_A)
                tDbg(i, THREAD_X_IDX, THRD_BLK_ABS_START_FREQ_IDX + FREQ_IDX, NH_IDX) = type_colwert<TComplexStorageOut>(shA(i, THREAD_X_IDX));
        }
#endif

        //---------------------------------------------------------------------------------------------------
        // Stage4: Solve by back substitution, compute residual error covariance matrix Ree as the ilwerse
        // of Gram matrix: Gilw = Ree and the MMSE coefficient matrix C
        // Solve U*[ Ree | C ] = [ Lilw | F ] for Gilw and C where Gilw = U\(Lilw) and C = U\F

        backSub<TCompute, N_ROWS_U, N_COLS_U, N_ROWS_A, N_COLS_A>(thisThrdBlk, START_COL_OFFSET_REE, shU, shA);
#ifdef ENABLE_DEBUG
        // A2
        for(uint32_t i = 0; i < N_ROWS_A; ++i)
        {
            if(THREAD_X_IDX < N_COLS_A)
                tDbg(i, THREAD_X_IDX, THRD_BLK_ABS_START_FREQ_IDX + FREQ_IDX, NH_IDX) = type_colwert<TComplexStorageOut>(shA(i, THREAD_X_IDX));
        }
#endif

        //--------------------------------------------------------------------------------------------------------
        // Stage5: Write the results (Ree and C) into device memory
#pragma unroll
        for(uint32_t i = 0; i < N_ITER_TO_WR_C; ++i)
        {
            uint32_t iCol = i * N_COLS_C_WR_PER_ITER + COL_IDX_C;

            // All threads may not participate in the last iteration
            if(iCol < N_COLS_C)
            {
                // Compute bias correction factor lambda and apply to coefficients
                TCompute lambda = lwGet<TCompute>(1) / (lwGet<TCompute>(1) - lwReal(shRee(ROW_IDX_C, ROW_IDX_C)));

                tCoef(iCol, FREQ_IDX, ROW_IDX_C, PRB_IDX) =
                    type_colwert<TComplexStorageOut>(shC(ROW_IDX_C, iCol) * lambda);
            }
        }

        if(THREAD_X_IDX < N_ROWS_REE)
        {
            // Compute ReeIlw while applying bias correction
            // TCompute reeIlw = (lwGet<TCompute>(1)/shMemBlkReeDiag(THREAD_X_IDX, FREQ_IDX)) - lwGet<TCompute>(1);
            TCompute ree    = lwReal(shRee(THREAD_X_IDX, THREAD_X_IDX));
            TCompute reeIlw = (lwGet<TCompute>(1) - ree) / ree;

            tReeDiagIlw(FREQ_IDX, THREAD_X_IDX, PRB_IDX) = type_colwert<TStorageOut>(reeIlw);
        }

#ifdef ENABLE_DEBUG
        printf("C[%d][%d][%d] = %f+j%f\n", ABS_FREQ_IDX, ROW_IDX_C, COL_IDX_C, shC(ROW_IDX_C, COL_IDX_C).x, shC(ROW_IDX_C, COL_IDX_C).y);
#endif
        lwrrIdx ^= 1;
    }
}

// Equalizer coefficient compute kernel for low order MIMO per PRB
// {N_LAYERS, N_BS_ANTS} = {1,2}, {2,2}, {1,4}, {2,4}, {4,4}, {1,8}, {2,8} and {4,8}
// Inputs and outputs assumed to be column major
// dimBlock: (8,N_FREQ_BINS_PER_ITER) for N_LAYERS = 2, N_BS_ANTS = 4; (32,N_FREQ_BINS_PER_ITER) for N_LAYERS = 4, N_BS_ANTS = 8
//           N_FREQ_BINS_PER_ITER = 4
//           Essentially, there are N_FREQ_BINS_PER_ITER group of threads, each thread group contains:
//           8 threads for N_LAYERS = 2 and 32 threads for N_LAYERS = 4
// dimGrid : Nprb
template <typename TStorageIn,
          typename TStorageOut,
          typename TCompute,
          uint32_t N_FREQ_BINS_PER_ITER, // # of frequency bins processed in 1 iteration by the thread block
          uint32_t N_BS_ANTS,            // # of BS antenna (# of rows in H matrix)
          uint32_t N_LAYERS,             // # of layers (# of cols in H matrix)
          uint32_t NH>                   // # of estimates of H in time
__global__ void
eq_mmse_coef_comp_low_mimo_kernel(tensor_ref<const typename complex_from_scalar<TStorageIn>::type, 4> tH,          // (N_BS_ANTS, N_LAYERS, NF, NH)
                                  tensor_ref<const typename complex_from_scalar<TStorageIn>::type, 4> tRwwIlw,     // (N_BS_ANTS, N_BS_ANTS, N_PRB, NH)
                                  tensor_ref<TStorageOut, 3>                                          tReeDiagIlw, // (N_SC, N_LAYERS, N_PRB, NH)   // (N_LAYERS, NF, NH)
                                  tensor_ref<typename complex_from_scalar<TStorageOut>::type, 4>      tCoef,       // (N_SC, N_LAYERS, N_BS_ANTS, N_PRB, NH) // (N_LAYERS, N_BS_ANTS, NF, NH)
                                  tensor_ref<typename complex_from_scalar<TStorageOut>::type, 4>      tDbg)
// void eqMMSECoefComputeLowMIMOKernel
{
    //--------------------------------------------------------------------------------------------------------
    // Dimensions

    // H  : Channel matrix
    constexpr uint32_t N_ROWS_H = N_BS_ANTS;
    constexpr uint32_t N_COLS_H = N_LAYERS;

    // Rww: Noise covariance matrix
    constexpr uint32_t N_ROWS_RWW = N_BS_ANTS;
    constexpr uint32_t N_COLS_RWW = N_BS_ANTS;

    // M  : Intermediate matrix, M = H'*RwwIlw
    constexpr uint32_t N_ROWS_M = N_COLS_H;   // N_LAYERS
    constexpr uint32_t N_COLS_M = N_COLS_RWW; // N_BS_ANTS

    // G  : Enhanced Gram matrix, G = H'*RwwIlw*H + ilw(Rxx)
    constexpr uint32_t N_ROWS_G = N_LAYERS;
    constexpr uint32_t N_COLS_G = N_LAYERS;

    // DU : Upper Triangular + Diagonal matrix in G = U'*(D*U)
    // constexpr uint32_t N_ROWS_DU = N_ROWS_G;
    // constexpr uint32_t N_COLS_DU = N_COLS_G;

    // Ree: Residual error covariance matrix, Ree = GIlw
    constexpr uint32_t N_ROWS_REE = N_ROWS_G;
    constexpr uint32_t N_COLS_REE = N_COLS_G;

    // C  : MMSE coefficient matrix, C = Ree*H'*RwwIlw = Ree*M
    constexpr uint32_t N_ROWS_C = N_ROWS_REE;
    constexpr uint32_t N_COLS_C = N_COLS_M;

    // I  : Identity matrix, I = G*GIlw
    constexpr uint32_t N_ROWS_I = N_ROWS_G;
    constexpr uint32_t N_COLS_I = N_COLS_G;

    // J  : Intermediate matrix used in Ree = GIlw computation, J = D*U*GIlw
    // constexpr uint32_t N_ROWS_J = N_ROWS_DU;
    // constexpr uint32_t N_COLS_J = N_COLS_G;

    // K  : Intermediate matrix used in C (MMSE coefficients) computation, K = D*U*C
    // constexpr uint32_t N_ROWS_K = N_ROWS_DU;
    // constexpr uint32_t N_COLS_K = N_COLS_C;

    // A  : Augmented result matrix, A = [I | M] -> [J | K] -> [Ree | C]
    constexpr uint32_t N_ROWS_A = N_ROWS_I;
    constexpr uint32_t N_COLS_A = N_COLS_I + N_COLS_M;

    // Need to compute:
    // Residual error covariance matrix Ree = GIlw and
    // MMSE coefficients C = Ree*H'*RwwIlw = Ree*M

    // a. Factorize G = U'*D*U (transforms G to DU)

    // G*GIlw = I => U'*D*U*GIlw = I
    // C = Ree*M = GIlw*M => G*C = M => U'*D*U*C = M

    // concatenating the two problems:
    // U'*D*U*[GIlw | C] = [I | M]

    // Set D*U*GIlw = J, D*U*C = K
    // b. Forward substitution : U'*[J    | K] = [I | M], solve for J and K
    // c. Backward substitution: DU*[GIlw | C] = [J | K], solve for GIlw and C

    // const uint32_t N_THREADS_X = blockDim.x;
    // const uint32_t N_THREADS = blockDim.x * blockDim.y; // N_FREQ_BINS_PER_ITER == blockDim.y

    static_assert(N_LAYERS <= N_BS_ANTS, "Recevied layer count should atmost equal base station antenna count");
    static_assert(0 == (LWPHY_N_TONES_PER_PRB % N_FREQ_BINS_PER_ITER), "Number of freq bins processed per iter must be a multiple of PRB size");
    constexpr uint32_t N_ITER_PER_PRB = LWPHY_N_TONES_PER_PRB / N_FREQ_BINS_PER_ITER;

    // Reciprocal of symbol energy
    const TCompute ES_ILW = lwGet<TCompute>(1);

    //--------------------------------------------------------------------------------------------------------
    // Compute indices used for element access
    const uint32_t THREAD_X_IDX = threadIdx.x;
    // const uint32_t THREAD_IDX   = (threadIdx.y * blockDim.x) + threadIdx.x;

    // There are N_FREQ_BINS_PER_ITER groups of threads with each group containing N_THREADS_X per group
    const uint32_t PRB_IDX                     = blockIdx.x;
    const uint32_t THRD_GRP_FREQ_OFFSET        = threadIdx.y;
    const uint32_t THRD_BLK_ABS_START_FREQ_IDX = PRB_IDX * LWPHY_N_TONES_PER_PRB;

    const uint32_t NH_IDX = 0; // @todo: add high mobility support i.e. handle multiple estimates of H in time

    // const uint32_t ROW_IDX_RWW = THREAD_IDX % N_ROWS_RWW;
    // const uint32_t COL_IDX_RWW = THREAD_IDX / N_ROWS_RWW; // COL_IDX_RWW needs a bounds check (since N_THREADS > # of Rww elements)

    const uint32_t ROW_IDX_H = THREAD_X_IDX % N_ROWS_H;
    const uint32_t COL_IDX_H = THREAD_X_IDX / N_ROWS_H;

    const uint32_t ROW_IDX_I = THREAD_X_IDX % N_ROWS_I;
    const uint32_t COL_IDX_I = THREAD_X_IDX / N_ROWS_I;

    const uint32_t ROW_IDX_M = THREAD_X_IDX % N_ROWS_M;
    const uint32_t COL_IDX_M = THREAD_X_IDX / N_ROWS_M;

    const uint32_t ROW_IDX_G = THREAD_X_IDX % N_ROWS_G;
    const uint32_t COL_IDX_G = THREAD_X_IDX / N_ROWS_G; // COL_IDX_G needs a bounds check (since N_THREADS_X > # of G elements)

    // const uint32_t ROW_IDX_REE = THREAD_X_IDX % N_ROWS_REE;
    // const uint32_t COL_IDX_REE = THREAD_X_IDX / N_ROWS_REE; // COL_IDX_REE needs a bounds check (since N_THREADS_X > # of Ree elements)

    const uint32_t ROW_IDX_C = THREAD_X_IDX % N_ROWS_C;
    const uint32_t COL_IDX_C = THREAD_X_IDX / N_ROWS_C;

    //--------------------------------------------------------------------------------------------------------
    // Shared memory allocation

    // SMEM allocation: C[N_TONES_PER_PRB]        , ReeDiag[N_TONES_PER_PRB],
    //                  Rww[1]                    ,
    //                  DIlw[N_FREQ_BINS_PER_ITER], A[N_FREQ_BINS_PER_ITER]    , G[N_FREQ_BINS_PER_ITER]
    // @tdoo: ReeDiag and DIlw are of type TCompute, consider allocating in TCompute and castinf to TComplexCompute
    constexpr uint32_t N_SMEM_ELEMS =
        (((N_ROWS_C + 1) * N_COLS_C) + N_ROWS_REE) * LWPHY_N_TONES_PER_PRB +
        ((N_ROWS_RWW + 1) * N_COLS_RWW) +
        (N_LAYERS + ((N_ROWS_G + 1) * N_COLS_G) + ((N_ROWS_A + 1) * N_COLS_A)) * N_FREQ_BINS_PER_ITER;

    typedef typename complex_from_scalar<TCompute>::type    TComplexCompute;
    typedef typename complex_from_scalar<TStorageIn>::type  TComplexStorageIn;
    typedef typename complex_from_scalar<TStorageOut>::type TComplexStorageOut;

    __shared__ TComplexCompute smemBlk[N_SMEM_ELEMS];

    // H overlays C (note: while H and C both have the same size they have different dimensions. Since
    // N_COLS_C > N_COLS_H, padding the rows of C by 1 makes C bigger overall, thus H start is aligned to C
    // start boundary and not vice-versa)
    constexpr uint32_t                                                        SMEM_START_OFFSET_C_BLK = 0;
    block_3D<TComplexCompute*, N_ROWS_C + 1, N_COLS_C, LWPHY_N_TONES_PER_PRB> shMemBlkC(&smemBlk[SMEM_START_OFFSET_C_BLK]);

    constexpr uint32_t                                     SMEM_START_OFFSET_REE_DIAG_BLK = SMEM_START_OFFSET_C_BLK + shMemBlkC.num_elem();
    block_2D<TCompute*, N_ROWS_REE, LWPHY_N_TONES_PER_PRB> shMemBlkReeDiag(reinterpret_cast<TCompute*>(&smemBlk[SMEM_START_OFFSET_REE_DIAG_BLK]));

    // Rww shared across all frequency bins in PRB
    constexpr uint32_t                                     SMEM_START_OFFSET_RWW = SMEM_START_OFFSET_REE_DIAG_BLK + shMemBlkReeDiag.num_elem();
    block_2D<TComplexCompute*, N_ROWS_RWW + 1, N_COLS_RWW> shRwwIlw(&smemBlk[SMEM_START_OFFSET_RWW]);

    const uint32_t                SMEM_START_OFFSET_DILW_BLK = SMEM_START_OFFSET_RWW + shRwwIlw.num_elem();
    const uint32_t                SMEM_START_OFFSET_DILW     = SMEM_START_OFFSET_DILW_BLK + THRD_GRP_FREQ_OFFSET * N_LAYERS;
    block_1D<TCompute*, N_LAYERS> shDIlw(reinterpret_cast<TCompute*>(&smemBlk[SMEM_START_OFFSET_DILW]));

    const uint32_t                                     SMEM_START_OFFSET_G_BLK = SMEM_START_OFFSET_DILW_BLK + N_FREQ_BINS_PER_ITER * shDIlw.num_elem();
    const uint32_t                                     SMEM_START_OFFSET_G     = SMEM_START_OFFSET_G_BLK + THRD_GRP_FREQ_OFFSET * (N_ROWS_G + 1) * N_COLS_G;
    block_2D<TComplexCompute*, N_ROWS_G + 1, N_COLS_G> shG(&smemBlk[SMEM_START_OFFSET_G]);

    const uint32_t                                     SMEM_START_OFFSET_A_BLK = SMEM_START_OFFSET_G_BLK + N_FREQ_BINS_PER_ITER * shG.num_elem();
    const uint32_t                                     SMEM_START_OFFSET_A     = SMEM_START_OFFSET_A_BLK + THRD_GRP_FREQ_OFFSET * (N_ROWS_A + 1) * N_COLS_A;
    block_2D<TComplexCompute*, N_ROWS_A + 1, N_COLS_A> shA(&smemBlk[SMEM_START_OFFSET_A]);

    // I and M overlay on A
    const uint32_t                                     SMEM_START_OFFSET_I = SMEM_START_OFFSET_A;
    block_2D<TComplexCompute*, N_ROWS_I + 1, N_COLS_I> shI(&smemBlk[SMEM_START_OFFSET_I]);

    const uint32_t                                     SMEM_START_OFFSET_M = SMEM_START_OFFSET_I + shI.num_elem();
    block_2D<TComplexCompute*, N_ROWS_M + 1, N_COLS_M> shM(&smemBlk[SMEM_START_OFFSET_M]);

    // SMEM overlays
    auto& shDU = shG; // G and DU have the same dimension

    // Overlays on A: A = [I | M] -> [J | K] -> [Ree | C]
    // I, J and Ree have the same dimension and are overlaid
    // M, K and C have the same dimension and are overlaid
    // auto& shJ   = shI;
    // auto& shRee = shJ;

    // auto& shK   = shM;
    // auto& shC   = shK;

    thread_block const& thisThrdBlk = this_thread_block();

#ifdef ENABLE_DEBUG
    if(0 != blockIdx.x) return;
#endif

    //--------------------------------------------------------------------------------------------------------
    // 1. Prefetch H and RwwIlw into shared memory

    // Read RwwIlw (RwwIlw is used by all subcarriers within the PRB)
    block_2D<const typename complex_from_scalar<TStorageIn>::type*, N_ROWS_RWW, N_COLS_RWW> srcRwwIlw(tRwwIlw.addr + tRwwIlw.offset(0, 0, PRB_IDX, NH_IDX));
    cmplxMatLoad<TStorageIn, TCompute, N_ROWS_RWW, N_COLS_RWW>(thisThrdBlk, srcRwwIlw, shRwwIlw);

    // Read H for whole PRB in one burst. Each N_THREADS_X group of threads can read one H matrix
#pragma unroll
    for(uint32_t f = 0; f < N_ITER_PER_PRB; ++f)
    {
        const uint32_t FREQ_IDX     = (f * N_FREQ_BINS_PER_ITER + THRD_GRP_FREQ_OFFSET);
        const uint32_t ABS_FREQ_IDX = THRD_BLK_ABS_START_FREQ_IDX + FREQ_IDX;

        // SMEM overlay, align H start to C start boundary
        const uint32_t                                     SMEM_START_OFFSET_C = SMEM_START_OFFSET_C_BLK + FREQ_IDX * (N_ROWS_C + 1) * N_COLS_C;
        const uint32_t                                     SMEM_START_OFFSET_H = SMEM_START_OFFSET_C;
        block_2D<TComplexCompute*, N_ROWS_H + 1, N_COLS_H> shH(&smemBlk[SMEM_START_OFFSET_H]);

        if(COL_IDX_H < N_COLS_H)
        {
            shH(ROW_IDX_H, COL_IDX_H) =
                type_colwert<TComplexCompute>(tH(ROW_IDX_H, COL_IDX_H, ABS_FREQ_IDX, NH_IDX));

            // tDbg(ROW_IDX_H,COL_IDX_H,ABS_FREQ_IDX, NH_IDX) = shH(ROW_IDX_H,COL_IDX_H);
#ifdef ENABLE_DEBUG
            printf("H[%d][%d][%d] = %f+j%f\n", ABS_FREQ_IDX, ROW_IDX_H, COL_IDX_H, shMemBlkH(ROW_IDX_H, COL_IDX_H, FREQ_IDX).x, shMemBlkH(ROW_IDX_H, COL_IDX_H, FREQ_IDX).y);
#endif
        }
    }

    // Wait for loads to complete. Thread(s) processing an entry of H or Rww may not be the same ones loading
    // it
    thisThrdBlk.sync();

    for(uint32_t f = 0; f < N_ITER_PER_PRB; ++f)
    {
        const uint32_t FREQ_IDX = (f * N_FREQ_BINS_PER_ITER + THRD_GRP_FREQ_OFFSET);

        const uint32_t                                     SMEM_START_OFFSET_C = SMEM_START_OFFSET_C_BLK + FREQ_IDX * (N_ROWS_C + 1) * N_COLS_C;
        block_2D<TComplexCompute*, N_ROWS_C + 1, N_COLS_C> shCRes(&smemBlk[SMEM_START_OFFSET_C]);

        // SMEM overlay, align H start to C start boundary
        const uint32_t                                     SMEM_START_OFFSET_H = SMEM_START_OFFSET_C;
        block_2D<TComplexCompute*, N_ROWS_H + 1, N_COLS_H> shH(&smemBlk[SMEM_START_OFFSET_H]);

        //---------------------------------------------------------------------------------------------------
        // 2. Compute enhanced Gram matrix: G = (H'*RwwIlw*H + RxxIlw)

        // Compute intermediate matrix M = H'*RwwIlw
        if(COL_IDX_M < N_COLS_M)
        {
            TComplexCompute M = lwGet<TComplexCompute>(0);

#pragma unroll
            for(uint32_t i = 0; i < N_ROWS_RWW; ++i)
            {
                M = lwFma(lwConj(shH(i, ROW_IDX_M)), shRwwIlw(i, COL_IDX_M), M);
#ifdef ENABLE_DEBUG
                if(1 == FREQ_IDX) printf("Iter %d: M[%d][%d][%d] = %f+j%f conjH[%d][%d] = %f+j%f RwwIlw[%d][%d] = %f+j%f\n", i, FREQ_IDX, ROW_IDX_M, COL_IDX_M, shM(ROW_IDX_M, COL_IDX_M).x, shM(ROW_IDX_M, COL_IDX_M).y, ROW_IDX_M, i, lwReal(lwConj(shH(i, ROW_IDX_M))), lwImag(lwConj(shH(i, ROW_IDX_M))), i, COL_IDX_M, shRwwIlw(i, COL_IDX_M).x, shRwwIlw(i, COL_IDX_M).y);
#endif
            }
            shM(ROW_IDX_M, COL_IDX_M) = M;
        }

        // Wait for matrix M computation to finish, before using it in computation of G
        thisThrdBlk.sync();

        // if((1 == f) && (COL_IDX_RWW < N_COLS_RWW)) tDbg(ROW_IDX_RWW,COL_IDX_RWW,PRB_IDX, NH_IDX) = shRwwIlw(ROW_IDX_RWW,COL_IDX_RWW);
        // tDbg(ROW_IDX_H,COL_IDX_H,THRD_BLK_ABS_START_FREQ_IDX+FREQ_IDX, NH_IDX) = shH(ROW_IDX_H,COL_IDX_H);
        // tDbg(ROW_IDX_M,COL_IDX_M,THRD_BLK_ABS_START_FREQ_IDX+FREQ_IDX, NH_IDX) = shM(ROW_IDX_M,COL_IDX_M);

        // Compute G = (M*H + RxxIlw)
        if(COL_IDX_G < N_COLS_G)
        {
            TComplexCompute G = lwGet<TComplexCompute>(0);

#pragma unroll
            for(uint32_t i = 0; i < N_ROWS_H; ++i)
            {
                G = lwFma(shM(ROW_IDX_G, i), shH(i, COL_IDX_G), G);
            }

            if(ROW_IDX_G == COL_IDX_G)
            {
                G += ES_ILW;
            }
            shG(ROW_IDX_G, COL_IDX_G) = G;

#ifdef ENABLE_DEBUG
            printf("After: M[%d][%d][%d] = %f+j%f\n", FREQ_IDX, ROW_IDX_M, COL_IDX_M, shM(ROW_IDX_M, COL_IDX_M).x, shM(ROW_IDX_M, COL_IDX_M).y);
            printf("G[%d][%d][%d] = %f+j%f\n", FREQ_IDX, ROW_IDX_G, COL_IDX_G, shG(ROW_IDX_G, COL_IDX_G).x, shG(ROW_IDX_G, COL_IDX_G).y);
#endif

            // tDbg(ROW_IDX_G,COL_IDX_G,THRD_BLK_ABS_START_FREQ_IDX+FREQ_IDX, NH_IDX) = shG(ROW_IDX_G,COL_IDX_G);
        }

        // Wait for matrix G computation to finish, before using it in computation of GIlw
        thisThrdBlk.sync();

        //-----------------------------------------------------------------------------------------------
        // 3. Compute residual error covariance matrix Ree as the ilwerse of Gram matrix: GIlw = Ree

        // 3a. LDL factorization: G = U'*D*U
        // Single threaded section (only N_FREQ_BINS_PER_ITER threads are active inside the if condition)

        if(0 == THREAD_X_IDX)
        {
#pragma unroll
            for(int32_t i = 0; i < N_LAYERS; ++i)
            {
                // compute ith diagonal entry of diagonal matrix D
                TCompute sum1 = lwGet<TCompute>(0);
                for(int32_t j = 0; j < i; ++j)
                {
                    sum1 += (lwReal(shDU(j, j)) * lwReal(lwConj(shDU(j, i)) * shDU(j, i)));

#ifdef ENABLE_DEBUG
                    printf("FREQ_IDX %d Row %d Iter %d DU[%d][%d][%d] = %f+j%f, DU[%d][%d][%d] = %f+j%f, sum1 = %f\n", FREQ_IDX, i, j, FREQ_IDX, j, j, shDU(j, j).x, shDU(j, j).y, FREQ_IDX, j, i, shDU(j, i).x, shDU(j, i).y, sum1);
#endif
                }
                shDU(i, i) = lwGet<TComplexCompute>(lwReal(shG(i, i)) - sum1);
                shDIlw(i)  = lwGet<TCompute>(1) / lwReal(shDU(i, i));

                // tDbg(i,i,THRD_BLK_ABS_START_FREQ_IDX+FREQ_IDX, NH_IDX) = shDU(i,i);

#ifdef ENABLE_DEBUG
                printf("DU[%d][%d][%d] = %f+j%f, DIlw[%d] = %f\n", FREQ_IDX, i, i, shDU(i, i).x, shDU(i, i).y, i, shDIlw(i));
#endif
                // compute upper diagonal elements of ith row of matrix U (U is an upper triangular matrix)
                for(int32_t j = i + 1; j < N_LAYERS; ++j)
                {
                    TComplexCompute sum2 = lwGet<TComplexCompute>(0);
                    for(int32_t k = 0; k < i; ++k)
                    {
                        sum2 += ((lwConj(shDU(k, i)) * shDU(k, j)) * lwReal(shDU(k, k)));
                    }
                    shDU(i, j) = (shG(i, j) - sum2) * shDIlw(i);

                    // tDbg(i,j,THRD_BLK_ABS_START_FREQ_IDX+FREQ_IDX, NH_IDX) = shDU(i,j);
#ifdef ENABLE_DEBUG
                    printf("FREQ_IDX %d Row %d Col %d DU[%d][%d][%d] = %f+j%f, sum2 = %f+j%f\n", FREQ_IDX, i, j, FREQ_IDX, i, j, shDU(i, j).x, shDU(i, j).y, sum2.x, sum2.y);
#endif
                }
            }
        }

        if(COL_IDX_I < N_COLS_I)
        {
            shI(ROW_IDX_I, COL_IDX_I) =
                (ROW_IDX_I != COL_IDX_I) ? lwGet<TComplexCompute>(0) : lwGet<TComplexCompute>(1);
        }

        // Wait for LDL factorization to complete
        thisThrdBlk.sync();

#ifdef ENABLE_DEBUG
        for(int32_t i = 0; i < N_ROWS_A; ++i)
        {
            const uint32_t COL_IDX_A = THREAD_X_IDX;
            if((COL_IDX_A < N_COLS_A) && (0 == FREQ_IDX)) printf("A[%d][%d][%d] = %f+j%f\n", FREQ_IDX, i, COL_IDX_A, shA(i, COL_IDX_A).x, shA(i, COL_IDX_A).y);
        }

        if((COL_IDX_I < N_COLS_I) && (0 == FREQ_IDX)) printf("I[%d][%d][%d] = %f+j%f\n", FREQ_IDX, ROW_IDX_I, COL_IDX_I, shI(ROW_IDX_I, COL_IDX_I).x, shI(ROW_IDX_I, COL_IDX_I).y);
        if((COL_IDX_M < N_COLS_M) && (0 == FREQ_IDX)) printf("M[%d][%d][%d] = %f+j%f\n", FREQ_IDX, ROW_IDX_M, COL_IDX_M, shM(ROW_IDX_M, COL_IDX_M).x, shM(ROW_IDX_M, COL_IDX_M).y);
#endif

        // 3b. Forward substitution: U'*[J | K] = [I | M], solve for J, K
        const uint32_t COL_IDX_A = THREAD_X_IDX;
        if(COL_IDX_A < N_COLS_A)
        {
#pragma unroll
            for(int32_t i = 0; i < N_LAYERS; ++i)
            {
                // Compute row i of intermediate matrices J, K
                TComplexCompute sum = lwGet<TComplexCompute>(0);
                for(int32_t j = 0; j < i; ++j)
                {
                    sum += (lwConj(shDU(j, i)) * shA(j, COL_IDX_A));
                }

                shA(i, COL_IDX_A) = shA(i, COL_IDX_A) - sum;
                // tDbg(i, COL_IDX_A, THRD_BLK_ABS_START_FREQ_IDX+FREQ_IDX, NH_IDX) = shA(i, COL_IDX_A);
            }
        }

        // Wait for Forward substitution to complete
        thisThrdBlk.sync();

        // 3c. Backward substitution: D*U*[GIlw | C] = [J | K], solve for GIlw, C
        if(COL_IDX_A < N_COLS_A)
        {
#pragma unroll
            for(int32_t i = N_LAYERS - 1; i >= 0; --i)
            {
                // Compute a row of intermediate matrices J, K
                TComplexCompute sum = lwGet<TComplexCompute>(0);
                for(int32_t j = i + 1; j < N_LAYERS; ++j)
                {
                    sum += (shA(j, COL_IDX_A) * shDU(i, j));
                }

                shA(i, COL_IDX_A) = (shA(i, COL_IDX_A) * shDIlw(i)) - sum;
                // tDbg(i, COL_IDX_A, THRD_BLK_ABS_START_FREQ_IDX+FREQ_IDX, NH_IDX) = shA(i, COL_IDX_A);

#ifdef ENABLE_DEBUG
                printf("A[%d][%d][%d] = %f+j%f\n", FREQ_IDX, i, COL_IDX_A, shA(i, COL_IDX_A).x, shA(i, COL_IDX_A).y);
#endif

                // Store results to be available for writes later
                if((COL_IDX_A >= N_COLS_REE) && (COL_IDX_A < N_COLS_A))
                {
                    const uint32_t COL_IDX_C_DERIVED = COL_IDX_A - N_COLS_REE;
                    shCRes(i, COL_IDX_C_DERIVED)     = shA(i, COL_IDX_A);
                    // tDbg(i, COL_IDX_C_DERIVED, THRD_BLK_ABS_START_FREQ_IDX+FREQ_IDX, NH_IDX) = shA(i, COL_IDX_A);//  shCRes(i, COL_IDX_C_DERIVED);

#ifdef ENABLE_DEBUG
                    if((0 == blockIdx.x) && (4 == FREQ_IDX)) printf("C[%d][%d][%d] = %f+j%f\n", FREQ_IDX, i, COL_IDX_C_DERIVED, shCRes(i, COL_IDX_C_DERIVED).x, shCRes(i, COL_IDX_C_DERIVED).y);
#endif
                }
                if(COL_IDX_A == i)
                {
                    shMemBlkReeDiag(i, FREQ_IDX) = lwReal(shA(i, i));
                    // shMemBlkReeDiag(i, FREQ_IDX) = lwGet<TCompute>(1)/(lwReal(shA(i, i)));
                    // tDbg(i, THRD_BLK_ABS_START_FREQ_IDX+FREQ_IDX, NH_IDX).x = shMemBlkReeDiag(i, FREQ_IDX);
                    // tDbg(i, THRD_BLK_ABS_START_FREQ_IDX+FREQ_IDX, NH_IDX).y = 0;
#ifdef ENABLE_DEBUG
                    if((0 == blockIdx.x) && (4 == FREQ_IDX)) printf("ReeDiag[%d][%d] = %f\n", FREQ_IDX, i, shMemBlkReeDiag(i));
#endif
                }
            }
        }

        // Wait for Ree = GIlw to be computed
        thisThrdBlk.sync();
    }

    //--------------------------------------------------------------------------------------------------------
    // 5. Apply bias correction and write MMSE coefficients out to global memory

    // Write C for whole PRB in one burst. Each N_THREADS_X group of threads can write one C matrix
#pragma unroll
    for(uint32_t f = 0; f < N_ITER_PER_PRB; ++f)
    {
        const uint32_t FREQ_IDX = (f * N_FREQ_BINS_PER_ITER + THRD_GRP_FREQ_OFFSET);
#if(EQ_COEF_APPLY_VER == 1)
        const uint32_t ABS_FREQ_IDX = THRD_BLK_ABS_START_FREQ_IDX + FREQ_IDX;
#endif
        if(COL_IDX_C < N_COLS_C)
        {
            // Compute bias correction factor lambda and apply to coefficients
            TCompute lambda = lwGet<TCompute>(1) / (lwGet<TCompute>(1) - shMemBlkReeDiag(ROW_IDX_C, FREQ_IDX));

#if(EQ_COEF_APPLY_VER == 1)
            tCoef(ROW_IDX_C, COL_IDX_C, ABS_FREQ_IDX, NH_IDX) =
                type_colwert<TComplexStorageOut>(shMemBlkC(ROW_IDX_C, COL_IDX_C, FREQ_IDX) * lambda);
#elif(EQ_COEF_APPLY_VER == 2)
            tCoef(COL_IDX_C, FREQ_IDX, ROW_IDX_C, PRB_IDX) =
                type_colwert<TComplexStorageOut>(shMemBlkC(ROW_IDX_C, COL_IDX_C, FREQ_IDX) * lambda);
#endif
        }
        if(THREAD_X_IDX < N_ROWS_REE)
        {
            // Compute ReeIlw while applying bias correction
            // TCompute reeIlw = (lwGet<TCompute>(1)/shMemBlkReeDiag(THREAD_X_IDX, FREQ_IDX)) - lwGet<TCompute>(1);
            TCompute reeIlw = (lwGet<TCompute>(1) - shMemBlkReeDiag(THREAD_X_IDX, FREQ_IDX)) / shMemBlkReeDiag(THREAD_X_IDX, FREQ_IDX);

#if(EQ_COEF_APPLY_VER == 1)
            tReeDiagIlw(THREAD_X_IDX, ABS_FREQ_IDX, NH_IDX) = type_colwert<TStorageOut>(reeIlw);
#elif(EQ_COEF_APPLY_VER == 2)
            tReeDiagIlw(FREQ_IDX, THREAD_X_IDX, PRB_IDX) = type_colwert<TStorageOut>(reeIlw);
#endif
        }

#ifdef ENABLE_DEBUG
        printf("C[%d][%d][%d] = %f+j%f\n", ABS_FREQ_IDX, ROW_IDX_C, COL_IDX_C, shC(ROW_IDX_C, COL_IDX_C).x, shC(ROW_IDX_C, COL_IDX_C).y);
#endif
    }

    // thisThrdBlk.sync();
}

static constexpr uint32_t MAX_BITS_QAM = LWPHY_QAM_256;
// static constexpr uint32_t N_MAX_QAM_LLR   = MAX_BITS_QAM;
static constexpr uint32_t MAX_BITS_PAM = (MAX_BITS_QAM / 2);
// static constexpr uint32_t N_MAX_PAM_LLR   = (MAX_BITS_PAM/2);
static constexpr uint32_t N_PAM_PER_QAM = (MAX_BITS_QAM / MAX_BITS_PAM);

__constant__ uint8_t LUT_PAM_OFFSET[] = {0, 1, 3, 6};
__constant__ float   LUT_SYMB_DIST_KPAM[] =
    {
        // PAM  OFFSET
        0.707106781186548f, //  1     0

        0.632455532033676f, //  2     1
        0.316227766016838f,

        0.617213399848368f, //  3     3
        0.308606699924184f,
        0.154303349962092f,

        0.613571991077897f, //  4     6
        0.306785995538948f,
        0.153392997769474f,
        0.076696498884737f};

template <typename TCompute,
          typename TStorageOut>
__device__ __forceinline__ void computePamLlr(int32_t                                      nPamBits,
                                              uint32_t                                     iqIdx,
                                              TCompute                                     noiseIlw,
                                              typename complex_from_scalar<TCompute>::type softEst,
                                              TCompute* __restrict__ pShWrkBuf,
                                              float4* __restrict__ pShLlr)
{
    constexpr uint32_t N_IQ = 2; // 2 samples: 1 I + 1 Q

    int32_t  pamIdx        = nPamBits - 1;
    int32_t  lastPamBitIdx = pamIdx;
    uint8_t  lutOffset     = LUT_PAM_OFFSET[pamIdx];
    TCompute dist          = LUT_SYMB_DIST_KPAM[lutOffset + lastPamBitIdx];

    // Inphase vs Quadrature branch
    TCompute pamSymb = (0 == iqIdx) ? lwReal(softEst) : lwImag(softEst);

    // Compute soft bits by soft slicing the received symbol and squared minimum distances of 2nd kind
    TCompute* pSoftBits = &pShWrkBuf[0];
    TCompute* pMinDist2 = &pShWrkBuf[MAX_BITS_PAM];

    pSoftBits[0] = pamSymb;

    uint8_t softBitSignBmsk = 0;
    for(int32_t i = 0; i < nPamBits - 1; ++i)
    {
        pSoftBits[i + 1] = LUT_SYMB_DIST_KPAM[lutOffset + i] - lwAbs(pSoftBits[i]);
        pMinDist2[i]     = dist + lwAbs(pSoftBits[i]);
        pMinDist2[i] *= pMinDist2[i];

        if(pSoftBits[i] < lwGet<TCompute>(0)) softBitSignBmsk |= (0x01 << i);

#ifdef ENABLE_DEBUG
        printf("computePamLLr: pamSymb = %f, pSoftBits[%d] = %f, pMinDist2[%d] = %f, dist = %f\n", pamSymb, i, pSoftBits[i], i, pMinDist2[i], dist);
#endif
    }
    pMinDist2[lastPamBitIdx] = dist + lwAbs(pSoftBits[lastPamBitIdx]);
    pMinDist2[lastPamBitIdx] *= pMinDist2[lastPamBitIdx];

    if(pSoftBits[lastPamBitIdx] < lwGet<TCompute>(0)) softBitSignBmsk |= (0x01 << lastPamBitIdx);

    // Minimum distance between received symbol and its closest constellation point (min distance of 1st kind)
    TCompute minDist1 = lwAbs(pSoftBits[lastPamBitIdx]) - dist;
    minDist1 *= minDist1;

    // Compute LLRs
    TStorageOut* pLlr = reinterpret_cast<TStorageOut*>(pShLlr);
    for(int32_t i = 0; i < nPamBits; ++i)
    {
        TCompute llr = ((pMinDist2[i] - minDist1) * noiseIlw);
        // if(pSoftBits[i] < lwGet<TCompute>(0)) llr = -llr;
        if(softBitSignBmsk & (0x01 << i)) llr = -llr;

        uint32_t llrIdx = (i * N_IQ) + iqIdx;
        pLlr[llrIdx]    = llr;

#ifdef ENABLE_DEBUG
        printf("computePamLLr: llr = %f, pamSymb = %f, pSoftBits[%d] = %f, pMinDist2[%d] = %f, minDist1 = %f, N0Ilw = %f\n", llr, pamSymb, i, pSoftBits[i], i, pMinDist2[i], minDist1, noiseIlw);
#endif
    }
}

template <typename TComplexCompute,
          uint32_t THRD_GRP_SIZE>
__device__ __forceinline__
    TComplexCompute
    thrdGrpAllReduceSum(thread_block_tile<THRD_GRP_SIZE> const& thisThrdGrp, TComplexCompute const& val)
{
    uint32_t        thrdGrpSize = thisThrdGrp.size();
    TComplexCompute sum         = val;
    for(int32_t i = thrdGrpSize / 2; i > 0; i /= 2)
    {
        sum.x += thisThrdGrp.shfl_xor(lwReal(sum), i);
        sum.y += thisThrdGrp.shfl_xor(lwImag(sum), i);
    }
    thisThrdGrp.sync();
    return sum;
}

// Per PRB equalizer coefficient application fused with soft demap
// Inputs and outputs assumed to be column major
// dimBlock: (N_LAYERS*N_BS_ANTS, N_FREQ_BINS_PER_ITER)
//           Essentially, there are N_FREQ_BINS_PER_ITER group of threads with each group size being N_LAYERS*N_BS_ANTS,
// dimGrid : Nprb
template <typename TStorageIn,
          typename TDataRx,
          typename TStorageOut,
          typename TCompute,
          uint32_t N_FREQ_BINS_PER_ITER, // # of frequency bins processed in 1 iteration by the thread block
          uint32_t N_ITER,               // # of iterations to run
          uint32_t N_BS_ANTS,            // # of BS antenna (# of cols of C matrix)
          uint32_t N_LAYERS,             // # of layers (# of rows of C matrix)
          uint32_t NH,                   // # of estimates of H in time
          uint32_t N_SYMBS_PER_THRD_BLK> // # of data symbols processed by a thread block
__global__ void
eq_mmse_soft_demap_kernel_v1(tensor_ref<const uint8_t, 1>                                        tDataSymbLoc, // ND
                             tensor_ref<const QAM_t, 1>                                          tQamInfo,     // (N_LAYERS, N_PRB, ND)
                             tensor_ref<const typename complex_from_scalar<TStorageIn>::type, 4> tCoef,        // (N_LAYERS, N_BS_ANTS, NF, NH)
                             tensor_ref<const TStorageIn, 3>                                     tReeDiagIlw,  // (N_LAYERS, NF, NH)
                             tensor_ref<const typename complex_from_scalar<TDataRx>::type, 3>    tDataRx,      // (NF, ND, N_BS_ANTS)
                             tensor_ref<typename complex_from_scalar<TStorageOut>::type, 3>      tDataEq,      // (N_LAYERS, NF, ND)
                             tensor_ref<TStorageOut, 4>                                          tLlr,         // (N_LLR, N_LAYERS, NF, ND)
                             tensor_ref<typename complex_from_scalar<TStorageOut>::type, 4>      tDbg)
{
    //--------------------------------------------------------------------------------------------------------
    // Dimensions
    constexpr uint32_t N_IQ = 2; // 2 samples: 1 I + 1 Q

    // Number of frequency bins processed by a thread block
    constexpr uint32_t N_FREQ_BINS_PER_THRD_BLK = N_FREQ_BINS_PER_ITER * N_ITER;
    // Number of thread blocks needed to process a PRB
    constexpr uint32_t N_THRD_BLKS_PER_PRB = LWPHY_N_TONES_PER_PRB / (N_FREQ_BINS_PER_THRD_BLK);

    // Process a PRB in N_ITER_PER_PRB with each iteration processing N_FREQ_BINS_PER_ITER tones
    // constexpr uint32_t N_ITER_= LWPHY_N_TONES_PER_PRB/N_FREQ_BINS_PER_ITER;

    // C         : coefficient matrix
    constexpr uint32_t N_ROWS_C                = N_LAYERS;
    constexpr uint32_t N_COLS_C                = N_BS_ANTS;
    constexpr uint32_t N_ROWS_C_TRNSP          = N_COLS_C;
    constexpr uint32_t N_COLS_C_TRNSP          = N_ROWS_C;
    constexpr uint32_t PER_LAYER_THRD_GRP_SIZE = N_BS_ANTS;
    static_assert(N_BS_ANTS <= N_THREADS_PER_WARP, "using co-operative groups to compute an inner product across N_BS_ANTS");

    // ReeDiagIlw: Ilwerse of the diagonal of residual error covariance matrix
    constexpr uint32_t N_ROWS_REE = N_LAYERS;
    // constexpr uint32_t N_COLS_REE = N_LAYERS;

    // Y         : Input data vector to be equalized
    constexpr uint32_t N_ROWS_Y = N_BS_ANTS;

    constexpr uint32_t N_INST = 2; // double buffering for pipelining

    //--------------------------------------------------------------------------------------------------------
    // Compute indices used for element access
    // const uint32_t N_THREADS_X  = blockDim.x;
    const uint32_t N_THREADS    = blockDim.x * blockDim.y;
    const uint32_t THREAD_X_IDX = threadIdx.x;
    const uint32_t THREAD_IDX   = (threadIdx.y * blockDim.x) + threadIdx.x;

    const uint32_t DATA_COL_START_IDX = blockIdx.y * N_SYMBS_PER_THRD_BLK;

    // There are N_FREQ_BINS_PER_ITER groups of threads with each group containing N_THREADS_X per group
    const uint32_t PRB_IDX                     = blockIdx.x / N_THRD_BLKS_PER_PRB;
    const uint32_t THRD_GRP_FREQ_OFFSET        = threadIdx.y;
    const uint32_t THRD_BLK_ABS_START_FREQ_IDX = blockIdx.x * N_FREQ_BINS_PER_THRD_BLK;
    const uint32_t THRD_BLK_ABS_END_FREQ_IDX   = THRD_BLK_ABS_START_FREQ_IDX + N_FREQ_BINS_PER_THRD_BLK;

    const uint32_t NH_IDX = 0; // @todo: add high mobility support i.e. handle multiple estimates of H in time

    // Read from global memory in column major order
    const uint32_t ROW_IDX_C = THREAD_X_IDX % N_ROWS_C;
    const uint32_t COL_IDX_C = THREAD_X_IDX / N_ROWS_C;

    // Store in shared in row major order
    const uint32_t ROW_IDX_C_TRNSP = THREAD_X_IDX % N_ROWS_C_TRNSP;
    const uint32_t COL_IDX_C_TRNSP = THREAD_X_IDX / N_ROWS_C_TRNSP;

    const uint32_t ROW_IDX_REE = THREAD_X_IDX % N_ROWS_REE;
    const uint32_t COL_IDX_REE = THREAD_X_IDX / N_ROWS_REE; // COL_IDX_REE needs a bounds check (since N_THREADS_X > # of Ree elements)

    const uint32_t ROW_IDX_Y = THREAD_X_IDX % N_ROWS_Y;
    const uint32_t COL_IDX_Y = THREAD_X_IDX / N_ROWS_Y;

    thread_block const& thisThrdBlk = this_thread_block();

    // 1 tile per layer, each tile of size N_ROWS_C = N_BS_ANTS
    thread_block_tile<PER_LAYER_THRD_GRP_SIZE> const& perLayerThrdGrp =
        tiled_partition<PER_LAYER_THRD_GRP_SIZE>(thisThrdBlk);

    // for each frequency bin, there are N_LAYERS x N_BS_ANTS threads. PER_LAYER_THREAD_IDX is an index to
    // enumerate the N_BS_ANTS threads for a given layer
    const uint32_t LAYER_IDX            = THREAD_X_IDX / N_BS_ANTS;
    const uint32_t PER_LAYER_THREAD_IDX = perLayerThrdGrp.thread_rank();
    // const uint32_t PER_LAYER_THREAD_GRP_IDX = N_THREADS_X / PER_LAYER_THRD_GRP_SIZE;

    //--------------------------------------------------------------------------------------------------------
    // Shared memory allocation
    __shared__ uint8_t shPamBitLen[N_LAYERS][N_SYMBS_PER_THRD_BLK];

    // C, ReeDiagIlw
    // SMEM allocation: C[N_FREQ_BINS_PER_ITER*N_INST_C], ReeDiagIlw[N_FREQ_BINS_PER_ITER*N_INST_C],
    constexpr uint32_t N_SMEM_ELEMS = ((((N_ROWS_C_TRNSP + 1) * N_COLS_C_TRNSP) + N_ROWS_Y + N_ROWS_REE) * N_FREQ_BINS_PER_ITER * N_INST);

    typedef typename complex_from_scalar<TCompute>::type    TComplexCompute;
    typedef typename complex_from_scalar<TStorageIn>::type  TComplexStorageIn;
    typedef typename complex_from_scalar<TStorageOut>::type TComplexStorageOut;

    __shared__ TComplexCompute smemBlk[N_SMEM_ELEMS];

    // Note that C is stored in row major order (i.e. transposed form) in shared memory to enable adjacent
    // threads to access adjacent row elements
    constexpr uint32_t                                                     SMEM_START_OFFSET_C_TRNSP_BLK = 0;
    const uint32_t                                                         SMEM_START_OFFSET_C_TRNSP     = SMEM_START_OFFSET_C_TRNSP_BLK + (THRD_GRP_FREQ_OFFSET * (N_ROWS_C_TRNSP + 1) * N_COLS_C_TRNSP * N_INST);
    block_3D<TComplexCompute*, N_ROWS_C_TRNSP + 1, N_COLS_C_TRNSP, N_INST> shCTrnsp(&smemBlk[SMEM_START_OFFSET_C_TRNSP]);

    const uint32_t                               SMEM_START_OFFSET_Y_BLK = SMEM_START_OFFSET_C_TRNSP_BLK + (N_FREQ_BINS_PER_ITER * shCTrnsp.num_elem());
    const uint32_t                               SMEM_START_OFFSET_Y     = SMEM_START_OFFSET_Y_BLK + (THRD_GRP_FREQ_OFFSET * N_ROWS_Y * N_INST);
    block_2D<TComplexCompute*, N_ROWS_Y, N_INST> shY(&smemBlk[SMEM_START_OFFSET_Y]);

    const uint32_t                          SMEM_START_OFFSET_REE_BLK = SMEM_START_OFFSET_Y_BLK + (N_FREQ_BINS_PER_ITER * shY.num_elem());
    const uint32_t                          SMEM_START_OFFSET_REE     = SMEM_START_OFFSET_REE_BLK + (THRD_GRP_FREQ_OFFSET * N_ROWS_REE * N_INST);
    block_2D<TCompute*, N_ROWS_REE, N_INST> shReeDiagIlw(reinterpret_cast<TCompute*>(&smemBlk[SMEM_START_OFFSET_REE]));

    __shared__ float4 shLlr[N_FREQ_BINS_PER_ITER * N_LAYERS * N_PAM_PER_QAM]; // [N_FREQ_BINS_PER_ITER][N_LAYERS][N_PAM_PER_QAM];
    const uint32_t    LLR_START_IDX =
        (THRD_GRP_FREQ_OFFSET * N_LAYERS * N_PAM_PER_QAM) + (LAYER_IDX * N_PAM_PER_QAM);

    // 2 buffers: one to store softBits and other to store minDist
    constexpr uint32_t PAM_LLR_COMP_WRK_BUF_LEN = 2 * MAX_BITS_PAM;
    __shared__ TCompute shWrkBuf[N_FREQ_BINS_PER_ITER * N_LAYERS * N_IQ * PAM_LLR_COMP_WRK_BUF_LEN];
    const uint32_t      WRK_BUF_START_IDX =
        ((((THRD_GRP_FREQ_OFFSET * N_LAYERS) + LAYER_IDX) * N_IQ) + PER_LAYER_THREAD_IDX) * PAM_LLR_COMP_WRK_BUF_LEN;

    //--------------------------------------------------------------------------------------------------------
    // Process
#ifdef ENABLE_DEBUG
    if(0 != blockIdx.x) return;
#endif

    // Load QAM info for each layer and for all the data symbols to be processed
    const uint32_t N_QAM_INFO_TO_RD       = N_LAYERS * N_SYMBS_PER_THRD_BLK;
    const uint32_t N_QAM_INFO_RD_PER_ITER = (N_QAM_INFO_TO_RD > N_THREADS) ? N_THREADS : N_QAM_INFO_TO_RD;
    const uint32_t N_ITER_TO_RD_QAM_INFO  = div_round_up(N_QAM_INFO_TO_RD, N_QAM_INFO_RD_PER_ITER);

#ifdef ENABLE_DEBUG
    if(0 == thisThrdBlk.thread_rank())
        printf("softDemap:N_SYMBS_PER_THRD_BLK %d N_QAM_INFO_TO_RD %d N_QAM_INFO_RD_PER_ITER %d N_ITER_TO_RD_QAM_INFO %d\n", N_SYMBS_PER_THRD_BLK, N_QAM_INFO_TO_RD, N_QAM_INFO_RD_PER_ITER, N_ITER_TO_RD_QAM_INFO);
#endif

    for(uint32_t i = 0; i < N_ITER_TO_RD_QAM_INFO; ++i)
    {
        uint32_t qamInfoElemIdx  = (i * N_QAM_INFO_RD_PER_ITER + THREAD_IDX);
        uint32_t qamInfoLayerIdx = qamInfoElemIdx % N_LAYERS;
        uint32_t qamInfoSymbIdx  = (qamInfoElemIdx / N_LAYERS);
        if((qamInfoLayerIdx < N_LAYERS) && (qamInfoSymbIdx < N_SYMBS_PER_THRD_BLK))
        {
            shPamBitLen[qamInfoLayerIdx][qamInfoSymbIdx] = static_cast<uint32_t>(tQamInfo(qamInfoLayerIdx, PRB_IDX, DATA_COL_START_IDX + qamInfoSymbIdx)) / 2;

#ifdef ENABLE_DEBUG
            printf("softDemap: layerIdx  %d symbolIdx %d qam %d\n", qamInfoLayerIdx, qamInfoSymbIdx, 2 * shPamBitLen[qamInfoLayerIdx][qamInfoSymbIdx]);
#endif
        }
    }

    // Prologue - outer loop
    // Read coefficients needed for the first iteration
    uint32_t f          = 0;
    uint32_t coefIdx    = 0;
    uint32_t freqIdx    = (f * N_FREQ_BINS_PER_ITER + THRD_GRP_FREQ_OFFSET);
    uint32_t absFreqIdx = THRD_BLK_ABS_START_FREQ_IDX + freqIdx;

    // Load C in regular form and store transposed
    shCTrnsp(COL_IDX_C, ROW_IDX_C, coefIdx) =
        type_colwert<TComplexCompute>(tCoef(ROW_IDX_C, COL_IDX_C, absFreqIdx, NH_IDX));
    if(ROW_IDX_REE == COL_IDX_REE)
    {
        shReeDiagIlw(ROW_IDX_REE, coefIdx) =
            type_colwert<TCompute>(tReeDiagIlw(ROW_IDX_REE, absFreqIdx, NH_IDX));
    }

    for(f = 1; f <= N_ITER; ++f)
    {
        // Prologue - inner loop
        // Read data for the first iteration
        uint32_t dataColIdx    = 0;
        uint32_t nxtDataColIdx = dataColIdx + 1;
        uint32_t yIdx          = 0;
        uint32_t nxtYIdx       = yIdx ^ 0x1;
        if(0 == COL_IDX_Y)
        {
            shY(ROW_IDX_Y, yIdx) =
                type_colwert<TComplexCompute>(tDataRx(absFreqIdx, tDataSymbLoc(DATA_COL_START_IDX + dataColIdx), ROW_IDX_Y));
        }

        // Read coefficients needed for the next iteration
        uint32_t nxtCoefIdx    = coefIdx ^ 0x1;
        uint32_t nxtFreqIdx    = (f * N_FREQ_BINS_PER_ITER + THRD_GRP_FREQ_OFFSET);
        uint32_t nxtAbsFreqIdx = THRD_BLK_ABS_START_FREQ_IDX + nxtFreqIdx;

        // Load C in regular form and store transposed
        if(nxtAbsFreqIdx < THRD_BLK_ABS_END_FREQ_IDX)
        {
            shCTrnsp(COL_IDX_C, ROW_IDX_C, nxtCoefIdx) =
                type_colwert<TComplexCompute>(tCoef(ROW_IDX_C, COL_IDX_C, nxtAbsFreqIdx, NH_IDX));
            if(ROW_IDX_REE == COL_IDX_REE)
            {
                shReeDiagIlw(ROW_IDX_REE, nxtCoefIdx) =
                    type_colwert<TCompute>(tReeDiagIlw(ROW_IDX_REE, nxtAbsFreqIdx, NH_IDX));
            }
        }

        // Stripe across time over all the data columns to be processed by this thread block for the given PRB
        for(nxtDataColIdx = dataColIdx + 1; nxtDataColIdx <= N_SYMBS_PER_THRD_BLK; ++nxtDataColIdx)
        {
            // Read data for next iteration
            if((0 == COL_IDX_Y) && (nxtDataColIdx < N_SYMBS_PER_THRD_BLK))
            {
                shY(ROW_IDX_Y, nxtYIdx) =
                    type_colwert<TComplexCompute>(tDataRx(absFreqIdx, tDataSymbLoc(DATA_COL_START_IDX + nxtDataColIdx), ROW_IDX_Y));
            }

            thisThrdBlk.sync();
            // Apply coefficients to Y

            // Each element of column vector Y scales a column of C. A co-operative group of N_COLS_C
            // threads multiplies one row of C with column vector Y - C(row,:)*Y(:)
            TComplexCompute prod = shCTrnsp(ROW_IDX_C_TRNSP, COL_IDX_C_TRNSP, coefIdx) * shY(ROW_IDX_Y, yIdx);

#ifdef ENABLE_DEBUG
            printf("softDemap: C[%d][%d][%d] = %f + j%f, y[%d][%d][%d] = %f + j%f, prod = %f + j%f\n", absFreqIdx, ROW_IDX_C_TRNSP, COL_IDX_C_TRNSP, lwReal(shCTrnsp(ROW_IDX_C_TRNSP, COL_IDX_C_TRNSP, coefIdx)), lwImag(shCTrnsp(ROW_IDX_C_TRNSP, COL_IDX_C_TRNSP, coefIdx)), absFreqIdx, dataColIdx, ROW_IDX_Y, lwReal(shY(ROW_IDX_Y, yIdx)), lwImag(shY(ROW_IDX_Y, yIdx)), lwReal(prod), lwImag(prod));
#endif

            // Accumulate across row (within co-operative group of N_BS_ANTS threads) to produce one element
            // of the result: C*y
            // Note that the result should be visible to all threads
            TComplexCompute softEst = thrdGrpAllReduceSum<TComplexCompute, PER_LAYER_THRD_GRP_SIZE>(perLayerThrdGrp, prod);

            if((0 == PER_LAYER_THREAD_IDX) || (1 == PER_LAYER_THREAD_IDX))
            {
                // PER_LAYER_THREAD_IDX = 0 processes inphase sample and PER_LAYER_THREAD_IDX = 1 processes
                // quadrature sample
                uint32_t iqIdx    = PER_LAYER_THREAD_IDX;
                int32_t  nPamBits = shPamBitLen[LAYER_IDX][dataColIdx];

                TCompute noiseIlw = shReeDiagIlw(LAYER_IDX, coefIdx);
                computePamLlr<TCompute, TStorageOut>(
                    nPamBits, iqIdx, noiseIlw, softEst, &shWrkBuf[WRK_BUF_START_IDX], &shLlr[LLR_START_IDX]);

                // Only 2 (out of N_BS_ANTS) threads used to compute LLRs
                coalesced_group activeThrds = coalesced_threads();
                activeThrds.sync();

#ifdef ENABLE_DEBUG
                if((2 == LAYER_IDX) && (3122 == absFreqIdx) && (8 == tDataSymbLoc(DATA_COL_START_IDX + dataColIdx)))
                    printf("computePamLlr: [%d][%d][%d] nPamBits = %d softEst = %f+j%f noiseIlw = %f \n\t llr[0] = %f llr[1] = %f llr[2] = %f llr[3] = %f llr[4] = %f llr[5] = %f llr[6] = %f llr[7] = %f\n",
                           LAYER_IDX,
                           absFreqIdx,
                           tDataSymbLoc(DATA_COL_START_IDX + dataColIdx),
                           nPamBits,
                           lwReal(softEst),
                           lwImag(softEst),
                           noiseIlw,
                           shLlr[LLR_START_IDX + 0].x,
                           shLlr[LLR_START_IDX + 0].y,
                           shLlr[LLR_START_IDX + 0].z,
                           shLlr[LLR_START_IDX + 0].w,
                           shLlr[LLR_START_IDX + 1].x,
                           shLlr[LLR_START_IDX + 1].y,
                           shLlr[LLR_START_IDX + 1].z,
                           shLlr[LLR_START_IDX + 1].w);
#endif

                // LLRs are packed in 2 vector float objects (float4 if FP32 and float2 if FP16). Enble writes
                // of the 2nd vector only when there are enough LLRs
                bool enableWrite = (PER_LAYER_THREAD_IDX <= ((nPamBits - 1) / 2)) ? true : false;
                if(enableWrite)
                {
                    // Write coalesced
                    uint32_t llrStartOffsetGmem =
                        tLlr.offset(PER_LAYER_THREAD_IDX * MAX_BITS_PAM, LAYER_IDX, absFreqIdx, DATA_COL_START_IDX + dataColIdx);
                    *reinterpret_cast<float4*>(tLlr.addr + llrStartOffsetGmem) = shLlr[LLR_START_IDX + PER_LAYER_THREAD_IDX];
                }
            }

            // Pick one of the N_BS_ANTS threads to store the resulting soft estimate
            if(2 == PER_LAYER_THREAD_IDX) tDataEq(LAYER_IDX, absFreqIdx, DATA_COL_START_IDX + dataColIdx) = type_colwert<TComplexStorageOut>(softEst);

#ifdef ENABLE_DEBUG
            if(2 == PER_LAYER_THREAD_IDX) printf("softDemap: f %d freqIdx %d softEst[%d][%d][%d] = %f + j%f\n", f, freqIdx, absFreqIdx, dataColIdx, LAYER_IDX, lwReal(softEst), lwImag(softEst));
#endif

            yIdx = nxtYIdx;
            nxtYIdx ^= 0x1;
            dataColIdx = nxtDataColIdx;
        }

        coefIdx    = nxtCoefIdx;
        freqIdx    = nxtFreqIdx;
        absFreqIdx = nxtAbsFreqIdx;
    }
}

// Per PRB equalizer coefficient application fused with soft demap
// Inputs and outputs assumed to be column major
// dimBlock: (N_BS_ANTS*N_PRB_TONES, N_THRD_BLK_DATA_SYMBS)
// dimGrid : (N_PRB, N_DATA_SYMBS/N_THRD_BLK_DATA_SYMBS)
// Note: NF = N_PRB_TONES * N_PRB
template <typename TStorageIn,
          typename TDataRx,
          typename TStorageOut,
          typename TCompute,
          uint32_t N_BS_ANTS,            // # of BS antenna (# of cols of C matrix)
          uint32_t N_LAYERS,             // # of layers (# of rows of C matrix)
          uint32_t NH,                   // # of estimates of H in time
          uint32_t N_SYMBS_PER_THRD_BLK> // # of data symbols processed by a thread block
__global__ void
eq_mmse_soft_demap_kernel_v2(tensor_ref<const uint8_t, 1>                                        tDataSymbLoc, // ND
                             tensor_ref<const QAM_t, 1>                                          tQamInfo,     // (N_LAYERS, N_PRB, ND)
                             tensor_ref<const typename complex_from_scalar<TStorageIn>::type, 4> tCoef,        // (N_BS_ANTS, N_PRB_TONES, N_LAYERS, N_PRB)
                             tensor_ref<const TStorageIn, 3>                                     tReeDiagIlw,  // (N_LAYERS, NF, NH)
                             tensor_ref<const typename complex_from_scalar<TDataRx>::type, 3>    tDataRx,      // (N_PRB_TONES*N_PRB, ND, N_BS_ANTS)
                             tensor_ref<typename complex_from_scalar<TStorageOut>::type, 3>      tDataEq,      // (N_LAYERS, NF, ND)
                             tensor_ref<TStorageOut, 4>                                          tLlr,         // (N_LLR, N_LAYERS, NF, ND)
                             tensor_ref<typename complex_from_scalar<TStorageOut>::type, 4>      tDbg)
{
    typedef typename complex_from_scalar<TCompute>::type    TComplexCompute;
    typedef typename complex_from_scalar<TStorageIn>::type  TComplexStorageIn;
    typedef typename complex_from_scalar<TStorageOut>::type TComplexStorageOut;

    thread_block const& thisThrdBlk = this_thread_block();

    // @todos:
    // rename N_TONES_PER_PRB, N_SC to N_PRB_TONES
    // rename N_BS_ANTS to N_BB_PORTS
    // abbreviate _THREAD_ to _THRD_ to be consisten
    // replace FREQ with TONES

    //--------------------------------------------------------------------------------------------------------
    // Dimensions
    constexpr uint32_t N_IQ = 2; // 2 samples: 1 I + 1 Q
    static_assert(N_BS_ANTS >= N_IQ, "Need N_IQ threads per PRB tone for soft demap compute");

    // Y         : Input data vector to be equalized
    constexpr uint32_t N_ROWS_Y = N_BS_ANTS;
    constexpr uint32_t N_COLS_Y = LWPHY_N_TONES_PER_PRB;

    constexpr uint32_t PRB_TONE_THRD_GRP_SIZE = N_BS_ANTS;
    static_assert(PRB_TONE_THRD_GRP_SIZE <= N_THREADS_PER_WARP, "using co-operative groups to compute an inner product across N_BS_ANTS");

    // C         : coefficient matrix
    constexpr uint32_t N_ROWS_C = N_BS_ANTS;
    constexpr uint32_t N_COLS_C = LWPHY_N_TONES_PER_PRB;

    // ReeDiagIlw: Ilwerse of the diagonal of residual error covariance matrix
    constexpr uint32_t N_ELEMS_REE_DIAG_ILW = LWPHY_N_TONES_PER_PRB;

    constexpr uint32_t N_INST = 2; // double buffer for pipelining

    //--------------------------------------------------------------------------------------------------------
    // Compute indices used for element access
    const uint32_t N_THREADS = thisThrdBlk.size();
    const uint32_t THRD_IDX  = thisThrdBlk.thread_rank();

    // Indices used for read access from GMEM: frequency first, BB port next
    // Y is laidout frequency first, so use threadIdx.x to read along frequency to perform a coalesced read
    const uint32_t GMEM_RD_FREQ_IDX    = threadIdx.x % LWPHY_N_TONES_PER_PRB;
    const uint32_t GMEM_RD_BB_PORT_IDX = threadIdx.x / LWPHY_N_TONES_PER_PRB;

    const uint32_t SMEM_WR_FREQ_IDX    = GMEM_RD_FREQ_IDX;
    const uint32_t SMEM_WR_BB_PORT_IDX = GMEM_RD_BB_PORT_IDX;
    const uint32_t SMEM_WR_LAYER_IDX   = SMEM_WR_BB_PORT_IDX; // Needs boundary check with N_LAYERS

    // Indices used for kernel shared memory access and writing to global memory: BB port first, frequency next
    const uint32_t BB_PORT_IDX = threadIdx.x % N_BS_ANTS;
    const uint32_t FREQ_IDX    = threadIdx.x / N_BS_ANTS;

    const uint32_t GMEM_WR_FREQ_IDX = FREQ_IDX;

    // PRB index processed by this thread
    const uint32_t PRB_IDX                     = blockIdx.x;
    const uint32_t THRD_BLK_ABS_START_FREQ_IDX = PRB_IDX * LWPHY_N_TONES_PER_PRB;

    // Subcarrier sample location in global memory
    const uint32_t GMEM_ABS_RD_FREQ_IDX = THRD_BLK_ABS_START_FREQ_IDX + GMEM_RD_FREQ_IDX;
    const uint32_t GMEM_ABS_WR_FREQ_IDX = THRD_BLK_ABS_START_FREQ_IDX + GMEM_WR_FREQ_IDX;

    // const uint32_t NH_IDX = 0; // @todo: add high mobility support i.e. handle multiple estimates of C in time

    const uint32_t DATA_SYMB_IDX           = threadIdx.y;
    const uint32_t DATA_SYMB_ABS_START_IDX = blockIdx.y * N_SYMBS_PER_THRD_BLK;
    const uint32_t DATA_SYMB_ABS_IDX       = DATA_SYMB_ABS_START_IDX + DATA_SYMB_IDX;

    const uint32_t SMEM_WR_DATA_SYMB_IDX = DATA_SYMB_IDX;

    // 1 tile per PRB tone, each tile of size N_BS_ANTS
    // The PRB tone thread group is used to compute the inner product across BB ports
    thread_block_tile<PRB_TONE_THRD_GRP_SIZE> const& prbToneThrdGrp =
        tiled_partition<PRB_TONE_THRD_GRP_SIZE>(thisThrdBlk);

    const uint32_t PER_LAYER_THRD_IDX = prbToneThrdGrp.thread_rank();

    //--------------------------------------------------------------------------------------------------------
    // Shared memory allocation
    __shared__ uint8_t shPamBitLen[N_LAYERS][N_SYMBS_PER_THRD_BLK];

    // SMEM1 allocation: C[N_INST], Y[N_SYMBS_PER_THRD_BLK] are overlaid (C, Y objects are both of the same size)
    constexpr uint32_t N_INST_SMEM       = (N_SYMBS_PER_THRD_BLK > N_INST) ? N_SYMBS_PER_THRD_BLK : N_INST;
    constexpr uint32_t N_SMEM_BLK1_ELEMS = (((N_ROWS_C + 1) * N_COLS_C) * N_INST_SMEM);
    __shared__ TComplexCompute smemBlk1[N_SMEM_BLK1_ELEMS];

    // SMEM2 allocation: ReeDiagIlw
    constexpr uint32_t N_SMEM_BLK2_ELEMS = (N_ELEMS_REE_DIAG_ILW + 1) * N_INST;
    __shared__ TCompute smemBlk2[N_SMEM_BLK2_ELEMS];

    constexpr uint32_t                                         SMEM_START_OFFSET_C = 0;
    block_3D<TComplexCompute*, N_ROWS_C + 1, N_COLS_C, N_INST> shC(&smemBlk1[SMEM_START_OFFSET_C]);

    // Y ooverlaid with  C
    constexpr uint32_t                                                       SMEM_START_OFFSET_Y = SMEM_START_OFFSET_C;
    block_3D<TComplexCompute*, N_ROWS_Y + 1, N_COLS_Y, N_SYMBS_PER_THRD_BLK> shY(&smemBlk1[SMEM_START_OFFSET_Y]);

    constexpr uint32_t                                    SMEM_START_OFFSET_REE_DIAG_ILW = 0;
    block_2D<TCompute*, N_ELEMS_REE_DIAG_ILW + 1, N_INST> shReeDiagIlw(&smemBlk2[SMEM_START_OFFSET_REE_DIAG_ILW]);

    // Storage for LLRs
    __shared__ float4 shLlr[LWPHY_N_TONES_PER_PRB * N_PAM_PER_QAM]; // [LWPHY_N_TONES_PER_PRB][N_PAM_PER_QAM];
    const uint32_t    LLR_START_IDX = FREQ_IDX * N_PAM_PER_QAM;

    // 2 scratchpad buffers, each of size MAX_BITS_PAM: one to store softBits and other to store minDist
    constexpr uint32_t PAM_LLR_COMP_WRK_BUF_LEN = 2 * MAX_BITS_PAM + 1;

    __shared__ TCompute shWrkBuf[LWPHY_N_TONES_PER_PRB * N_IQ * PAM_LLR_COMP_WRK_BUF_LEN];
    const uint32_t      WRK_BUF_START_IDX =
        ((FREQ_IDX * N_IQ) + PER_LAYER_THRD_IDX) * PAM_LLR_COMP_WRK_BUF_LEN;

    //--------------------------------------------------------------------------------------------------------
    // Load one time information: data to be equalized into shared memory

    // Each per PRB tone thread group co-operates to compute an inner product across N_BS_ANTS. However,
    // since data is laid out frequency first, use shared memory to transform from frequency first to BB port
    // first order.
    // Threads load gmemY[N_PRB_TONES][N_BS_ANTS][N_DATA_SYMBS_PER_THRD_BLK] to shmemY[N_BS_ANTS][N_PRB_TONES][N_DATA_SYMBS_PER_THRD_BLK]
    shY(SMEM_WR_BB_PORT_IDX, SMEM_WR_FREQ_IDX, SMEM_WR_DATA_SYMB_IDX) =
        type_colwert<TComplexCompute>(tDataRx(GMEM_ABS_RD_FREQ_IDX, tDataSymbLoc(DATA_SYMB_ABS_IDX), GMEM_RD_BB_PORT_IDX));

    thisThrdBlk.sync();
    TComplexCompute Y = shY(BB_PORT_IDX, FREQ_IDX, DATA_SYMB_IDX);

    //--------------------------------------------------------------------------------------------------------
    // Process
#ifdef ENABLE_DEBUG
    if((0 != blockIdx.x) || (0 != blockIdx.y)) return;
#endif

    // Load QAM info for each layer and for all the data symbols to be processed
    const uint32_t N_QAM_INFO_TO_RD       = N_LAYERS * N_SYMBS_PER_THRD_BLK;
    const uint32_t N_QAM_INFO_RD_PER_ITER = (N_QAM_INFO_TO_RD > N_THREADS) ? N_THREADS : N_QAM_INFO_TO_RD;
    const uint32_t N_ITER_TO_RD_QAM_INFO  = div_round_up(N_QAM_INFO_TO_RD, N_QAM_INFO_RD_PER_ITER);

#ifdef ENABLE_DEBUG
    if(0 == thisThrdBlk.thread_rank())
        printf("softDemap:N_SYMBS_PER_THRD_BLK %d N_QAM_INFO_TO_RD %d N_QAM_INFO_RD_PER_ITER %d N_ITER_TO_RD_QAM_INFO %d\n", N_SYMBS_PER_THRD_BLK, N_QAM_INFO_TO_RD, N_QAM_INFO_RD_PER_ITER, N_ITER_TO_RD_QAM_INFO);
#endif

    for(uint32_t i = 0; i < N_ITER_TO_RD_QAM_INFO; ++i)
    {
        uint32_t qamInfoElemIdx  = (i * N_QAM_INFO_RD_PER_ITER + THRD_IDX);
        uint32_t qamInfoLayerIdx = qamInfoElemIdx % N_LAYERS;
        uint32_t qamInfoSymbIdx  = (qamInfoElemIdx / N_LAYERS);
        if((qamInfoLayerIdx < N_LAYERS) && (qamInfoSymbIdx < N_SYMBS_PER_THRD_BLK))
        {
            shPamBitLen[qamInfoLayerIdx][qamInfoSymbIdx] = static_cast<uint32_t>(tQamInfo(qamInfoLayerIdx, PRB_IDX, DATA_SYMB_ABS_START_IDX + qamInfoSymbIdx)) / 2;

#ifdef ENABLE_DEBUG
            printf("softDemap: layerIdx  %d symbolIdx %d qam %d\n", qamInfoLayerIdx, qamInfoSymbIdx, 2 * shPamBitLen[qamInfoLayerIdx][qamInfoSymbIdx]);
#endif
        }
    }

    //--------------------------------------------------------------------------------------------------------
    // Prefetch Coef for all N_SC subcarriers, all N_LAYERS and all N_BS_ANTS BB ports into registers

    // Prologue
    // Read coefficients, ReeDiagIlw for the first iteration
    uint32_t lwrrIdx     = 0;
    uint32_t nxtIdx      = lwrrIdx ^ 0x1;
    uint32_t layerIdx    = 0;
    uint32_t nxtLayerIdx = layerIdx + 1;

    if(0 == DATA_SYMB_IDX)
    {
        // Coefficients laidout in application friendly format
        shC(BB_PORT_IDX, FREQ_IDX, lwrrIdx) =
            type_colwert<TComplexCompute>(tCoef(BB_PORT_IDX, FREQ_IDX, layerIdx, PRB_IDX));
        if(0 == SMEM_WR_LAYER_IDX)
        {
            shReeDiagIlw(SMEM_WR_FREQ_IDX, lwrrIdx) =
                type_colwert<TCompute>(tReeDiagIlw(GMEM_RD_FREQ_IDX, layerIdx, PRB_IDX));
        }
    }
    // #pragma unroll
    for(nxtLayerIdx = layerIdx + 1; nxtLayerIdx <= N_LAYERS; ++nxtLayerIdx)
    {
        thisThrdBlk.sync();

        // Read coefficients, ReeDiagIlw for the next iteration
        if((0 == DATA_SYMB_IDX) && (nxtLayerIdx < N_LAYERS))
        {
            shC(BB_PORT_IDX, FREQ_IDX, nxtIdx) =
                type_colwert<TComplexCompute>(tCoef(BB_PORT_IDX, FREQ_IDX, nxtLayerIdx, PRB_IDX));
            if(0 == SMEM_WR_LAYER_IDX)
            {
                shReeDiagIlw(SMEM_WR_FREQ_IDX, nxtIdx) =
                    type_colwert<TCompute>(tReeDiagIlw(GMEM_RD_FREQ_IDX, nxtLayerIdx, PRB_IDX));
            }
        }

        // Apply coefficients of a given layer to Y

        // Compute the product of coefficient vector C(0:N_BS_ANTS-1,freqIdx,layerIdx) with data vector
        // Y(0:N_BS_ANTS-1,freqIdx,symbIdx) using a co-operative group of N_BS_ANTS.
        // There is one such vector product per frequency bin
        TComplexCompute prod = shC(BB_PORT_IDX, FREQ_IDX, lwrrIdx) * Y;

#ifdef ENABLE_DEBUG
        printf("softDemap: C[%d][%d][%d][%d] = %f + j%f, y[%d][%d][%d] = %f + j%f, prod = %f + j%f\n", BB_PORT_IDX, FREQ_IDX, layerIdx, PRB_IDX, lwReal(shC(BB_PORT_IDX, FREQ_IDX, lwrrIdx)), lwImag(shC(BB_PORT_IDX, FREQ_IDX, lwrrIdx)), BB_PORT_IDX, FREQ_IDX, tDataSymbLoc(DATA_SYMB_ABS_IDX), lwReal(Y), lwImag(Y), lwReal(prod), lwImag(prod));
#endif

        // Accumulate within the co-operative group of N_BS_ANTS threads to produce one element
        // of the result: C*y
        // Note that the result should be visible to all threads
        TComplexCompute softEst = thrdGrpAllReduceSum<TComplexCompute, PRB_TONE_THRD_GRP_SIZE>(prbToneThrdGrp, prod);

        // if((0 == PER_LAYER_THRD_IDX) || (1 == PER_LAYER_THRD_IDX))
        if(PER_LAYER_THRD_IDX < 2)
        {
            // PER_LAYER_THRD_IDX = 0 processes inphase sample and PER_LAYER_THRD_IDX = 1 processes
            // quadrature sample
            uint32_t iqIdx    = PER_LAYER_THRD_IDX;
            int32_t  nPamBits = shPamBitLen[layerIdx][DATA_SYMB_IDX];
            TCompute noiseIlw = shReeDiagIlw(FREQ_IDX, lwrrIdx);

#ifdef ENABLE_DEBUG
            if((0 == layerIdx) && (1 == GMEM_ABS_WR_FREQ_IDX) && (0 == tDataSymbLoc(DATA_SYMB_ABS_IDX)))
#endif
                computePamLlr<TCompute, TStorageOut>(
                    nPamBits, iqIdx, noiseIlw, softEst, &shWrkBuf[WRK_BUF_START_IDX], &shLlr[LLR_START_IDX]);

            // Only 2 (out of N_BS_ANTS) threads used to compute LLRs
            coalesced_group activeThrds = coalesced_threads();
            activeThrds.sync();

#ifdef ENABLE_DEBUG
            // if((2 == layerIdx) && (3122 == GMEM_ABS_WR_FREQ_IDX) && (8 == tDataSymbLoc(DATA_SYMB_ABS_IDX)))
            if((0 == layerIdx) && (1 == GMEM_ABS_WR_FREQ_IDX) && (0 == tDataSymbLoc(DATA_SYMB_ABS_IDX)))
                printf("computePamLlr: [%d][%d][%d][%d] nPamBits = %d softEst = %f+j%f noiseIlw = %f \n\t llr[0] = %f llr[1] = %f llr[2] = %f llr[3] = %f llr[4] = %f llr[5] = %f llr[6] = %f llr[7] = %f\n",
                       layerIdx,
                       FREQ_IDX,
                       PRB_IDX,
                       tDataSymbLoc(DATA_SYMB_ABS_IDX),
                       nPamBits,
                       lwReal(softEst),
                       lwImag(softEst),
                       noiseIlw,
                       shLlr[LLR_START_IDX + 0].x,
                       shLlr[LLR_START_IDX + 0].y,
                       shLlr[LLR_START_IDX + 0].z,
                       shLlr[LLR_START_IDX + 0].w,
                       shLlr[LLR_START_IDX + 1].x,
                       shLlr[LLR_START_IDX + 1].y,
                       shLlr[LLR_START_IDX + 1].z,
                       shLlr[LLR_START_IDX + 1].w);
#endif

            // LLRs are packed in 2 vector float objects (float4 if FP32 and float2 if FP16). Enble writes
            // of the 2nd vector only when there are enough LLRs
            bool enableWrite = (PER_LAYER_THRD_IDX <= ((nPamBits - 1) / 2)) ? true : false;
            if(enableWrite)
            {
                // Write coalesced
                uint32_t llrStartOffsetGmem =
                    tLlr.offset(PER_LAYER_THRD_IDX * MAX_BITS_PAM, layerIdx, GMEM_ABS_WR_FREQ_IDX, DATA_SYMB_ABS_IDX);

                // *reinterpret_cast<float4*>(tLlr.addr + llrStartOffsetGmem) = shLlr[LLR_START_IDX + PER_LAYER_THRD_IDX];
                *reinterpret_cast<float4*>(tLlr.addr + llrStartOffsetGmem) = static_cast<float4>(shLlr[LLR_START_IDX + PER_LAYER_THRD_IDX]);
            }
        }

#ifdef WRITE_EQ_OUTPUT
        // Pick one of the N_BS_ANTS threads to store the resulting soft estimate
        if(2 == PER_LAYER_THRD_IDX) tDataEq(layerIdx, GMEM_ABS_WR_FREQ_IDX, DATA_SYMB_ABS_IDX) = type_colwert<TComplexStorageOut>(softEst);
#endif

        lwrrIdx = nxtIdx;
        nxtIdx ^= 0x1;
        layerIdx = nxtLayerIdx;
    }
}

template <typename TStorageIn,
          typename TDataRx,
          typename TStorageOut,
          typename TCompute,
          uint32_t N_BS_ANTS, // # of BS antenna (# of rows in H matrix)
          uint32_t N_LAYERS,  // # of layers (# of cols in H matrix)
          uint32_t ND,        // # of OFDM symbols bearing data
          uint32_t NH,        // # of estimates of H in time
          QAM_t    QAM>
void eq_mmse_kernel_launch(uint32_t           Nf,
                           const_tensor_pair& tData_sym_loc,
                           const_tensor_pair& tData_rx,
                           const_tensor_pair& tH,
                           const_tensor_pair& tNoise_pwr,
                           tensor_pair&       tData_eq,
                           tensor_pair&       tRee_diag,
                           tensor_pair&       tLLR,
                           lwdaStream_t       strm)
{
    int* pDbg = nullptr;
    // Round up to a multiple of warp thread count
    const uint32_t N_THREADS_PER_THREAD_BLOCK =
        round_up_to_next<uint32_t>(ND + 2 * N_LAYERS, N_THREADS_PER_WARP);
    dim3 gridDim(Nf);
    dim3 blockDim(N_THREADS_PER_THREAD_BLOCK);

    typedef typename complex_from_scalar<TStorageIn>::type  TComplexStorageIn;
    typedef typename complex_from_scalar<TStorageOut>::type TComplexStorageOut;
    typedef typename complex_from_scalar<TDataRx>::type     TComplexDataRx;

    tensor_ref<const uint8_t, 1>           Data_sym_loc(tData_sym_loc);
    tensor_ref<const TComplexDataRx, 3>    Data_rx(tData_rx);
    tensor_ref<const TComplexStorageIn, 4> H(tH);
    tensor_ref<const TStorageIn, 2>        Noise_pwr(tNoise_pwr);
    tensor_ref<TComplexStorageOut, 3>      Data_eq(tData_eq);
    tensor_ref<TStorageOut, 3>             Ree_diag(tRee_diag);
    tensor_ref<TStorageOut, 4>             LLR(tLLR);

    eq_mmse_kernel<TStorageIn, TDataRx, TStorageOut, TCompute, N_BS_ANTS, N_LAYERS, ND, NH, QAM>
        <<<gridDim, blockDim, 0, strm>>>(Data_sym_loc,
                                         Data_rx,
                                         H,
                                         Noise_pwr,
                                         Data_eq,
                                         Ree_diag,
                                         LLR,
                                         pDbg);

#ifdef ENABLE_PROFILING
    lwdaEvent_t eStart, eStop;

    LWDA_CHECK(lwdaEventCreateWithFlags(&eStart, lwdaEventBlockingSync));
    LWDA_CHECK(lwdaEventCreateWithFlags(&eStop, lwdaEventBlockingSync));

    constexpr uint32_t N_ITER = 1000;
    LWDA_CHECK(lwdaEventRecord(eStart));

    for(uint32_t i = 0; i < N_ITER; ++i)
    {
        eq_mmse_kernel<TStorageIn, TDataRx, TStorageOut, TCompute, N_BS_ANTS, N_LAYERS, ND, NH, QAM>
            <<<gridDim, blockDim, 0, strm>>>(Data_sym_loc,
                                             Data_rx,
                                             H,
                                             Noise_pwr,
                                             Data_eq,
                                             Ree_diag,
                                             LLR,
                                             pDbg);
    }

    LWDA_CHECK(lwdaEventRecord(eStop));
    LWDA_CHECK(lwdaEventSynchronize(eStop));

    float elapsedMs = 0.0f;
    lwdaEventElapsedTime(&elapsedMs, eStart, eStop);

    printf("Average elapsed time in usec (LWCA event) = %9.4f\n", elapsedMs * 1000 / N_ITER);

    LWDA_CHECK(lwdaEventDestroy(eStart));
    LWDA_CHECK(lwdaEventDestroy(eStop));
#endif
}

template <typename TStorageIn,
          typename TStorageOut,
          typename TCompute,
          uint32_t N_BS_ANTS, // # of BS antenna (# of rows in H matrix)
          uint32_t N_LAYERS,  // # of layers (# of cols in H matrix)
          uint32_t NH>        // # of estimates of H in time
void
eq_mmse_coef_comp_massive_mimo_kernel_launch(uint32_t           Nprb,
                                             const_tensor_pair& tH,
                                             const_tensor_pair& tRwwIlw,
                                             tensor_pair&       tCoef,
                                             tensor_pair&       tReeDiagIlw,
                                             tensor_pair&       tDbg,
                                             lwdaStream_t       strm)
{
    constexpr uint32_t N_THRD_BLK_TONES   = 1; // LWPHY_N_TONES_PER_PRB;
    constexpr uint32_t N_TONES_PER_ITER   = 1; // 12;
    constexpr uint32_t N_THRD_BLK_PER_PRB = (LWPHY_N_TONES_PER_PRB / N_THRD_BLK_TONES);
    // (N_BS_ANTS+N_LAYERS) > (N_BS_ANTS*N_LAYERS) if N_LAYERS = 1
    // const uint32_t N_THRDS_PER_TONE = std::max(N_BS_ANTS * N_LAYERS, N_BS_ANTS + N_LAYERS);
    // const uint32_t N_THRDS_PER_TONE = round_up_to_next<uint32_t>(N_BS_ANTS + (2 * N_LAYERS), N_THREADS_PER_WARP);
    // const uint32_t     N_THRDS_PER_TONE   = N_BS_ANTS + (2 * N_LAYERS);

    const uint32_t N_THRDS_PER_TONE = N_LAYERS * N_LAYERS;

    static_assert(0 == (LWPHY_N_TONES_PER_PRB % N_THRD_BLK_TONES), "Number of tones processed per thread block must be a multiple of PRB size");
    static_assert(0 == (N_THRD_BLK_TONES % N_TONES_PER_ITER), "Number of tones processed per iteration must be a multiple of number of tones processed by the thread block");

    dim3 gridDim(N_THRD_BLK_PER_PRB, Nprb);
    dim3 blockDim(N_THRDS_PER_TONE, N_TONES_PER_ITER);

    typedef typename complex_from_scalar<TCompute>::type    TComplexCompute;
    typedef typename complex_from_scalar<TStorageIn>::type  TComplexStorageIn;
    typedef typename complex_from_scalar<TStorageOut>::type TComplexStorageOut;

    tensor_ref<const TComplexStorageIn, 4> H(tH);
    tensor_ref<const TComplexStorageIn, 4> RwwIlw(tRwwIlw);
    tensor_ref<TStorageOut, 3>             ReeDiagIlw(tReeDiagIlw);
    tensor_ref<TComplexStorageOut, 4>      C(tCoef);
    tensor_ref<TComplexStorageOut, 4>      Dbg(tDbg);

    // For V100, max permitted shared memory capacity is 96KB

#if 0
    constexpr int32_t  N_ITER = N_THRD_BLK_TONES / N_TONES_PER_ITER;
    constexpr uint32_t N_INST = (1 == N_ITER) ? 1 : 2; // double buffering for pipelining
    constexpr uint32_t N_SMEM_ELEMS =
        (((N_BS_ANTS + 1) * N_LAYERS * N_INST) +
         ((N_LAYERS + 1) * (N_LAYERS + N_LAYERS + N_BS_ANTS))) *
            N_TONES_PER_ITER;

    int nShmemBytes    = N_SMEM_ELEMS * sizeof(TComplexCompute);
    int nMaxShmemBytes = nShmemBytes;
    lwdaFuncSetAttribute(eq_mmse_coef_comp_massive_mimo_kernel_v1<TStorageIn, TStorageOut, TCompute, N_THRD_BLK_TONES, N_TONES_PER_ITER, N_BS_ANTS, N_LAYERS, NH>,
                         lwdaFuncAttributeMaxDynamicSharedMemorySize,
                         nMaxShmemBytes);
#else

    int                nShmemBytes        = 0;
#endif
    eq_mmse_coef_comp_massive_mimo_kernel_v1<TStorageIn, TStorageOut, TCompute, N_THRD_BLK_TONES, N_TONES_PER_ITER, N_BS_ANTS, N_LAYERS, NH>
        <<<gridDim, blockDim, nShmemBytes, strm>>>(H,
                                                   RwwIlw,
                                                   ReeDiagIlw,
                                                   C,
                                                   Dbg);
}

template <typename TStorageIn,
          typename TStorageOut,
          typename TCompute,
          uint32_t N_BS_ANTS, // # of BS antenna (# of rows in H matrix)
          uint32_t N_LAYERS,  // # of layers (# of cols in H matrix)
          uint32_t NH>        // # of estimates of H in time
void
eq_mmse_coef_comp_high_mimo_kernel_launch(uint32_t           Nprb,
                                          const_tensor_pair& tH,
                                          const_tensor_pair& tRwwIlw,
                                          tensor_pair&       tCoef,
                                          tensor_pair&       tReeDiagIlw,
                                          tensor_pair&       tDbg,
                                          lwdaStream_t       strm)
{
#if(EQ_COEF_COMP_H_MIMO_VER == 1)
    constexpr uint32_t N_THRD_BLK_TONES   = 6; // LWPHY_N_TONES_PER_PRB;
    constexpr uint32_t N_TONES_PER_ITER   = 2; // 12;
    constexpr uint32_t N_THRD_BLK_PER_PRB = (LWPHY_N_TONES_PER_PRB / N_THRD_BLK_TONES);
    // (N_BS_ANTS+N_LAYERS) > (N_BS_ANTS*N_LAYERS) if N_LAYERS = 1
    const uint32_t N_THRDS_PER_TONE = std::max(N_BS_ANTS * N_LAYERS, N_BS_ANTS + N_LAYERS);
#elif(EQ_COEF_COMP_H_MIMO_VER == 2)
    constexpr uint32_t N_THRD_BLK_TONES   = 6; // 3;// 6;
    constexpr uint32_t N_TONES_PER_ITER   = 2; // 1;// 2;
    constexpr uint32_t N_THRD_BLK_PER_PRB = (LWPHY_N_TONES_PER_PRB / N_THRD_BLK_TONES);
    // const uint32_t N_THRDS_PER_TONE = round_up_to_next<uint32_t>(N_BS_ANTS + (2 * N_LAYERS), N_THREADS_PER_WARP);
    const uint32_t N_THRDS_PER_TONE = N_BS_ANTS + (2 * N_LAYERS); // width of the augmented matrix to be factorized
#endif

    static_assert(0 == (LWPHY_N_TONES_PER_PRB % N_THRD_BLK_TONES), "Number of tones processed per thread block must be a multiple of PRB size");
    static_assert(0 == (N_THRD_BLK_TONES % N_TONES_PER_ITER), "Number of tones processed per iteration must be a multiple of number of tones processed by the thread block");

    dim3 gridDim(N_THRD_BLK_PER_PRB, Nprb);
    dim3 blockDim(N_THRDS_PER_TONE, N_TONES_PER_ITER);

    typedef typename complex_from_scalar<TStorageIn>::type  TComplexStorageIn;
    typedef typename complex_from_scalar<TStorageOut>::type TComplexStorageOut;

    tensor_ref<const TComplexStorageIn, 4> H(tH);
    tensor_ref<const TComplexStorageIn, 4> RwwIlw(tRwwIlw);
    tensor_ref<TStorageOut, 3>             ReeDiagIlw(tReeDiagIlw);
    tensor_ref<TComplexStorageOut, 4>      C(tCoef);
    tensor_ref<TComplexStorageOut, 4>      Dbg(tDbg);

    // v1 has a better memory BW utilization (85% v1 vs 71% v2) but 4us (58us vs 62us) slower for 100MHz (273PRB) usecase
#if(EQ_COEF_COMP_H_MIMO_VER == 1)
    eq_mmse_coef_comp_high_mimo_kernel_v1<TStorageIn, TStorageOut, TCompute, N_THRD_BLK_TONES, N_TONES_PER_ITER, N_BS_ANTS, N_LAYERS, NH>
        <<<gridDim, blockDim, 0, strm>>>(H,
                                         RwwIlw,
                                         ReeDiagIlw,
                                         C,
                                         Dbg);
#elif(EQ_COEF_COMP_H_MIMO_VER == 2)
    eq_mmse_coef_comp_high_mimo_kernel_v2<TStorageIn, TStorageOut, TCompute, N_THRD_BLK_TONES, N_TONES_PER_ITER, N_BS_ANTS, N_LAYERS, NH>
        <<<gridDim, blockDim, 0, strm>>>(H,
                                         RwwIlw,
                                         ReeDiagIlw,
                                         C,
                                         Dbg);
#endif
}

template <typename TStorageIn,
          typename TStorageOut,
          typename TCompute,
          uint32_t N_BS_ANTS, // # of BS antenna (# of rows in H matrix)
          uint32_t N_LAYERS,  // # of layers (# of cols in H matrix)
          uint32_t NH>        // # of estimates of H in time
void
eq_mmse_coef_comp_low_mimo_kernel_launch(uint32_t           Nprb,
                                         const_tensor_pair& tH,
                                         const_tensor_pair& tRwwIlw,
                                         tensor_pair&       tCoef,
                                         tensor_pair&       tReeDiagIlw,
                                         tensor_pair&       tDbg,
                                         lwdaStream_t       strm)
{
    constexpr uint32_t N_FREQ_BINS_PER_ITER = 4; // 12;
    // (N_BS_ANTS+N_LAYERS) > (N_BS_ANTS*N_LAYERS) if N_LAYERS = 1
    const uint32_t N_THREADS_PER_FREQ_BIN = std::max(N_BS_ANTS * N_LAYERS, N_BS_ANTS + N_LAYERS);
    dim3           gridDim(Nprb);
    dim3           blockDim(N_THREADS_PER_FREQ_BIN, N_FREQ_BINS_PER_ITER);

    typedef typename complex_from_scalar<TStorageIn>::type  TComplexStorageIn;
    typedef typename complex_from_scalar<TStorageOut>::type TComplexStorageOut;

    tensor_ref<const TComplexStorageIn, 4> H(tH);
    tensor_ref<const TComplexStorageIn, 4> RwwIlw(tRwwIlw);
    tensor_ref<TStorageOut, 3>             ReeDiagIlw(tReeDiagIlw);
    tensor_ref<TComplexStorageOut, 4>      C(tCoef);
    tensor_ref<TComplexStorageOut, 4>      Dbg(tDbg);

    eq_mmse_coef_comp_low_mimo_kernel<TStorageIn, TStorageOut, TCompute, N_FREQ_BINS_PER_ITER, N_BS_ANTS, N_LAYERS, NH>
        <<<gridDim, blockDim, 0, strm>>>(H,
                                         RwwIlw,
                                         ReeDiagIlw,
                                         C,
                                         Dbg);
}

template <typename TStorageIn,
          typename TDataRx,
          typename TStorageOut,
          typename TCompute,
          uint32_t N_BS_ANTS, // # of BS antenna (# of cols of C matrix)
          uint32_t N_LAYERS,  // # of layers (# of rows of C matrix)
          uint32_t NH,        // # of estimates of H in time
          uint32_t N_SYMBS_PER_THRD_BLK,
          uint32_t N_FREQ_BINS_PER_ITER,
          uint32_t N_ITER>
void eq_mmse_soft_demap_kernel_launch(uint32_t           Nprb,
                                      uint32_t           Nd,
                                      const_tensor_pair& tDataSymbLoc,
                                      const_tensor_pair& tQamInfo,
                                      const_tensor_pair& tCoef,
                                      const_tensor_pair& tReeDiagIlw,
                                      const_tensor_pair& tDataRx,
                                      tensor_pair&       tDataEq,
                                      tensor_pair&       tLlr,
                                      tensor_pair&       tDbg,
                                      lwdaStream_t       strm)
{
#if(EQ_COEF_APPLY_VER == 1)
    constexpr uint32_t N_THREADS_PER_FREQ_BIN = N_BS_ANTS * N_LAYERS;
    // Number of frequency bins processed by a thread block
    constexpr uint32_t N_FREQ_BINS_PER_THRD_BLK = N_FREQ_BINS_PER_ITER * N_ITER;
    // Number of thread blocks needed to process a PRB
    constexpr uint32_t N_THRD_BLKS_PER_PRB = LWPHY_N_TONES_PER_PRB / (N_FREQ_BINS_PER_THRD_BLK);

    dim3 gridDim(Nprb * N_THRD_BLKS_PER_PRB, Nd / N_SYMBS_PER_THRD_BLK);
    dim3 blockDim(N_THREADS_PER_FREQ_BIN, N_FREQ_BINS_PER_ITER);

    typedef typename complex_from_scalar<TStorageIn>::type  TComplexStorageIn;
    typedef typename complex_from_scalar<TStorageOut>::type TComplexStorageOut;
    typedef typename complex_from_scalar<TDataRx>::type     TComplexDataRx;

    tensor_ref<const uint8_t, 1>           DataSymbLoc(tDataSymbLoc);
    tensor_ref<const QAM_t, 1>             QamInfo(tQamInfo);
    tensor_ref<const TComplexStorageIn, 4> Coef(tCoef);
    tensor_ref<const TStorageIn, 3>        ReeDiagIlw(tReeDiagIlw);
    tensor_ref<const TComplexDataRx, 3>    DataRx(tDataRx);
    tensor_ref<TComplexStorageOut, 3>      DataEq(tDataEq);
    tensor_ref<TStorageOut, 4>             Llr(tLlr);
    tensor_ref<TComplexStorageOut, 4>      Dbg(tDbg);

    eq_mmse_soft_demap_kernel_v1<TStorageIn,
                                 TDataRx,
                                 TStorageOut,
                                 TCompute,
                                 N_FREQ_BINS_PER_ITER,
                                 N_ITER,
                                 N_BS_ANTS,
                                 N_LAYERS,
                                 NH,
                                 N_SYMBS_PER_THRD_BLK>
        <<<gridDim, blockDim, 0, strm>>>(DataSymbLoc,
                                         QamInfo,
                                         Coef,
                                         ReeDiagIlw,
                                         DataRx,
                                         DataEq,
                                         Llr,
                                         Dbg);
#elif(EQ_COEF_APPLY_VER == 2)
    // Number of frequency bins processed by a thread block
    constexpr uint32_t N_FREQ_BINS_PER_THRD_BLK = LWPHY_N_TONES_PER_PRB;
    // Ensure max(N_BS_ANTS, N_LAYERS) threads exist per PRB tone
    constexpr uint32_t N_THRDS_PER_THRD_BLK = N_BS_ANTS * N_FREQ_BINS_PER_THRD_BLK;
    dim3               gridDim(Nprb, Nd / N_SYMBS_PER_THRD_BLK);
    dim3               blockDim(N_THRDS_PER_THRD_BLK, N_SYMBS_PER_THRD_BLK);

    typedef typename complex_from_scalar<TStorageIn>::type  TComplexStorageIn;
    typedef typename complex_from_scalar<TStorageOut>::type TComplexStorageOut;
    typedef typename complex_from_scalar<TDataRx>::type     TComplexDataRx;

    tensor_ref<const uint8_t, 1>           DataSymbLoc(tDataSymbLoc);
    tensor_ref<const QAM_t, 1>             QamInfo(tQamInfo);
    tensor_ref<const TComplexStorageIn, 4> Coef(tCoef);
    tensor_ref<const TStorageIn, 3>        ReeDiagIlw(tReeDiagIlw);
    tensor_ref<const TComplexDataRx, 3>    DataRx(tDataRx);
    tensor_ref<TComplexStorageOut, 3>      DataEq(tDataEq);
    tensor_ref<TStorageOut, 4>             Llr(tLlr);
    tensor_ref<TComplexStorageOut, 4>      Dbg(tDbg);

    eq_mmse_soft_demap_kernel_v2<TStorageIn,
                                 TDataRx,
                                 TStorageOut,
                                 TCompute,
                                 N_BS_ANTS,
                                 N_LAYERS,
                                 NH,
                                 N_SYMBS_PER_THRD_BLK>
        <<<gridDim, blockDim, 0, strm>>>(DataSymbLoc,
                                         QamInfo,
                                         Coef,
                                         ReeDiagIlw,
                                         DataRx,
                                         DataEq,
                                         Llr,
                                         Dbg);
#endif
}

template <typename TStorageIn, typename TDataRx, typename TStorageOut, typename TCompute>
void eq_kernel_launch(uint32_t           nBSAnts,
                      uint32_t           nLayers,
                      uint32_t           Nh,
                      uint32_t           Nf,
                      uint32_t           Nd,
                      QAM_t              qam,
                      const_tensor_pair& tData_sym_loc,
                      const_tensor_pair& tData_rx,
                      const_tensor_pair& tH,
                      const_tensor_pair& tNoise_pwr,
                      tensor_pair&       tData_eq,
                      tensor_pair&       tRee_diag,
                      tensor_pair&       tLLR,
                      lwdaStream_t       strm)
{
    if((16 == nBSAnts) && (8 == nLayers) && (1 == Nh) && (6 == Nd) && (QAM_t::QAM_64 == qam))
    {
        constexpr uint32_t N_BS_ANTS = 16; // # of BS antenna (# of rows in H matrix)
        constexpr uint32_t N_LAYERS  = 8;  // # of layers (# of cols in H matrix)
        constexpr uint32_t ND        = 6;  // # of OFDM symbols bearing data
        constexpr uint32_t NH        = 1;  // # of estimates of H in time
        constexpr QAM_t    QAM       = QAM_t::QAM_64;
        eq_mmse_kernel_launch<TStorageIn, TDataRx, TStorageOut, TCompute, N_BS_ANTS, N_LAYERS, ND, NH, QAM>(Nf,
                                                                                                            tData_sym_loc,
                                                                                                            tData_rx,
                                                                                                            tH,
                                                                                                            tNoise_pwr,
                                                                                                            tData_eq,
                                                                                                            tRee_diag,
                                                                                                            tLLR,
                                                                                                            strm);
    }
    else if((16 == nBSAnts) && (16 == nLayers) && (1 == Nh) && (12 == Nd) && (QAM_t::QAM_64 == qam))
    {
        constexpr uint32_t N_BS_ANTS = 16; // # of BS antenna (# of rows in H matrix)
        constexpr uint32_t N_LAYERS  = 16; // # of layers (# of cols in H matrix)
        constexpr uint32_t ND        = 12; // # of OFDM symbols bearing data
        constexpr uint32_t NH        = 1;  // # of estimates of H in time
        constexpr QAM_t    QAM       = QAM_t::QAM_64;
        eq_mmse_kernel_launch<TStorageIn, TDataRx, TStorageOut, TCompute, N_BS_ANTS, N_LAYERS, ND, NH, QAM>(Nf,
                                                                                                            tData_sym_loc,
                                                                                                            tData_rx,
                                                                                                            tH,
                                                                                                            tNoise_pwr,
                                                                                                            tData_eq,
                                                                                                            tRee_diag,
                                                                                                            tLLR,
                                                                                                            strm);
    }
    else if((8 == nBSAnts) && (4 == nLayers) && (1 == Nh) && (8 == Nd) && (QAM_t::QAM_64 == qam))
    {
        constexpr uint32_t N_BS_ANTS = 8; // # of BS antenna (# of rows in H matrix)
        constexpr uint32_t N_LAYERS  = 4; // # of layers (# of cols in H matrix)
        constexpr uint32_t ND        = 8; // # of OFDM symbols bearing data
        constexpr uint32_t NH        = 1; // # of estimates of H in time
        constexpr QAM_t    QAM       = QAM_t::QAM_64;
        eq_mmse_kernel_launch<TStorageIn, TDataRx, TStorageOut, TCompute, N_BS_ANTS, N_LAYERS, ND, NH, QAM>(Nf,
                                                                                                            tData_sym_loc,
                                                                                                            tData_rx,
                                                                                                            tH,
                                                                                                            tNoise_pwr,
                                                                                                            tData_eq,
                                                                                                            tRee_diag,
                                                                                                            tLLR,
                                                                                                            strm);
    }
    else if((8 == nBSAnts) && (4 == nLayers) && (1 == Nh) && (12 == Nd) && (QAM_t::QAM_64 == qam))
    {
        constexpr uint32_t N_BS_ANTS = 8;  // # of BS antenna (# of rows in H matrix)
        constexpr uint32_t N_LAYERS  = 4;  // # of layers (# of cols in H matrix)
        constexpr uint32_t ND        = 12; // # of OFDM symbols bearing data
        constexpr uint32_t NH        = 1;  // # of estimates of H in time
        constexpr QAM_t    QAM       = QAM_t::QAM_64;
        eq_mmse_kernel_launch<TStorageIn, TDataRx, TStorageOut, TCompute, N_BS_ANTS, N_LAYERS, ND, NH, QAM>(Nf,
                                                                                                            tData_sym_loc,
                                                                                                            tData_rx,
                                                                                                            tH,
                                                                                                            tNoise_pwr,
                                                                                                            tData_eq,
                                                                                                            tRee_diag,
                                                                                                            tLLR,
                                                                                                            strm);
    }
    else if((8 == nBSAnts) && (4 == nLayers) && (1 == Nh) && (9 == Nd) && (QAM_t::QAM_64 == qam))
    {
        constexpr uint32_t N_BS_ANTS = 8; // # of BS antenna (# of rows in H matrix)
        constexpr uint32_t N_LAYERS  = 4; // # of layers (# of cols in H matrix)
        constexpr uint32_t ND        = 9; // # of OFDM symbols bearing data
        constexpr uint32_t NH        = 1; // # of estimates of H in time
        constexpr QAM_t    QAM       = QAM_t::QAM_64;
        eq_mmse_kernel_launch<TStorageIn, TDataRx, TStorageOut, TCompute, N_BS_ANTS, N_LAYERS, ND, NH, QAM>(Nf,
                                                                                                            tData_sym_loc,
                                                                                                            tData_rx,
                                                                                                            tH,
                                                                                                            tNoise_pwr,
                                                                                                            tData_eq,
                                                                                                            tRee_diag,
                                                                                                            tLLR,
                                                                                                            strm);
    }
    else
    {
        printf("Channel Eq: No kernel available to launch with requested configuration: nBSAnts %d nLayers %d Nh %d Nd %d\n"
               "QAM %d \n",
               nBSAnts,
               nLayers,
               Nh,
               Nd,
               static_cast<uint32_t>(qam));
    }
}

template <typename TStorageIn, typename TStorageOut, typename TCompute>
void eq_coef_comp_kernel_launch(uint32_t           nBSAnts,
                                uint32_t           nLayers,
                                uint32_t           Nh,
                                uint32_t           Nprb,
                                const_tensor_pair& tH,
                                const_tensor_pair& tRwwIlw,
                                tensor_pair&       tCoef,
                                tensor_pair&       tReeDiagIlw,
                                tensor_pair&       tDbg,
                                lwdaStream_t       strm)
{
    // Low MIMO regime
    if(((8 == nBSAnts) || (4 == nBSAnts)) && (1 == Nh))
    {
        constexpr uint32_t NH = 1; // # of estimates of H in time
        switch(nBSAnts)
        {
        // nBSAnts == 8
        case 8:
        {
            constexpr uint32_t N_BS_ANTS = 8; // # of BS antenna (# of rows in H matrix)
            switch(nLayers)
            {
            // nLayers == 8
            case 8:
            {
                constexpr uint32_t N_LAYERS = 8; // # of layers (# of cols in H matrix)
                eq_mmse_coef_comp_high_mimo_kernel_launch<TStorageIn, TStorageOut, TCompute, N_BS_ANTS, N_LAYERS, NH>(Nprb,
                                                                                                                      tH,
                                                                                                                      tRwwIlw,
                                                                                                                      tCoef,
                                                                                                                      tReeDiagIlw,
                                                                                                                      tDbg,
                                                                                                                      strm);
                break;
            }
            // nLayers == 4
            case 4:
            {
                constexpr uint32_t N_LAYERS = 4; // # of layers (# of cols in H matrix)
                eq_mmse_coef_comp_low_mimo_kernel_launch<TStorageIn, TStorageOut, TCompute, N_BS_ANTS, N_LAYERS, NH>(Nprb,
                                                                                                                     tH,
                                                                                                                     tRwwIlw,
                                                                                                                     tCoef,
                                                                                                                     tReeDiagIlw,
                                                                                                                     tDbg,
                                                                                                                     strm);
                break;
            }
            // nLayers == 2
            case 2:
            {
                constexpr uint32_t N_LAYERS = 2; // # of layers (# of cols in H matrix)
                eq_mmse_coef_comp_low_mimo_kernel_launch<TStorageIn, TStorageOut, TCompute, N_BS_ANTS, N_LAYERS, NH>(Nprb,
                                                                                                                     tH,
                                                                                                                     tRwwIlw,
                                                                                                                     tCoef,
                                                                                                                     tReeDiagIlw,
                                                                                                                     tDbg,
                                                                                                                     strm);
                break;
            }
                // nLayers == 1
            case 1:
            {
                constexpr uint32_t N_LAYERS = 1; // # of layers (# of cols in H matrix)
                eq_mmse_coef_comp_low_mimo_kernel_launch<TStorageIn, TStorageOut, TCompute, N_BS_ANTS, N_LAYERS, NH>(Nprb,
                                                                                                                     tH,
                                                                                                                     tRwwIlw,
                                                                                                                     tCoef,
                                                                                                                     tReeDiagIlw,
                                                                                                                     tDbg,
                                                                                                                     strm);
                break;
            }
            default:
            {
                printf("eqCoefCompKernelLaunch: No kernel available to launch with requested configuration: nBSAnts %d nLayers %d Nh %d\n",
                       nBSAnts,
                       nLayers,
                       Nh);
                break;
            }
            }

            break;
        }

        // nBSAnts == 4
        case 4:
        {
            constexpr uint32_t N_BS_ANTS = 4; // # of BS antenna (# of rows in H matrix)

            switch(nLayers)
            {
            // nLayers == 4
            case 4:
            {
                constexpr uint32_t N_LAYERS = 4; // # of layers (# of cols in H matrix)
                eq_mmse_coef_comp_low_mimo_kernel_launch<TStorageIn, TStorageOut, TCompute, N_BS_ANTS, N_LAYERS, NH>(Nprb,
                                                                                                                     tH,
                                                                                                                     tRwwIlw,
                                                                                                                     tCoef,
                                                                                                                     tReeDiagIlw,
                                                                                                                     tDbg,
                                                                                                                     strm);
                break;
            }

            // nLayers == 2
            case 2:
            {
                constexpr uint32_t N_LAYERS = 2; // # of layers (# of cols in H matrix)
                eq_mmse_coef_comp_low_mimo_kernel_launch<TStorageIn, TStorageOut, TCompute, N_BS_ANTS, N_LAYERS, NH>(Nprb,
                                                                                                                     tH,
                                                                                                                     tRwwIlw,
                                                                                                                     tCoef,
                                                                                                                     tReeDiagIlw,
                                                                                                                     tDbg,
                                                                                                                     strm);
                break;
            }

            // nLayers == 1
            case 1:
            {
                constexpr uint32_t N_LAYERS = 1; // # of layers (# of cols in H matrix)
                eq_mmse_coef_comp_low_mimo_kernel_launch<TStorageIn, TStorageOut, TCompute, N_BS_ANTS, N_LAYERS, NH>(Nprb,
                                                                                                                     tH,
                                                                                                                     tRwwIlw,
                                                                                                                     tCoef,
                                                                                                                     tReeDiagIlw,
                                                                                                                     tDbg,
                                                                                                                     strm);
                break;
            }
            default:
            {
                printf("eqCoefCompKernelLaunch: No kernel available to launch with requested configuration: nBSAnts %d nLayers %d Nh %d\n",
                       nBSAnts,
                       nLayers,
                       Nh);
                break;
            }
            }

            break;
        }

        default:
        {
            printf("eqCoefCompKernelLaunch: No kernel available to launch with requested configuration: nBSAnts %d nLayers %d Nh %d\n",
                   nBSAnts,
                   nLayers,
                   Nh);
            break;
        }
        }
    }
    // High MIMO regime
    else if((16 == nBSAnts) && (1 == Nh))
    {
        constexpr uint32_t NH = 1; // # of estimates of H in time
        switch(nBSAnts)
        {
        // nBSAnts == 16
        case 16:
        {
            constexpr uint32_t N_BS_ANTS = 16; // # of BS antenna (# of rows in H matrix)
            switch(nLayers)
            {
            // nLayers == 8
            case 8:
            {
                constexpr uint32_t N_LAYERS = 8; // # of layers (# of cols in H matrix)
                eq_mmse_coef_comp_high_mimo_kernel_launch<TStorageIn, TStorageOut, TCompute, N_BS_ANTS, N_LAYERS, NH>(Nprb,
                                                                                                                      tH,
                                                                                                                      tRwwIlw,
                                                                                                                      tCoef,
                                                                                                                      tReeDiagIlw,
                                                                                                                      tDbg,
                                                                                                                      strm);
                break;
            }
            // nLayers == 16
            case 16:
            {
                constexpr uint32_t N_LAYERS = 16; // # of layers (# of cols in H matrix)
                eq_mmse_coef_comp_high_mimo_kernel_launch<TStorageIn, TStorageOut, TCompute, N_BS_ANTS, N_LAYERS, NH>(Nprb,
                                                                                                                      tH,
                                                                                                                      tRwwIlw,
                                                                                                                      tCoef,
                                                                                                                      tReeDiagIlw,
                                                                                                                      tDbg,
                                                                                                                      strm);
                break;
            }
            default:
            {
                printf("eqCoefCompKernelLaunch: No kernel available to launch with requested configuration: nBSAnts %d nLayers %d Nh %d\n",
                       nBSAnts,
                       nLayers,
                       Nh);
                break;
            }
            }

            break;
        }

        default:
        {
            printf("eqCoefCompKernelLaunch: No kernel available to launch with requested configuration: nBSAnts %d nLayers %d Nh %d\n",
                   nBSAnts,
                   nLayers,
                   Nh);
            break;
        }
        }
    }
    // Massive MIMO regime
    else if((64 == nBSAnts) && (1 == Nh))
    {
        constexpr uint32_t NH = 1; // # of estimates of H in time
        switch(nBSAnts)
        {
        // nBSAnts == 64
        case 64:
        {
            constexpr uint32_t N_BS_ANTS = 64; // # of BS antenna (# of rows in H matrix)
            switch(nLayers)
            {
            // nLayers == 16
            case 16:
            {
                constexpr uint32_t N_LAYERS = 16; // # of layers (# of cols in H matrix)
                eq_mmse_coef_comp_massive_mimo_kernel_launch<TStorageIn, TStorageOut, TCompute, N_BS_ANTS, N_LAYERS, NH>(Nprb,
                                                                                                                         tH,
                                                                                                                         tRwwIlw,
                                                                                                                         tCoef,
                                                                                                                         tReeDiagIlw,
                                                                                                                         tDbg,
                                                                                                                         strm);
                break;
            }
            default:
            {
                printf("eqCoefCompKernelLaunch: No kernel available to launch with requested configuration: nBSAnts %d nLayers %d Nh %d\n",
                       nBSAnts,
                       nLayers,
                       Nh);
                break;
            }
            }

            break;
        }
        default:
        {
            printf("eqCoefCompKernelLaunch: No kernel available to launch with requested configuration: nBSAnts %d nLayers %d Nh %d\n",
                   nBSAnts,
                   nLayers,
                   Nh);
            break;
        }
        }
    }
    else
    {
        printf("eqCoefCompKernelLaunch: No kernel available to launch with requested configuration: nBSAnts %d nLayers %d Nh %d\n",
               nBSAnts,
               nLayers,
               Nh);
    }
}

template <typename TStorageIn, typename TDataRx, typename TStorageOut, typename TCompute>
void eq_soft_demap_kernel_launch(uint32_t           nBSAnts,
                                 uint32_t           nLayers,
                                 uint32_t           Nh,
                                 uint32_t           Nd,
                                 uint32_t           Nprb,
                                 const_tensor_pair& tDataSymbLoc,
                                 const_tensor_pair& tQamInfo,
                                 const_tensor_pair& tCoef,
                                 const_tensor_pair& tReeDiagIlw,
                                 const_tensor_pair& tDataRx,
                                 tensor_pair&       tDataEq,
                                 tensor_pair&       tLlr,
                                 tensor_pair&       tDbg,
                                 lwdaStream_t       strm)
{
    if(((8 == nBSAnts) || (4 == nBSAnts)) && (1 == Nh))
    {
        constexpr uint32_t NH = 1; // # of estimates of H in time
        switch(nBSAnts)
        {
        // nBSAnts == 8
        case 8:
        {
            constexpr uint32_t N_BS_ANTS = 8; // # of BS antenna (# of rows in H matrix)
            switch(nLayers)
            {
            // nLayers == 8
            case 8:
            {
                constexpr uint32_t N_LAYERS             = 8; // # of layers (# of cols in H matrix)
                constexpr uint32_t N_FREQ_BINS_PER_ITER = 4; // # of subcarriers process per iteration (i.e numer of threads groups)
                constexpr uint32_t N_ITER               = 3; // # of iterations to process a PRB
                constexpr uint32_t N_SYMBS_PER_THRD_BLK = 1; // # of data symbols processed by a thread block

                eq_mmse_soft_demap_kernel_launch<TStorageIn,
                                                 TDataRx,
                                                 TStorageOut,
                                                 TCompute,
                                                 N_BS_ANTS,
                                                 N_LAYERS,
                                                 NH,
                                                 N_SYMBS_PER_THRD_BLK,
                                                 N_FREQ_BINS_PER_ITER,
                                                 N_ITER>(Nprb,
                                                         Nd,
                                                         tDataSymbLoc,
                                                         tQamInfo,
                                                         tCoef,
                                                         tReeDiagIlw,
                                                         tDataRx,
                                                         tDataEq,
                                                         tLlr,
                                                         tDbg,
                                                         strm);
                break;
            }
            // nLayers == 4
            case 4:
            {
                constexpr uint32_t N_LAYERS             = 4; // # of layers (# of cols in H matrix)
                constexpr uint32_t N_FREQ_BINS_PER_ITER = 4; // # of subcarriers process per iteration (i.e numer of threads groups)
                constexpr uint32_t N_ITER               = 3; // # of iterations to process a PRB
                constexpr uint32_t N_SYMBS_PER_THRD_BLK = 1; // # of data symbols processed by a thread block

                eq_mmse_soft_demap_kernel_launch<TStorageIn,
                                                 TDataRx,
                                                 TStorageOut,
                                                 TCompute,
                                                 N_BS_ANTS,
                                                 N_LAYERS,
                                                 NH,
                                                 N_SYMBS_PER_THRD_BLK,
                                                 N_FREQ_BINS_PER_ITER,
                                                 N_ITER>(Nprb,
                                                         Nd,
                                                         tDataSymbLoc,
                                                         tQamInfo,
                                                         tCoef,
                                                         tReeDiagIlw,
                                                         tDataRx,
                                                         tDataEq,
                                                         tLlr,
                                                         tDbg,
                                                         strm);
                break;
            }
            // nLayers == 2
            case 2:
            {
                constexpr uint32_t N_LAYERS             = 2; // # of layers (# of cols in H matrix)
                constexpr uint32_t N_FREQ_BINS_PER_ITER = 4; // # of subcarriers process per iteration (i.e numer of threads groups)
                constexpr uint32_t N_ITER               = 3; // # of iterations to process a PRB
                constexpr uint32_t N_SYMBS_PER_THRD_BLK = 1; // # of data symbols processed by a thread block

                eq_mmse_soft_demap_kernel_launch<TStorageIn,
                                                 TDataRx,
                                                 TStorageOut,
                                                 TCompute,
                                                 N_BS_ANTS,
                                                 N_LAYERS,
                                                 NH,
                                                 N_SYMBS_PER_THRD_BLK,
                                                 N_FREQ_BINS_PER_ITER,
                                                 N_ITER>(Nprb,
                                                         Nd,
                                                         tDataSymbLoc,
                                                         tQamInfo,
                                                         tCoef,
                                                         tReeDiagIlw,
                                                         tDataRx,
                                                         tDataEq,
                                                         tLlr,
                                                         tDbg,
                                                         strm);
                break;
            }
            // nLayers == 1
            case 1:
            {
                constexpr uint32_t N_LAYERS             = 1; // # of layers (# of cols in H matrix)
                constexpr uint32_t N_FREQ_BINS_PER_ITER = 4; // # of subcarriers process per iteration (i.e numer of threads groups)
                constexpr uint32_t N_ITER               = 3; // # of iterations to process a PRB
                constexpr uint32_t N_SYMBS_PER_THRD_BLK = 1; // # of data symbols processed by a thread block

                eq_mmse_soft_demap_kernel_launch<TStorageIn,
                                                 TDataRx,
                                                 TStorageOut,
                                                 TCompute,
                                                 N_BS_ANTS,
                                                 N_LAYERS,
                                                 NH,
                                                 N_SYMBS_PER_THRD_BLK,
                                                 N_FREQ_BINS_PER_ITER,
                                                 N_ITER>(Nprb,
                                                         Nd,
                                                         tDataSymbLoc,
                                                         tQamInfo,
                                                         tCoef,
                                                         tReeDiagIlw,
                                                         tDataRx,
                                                         tDataEq,
                                                         tLlr,
                                                         tDbg,
                                                         strm);
                break;
            }
            default:
            {
                printf("eqSoftDemapKernelLaunch: No kernel available to launch with requested configuration: nBSAnts %d nLayers %d Nh %d ND %d\n",
                       nBSAnts,
                       nLayers,
                       Nh,
                       Nd);
                break;
            }
            }

            break;
        }

        // nBsAnts == 4
        case 4:
        {
            constexpr uint32_t N_BS_ANTS = 4; // # of BS antenna (# of rows in H matrix)
            switch(nLayers)
            {
            // nLayers == 4
            case 4:
            {
                constexpr uint32_t N_LAYERS             = 4; // # of layers (# of cols in H matrix)
                constexpr uint32_t N_FREQ_BINS_PER_ITER = 4; // # of subcarriers process per iteration (i.e numer of threads groups)
                constexpr uint32_t N_ITER               = 3; // # of iterations to process a PRB
                constexpr uint32_t N_SYMBS_PER_THRD_BLK = 1; // # of data symbols processed by a thread block

                eq_mmse_soft_demap_kernel_launch<TStorageIn,
                                                 TDataRx,
                                                 TStorageOut,
                                                 TCompute,
                                                 N_BS_ANTS,
                                                 N_LAYERS,
                                                 NH,
                                                 N_SYMBS_PER_THRD_BLK,
                                                 N_FREQ_BINS_PER_ITER,
                                                 N_ITER>(Nprb,
                                                         Nd,
                                                         tDataSymbLoc,
                                                         tQamInfo,
                                                         tCoef,
                                                         tReeDiagIlw,
                                                         tDataRx,
                                                         tDataEq,
                                                         tLlr,
                                                         tDbg,
                                                         strm);
                break;
            }
            // nLayers == 2
            case 2:
            {
                constexpr uint32_t N_LAYERS             = 2; // # of layers (# of cols in H matrix)
                constexpr uint32_t N_FREQ_BINS_PER_ITER = 4; // # of subcarriers process per iteration (i.e numer of threads groups)
                constexpr uint32_t N_ITER               = 3; // # of iterations to process a PRB
                constexpr uint32_t N_SYMBS_PER_THRD_BLK = 1; // # of data symbols processed by a thread block

                eq_mmse_soft_demap_kernel_launch<TStorageIn,
                                                 TDataRx,
                                                 TStorageOut,
                                                 TCompute,
                                                 N_BS_ANTS,
                                                 N_LAYERS,
                                                 NH,
                                                 N_SYMBS_PER_THRD_BLK,
                                                 N_FREQ_BINS_PER_ITER,
                                                 N_ITER>(Nprb,
                                                         Nd,
                                                         tDataSymbLoc,
                                                         tQamInfo,
                                                         tCoef,
                                                         tReeDiagIlw,
                                                         tDataRx,
                                                         tDataEq,
                                                         tLlr,
                                                         tDbg,
                                                         strm);
                break;
            }
            // nLayers == 1
            case 1:
            {
                constexpr uint32_t N_LAYERS             = 1; // # of layers (# of cols in H matrix)
                constexpr uint32_t N_FREQ_BINS_PER_ITER = 4; // # of subcarriers process per iteration (i.e numer of threads groups)
                constexpr uint32_t N_ITER               = 3; // # of iterations to process a PRB
                constexpr uint32_t N_SYMBS_PER_THRD_BLK = 1; // # of data symbols processed by a thread block

                eq_mmse_soft_demap_kernel_launch<TStorageIn,
                                                 TDataRx,
                                                 TStorageOut,
                                                 TCompute,
                                                 N_BS_ANTS,
                                                 N_LAYERS,
                                                 NH,
                                                 N_SYMBS_PER_THRD_BLK,
                                                 N_FREQ_BINS_PER_ITER,
                                                 N_ITER>(Nprb,
                                                         Nd,
                                                         tDataSymbLoc,
                                                         tQamInfo,
                                                         tCoef,
                                                         tReeDiagIlw,
                                                         tDataRx,
                                                         tDataEq,
                                                         tLlr,
                                                         tDbg,
                                                         strm);
                break;
            }
            default:
            {
                printf("eqSoftDemapKernelLaunch: No kernel available to launch with requested configuration: nBSAnts %d nLayers %d Nh %d ND %d\n",
                       nBSAnts,
                       nLayers,
                       Nh,
                       Nd);
                break;
            }
            }

            break;
        }

        default:
        {
            printf("eqSoftDemapKernelLaunch: No kernel available to launch with requested configuration: nBSAnts %d nLayers %d Nh %d ND %d\n",
                   nBSAnts,
                   nLayers,
                   Nh,
                   Nd);
            break;
        }
        }
    }
    else if((16 == nBSAnts) && (1 == Nh))
    {
        constexpr uint32_t NH = 1; // # of estimates of H in time
        switch(nBSAnts)
        {
        // nBSAnts == 16
        case 16:
        {
            constexpr uint32_t N_BS_ANTS = 16; // # of BS antenna (# of rows in H matrix)
            switch(nLayers)
            {
            // nLayers == 8
            case 8:
            {
                constexpr uint32_t N_LAYERS             = 8; // # of layers (# of cols in H matrix)
                constexpr uint32_t N_FREQ_BINS_PER_ITER = 4; // # of subcarriers process per iteration (i.e numer of threads groups)
                constexpr uint32_t N_ITER               = 3; // # of iterations to process a PRB
                constexpr uint32_t N_SYMBS_PER_THRD_BLK = 1; // # of data symbols processed by a thread block

                eq_mmse_soft_demap_kernel_launch<TStorageIn,
                                                 TDataRx,
                                                 TStorageOut,
                                                 TCompute,
                                                 N_BS_ANTS,
                                                 N_LAYERS,
                                                 NH,
                                                 N_SYMBS_PER_THRD_BLK,
                                                 N_FREQ_BINS_PER_ITER,
                                                 N_ITER>(Nprb,
                                                         Nd,
                                                         tDataSymbLoc,
                                                         tQamInfo,
                                                         tCoef,
                                                         tReeDiagIlw,
                                                         tDataRx,
                                                         tDataEq,
                                                         tLlr,
                                                         tDbg,
                                                         strm);
                break;
            }
            // nLayers == 16
            case 16:
            {
                constexpr uint32_t N_LAYERS             = 16; // # of layers (# of cols in H matrix)
                constexpr uint32_t N_FREQ_BINS_PER_ITER = 4;  // # of subcarriers process per iteration (i.e numer of threads groups)
                constexpr uint32_t N_ITER               = 3;  // # of iterations to process a PRB
                constexpr uint32_t N_SYMBS_PER_THRD_BLK = 1;  // # of data symbols processed by a thread block

                eq_mmse_soft_demap_kernel_launch<TStorageIn,
                                                 TDataRx,
                                                 TStorageOut,
                                                 TCompute,
                                                 N_BS_ANTS,
                                                 N_LAYERS,
                                                 NH,
                                                 N_SYMBS_PER_THRD_BLK,
                                                 N_FREQ_BINS_PER_ITER,
                                                 N_ITER>(Nprb,
                                                         Nd,
                                                         tDataSymbLoc,
                                                         tQamInfo,
                                                         tCoef,
                                                         tReeDiagIlw,
                                                         tDataRx,
                                                         tDataEq,
                                                         tLlr,
                                                         tDbg,
                                                         strm);
                break;
            }
            default:
            {
                printf("eqSoftDemapKernelLaunch: No kernel available to launch with requested configuration: nBSAnts %d nLayers %d Nh %d ND %d\n",
                       nBSAnts,
                       nLayers,
                       Nh,
                       Nd);
                break;
            }
            }

            break;
        }

        default:
        {
            printf("eqSoftDemapKernelLaunch: No kernel available to launch with requested configuration: nBSAnts %d nLayers %d Nh %d ND %d\n",
                   nBSAnts,
                   nLayers,
                   Nh,
                   Nd);
            break;
        }
        }
    }
    else if((64 == nBSAnts) && (1 == Nh))
    {
       // Coef application for 64 BB ports is not covered (since 64 BB ports is used in context of beamforming where RU is expected to apply the coefficients)
    }
    else
    {
        printf("eqSoftDemapKernelLaunch: No kernel available to launch with requested configuration: nBSAnts %d nLayers %d Nh %d ND %d\n",
               nBSAnts,
               nLayers,
               Nh,
               Nd);
    }
}

void eqCoefCompute(uint32_t           nBSAnts,
                   uint32_t           nLayers,
                   uint32_t           Nh,
                   uint32_t           Nprb,
                   const_tensor_pair& tH,
                   const_tensor_pair& tRwwIlw,
                   tensor_pair&       tCoef,
                   tensor_pair&       tReeDiagIlw,
                   tensor_pair&       tDbg,
                   lwdaStream_t       strm)
{
    //printf("channel_eq::eqCoefCompute() begin()\n");
    using TCompute = float;
    if(LWPHY_C_32F == tH.first.get().type())
    {
        using TStorageIn = scalar_from_complex<data_type_traits<LWPHY_C_32F>::type>::type;
        if(LWPHY_C_32F == tCoef.first.get().type())
        {
            using TStorageOut = scalar_from_complex<data_type_traits<LWPHY_C_32F>::type>::type;
            eq_coef_comp_kernel_launch<TStorageIn, TStorageOut, TCompute>(nBSAnts,
                                                                          nLayers,
                                                                          Nh,
                                                                          Nprb,
                                                                          tH,
                                                                          tRwwIlw,
                                                                          tCoef,
                                                                          tReeDiagIlw,
                                                                          tDbg,
                                                                          strm);
        }
        else if(LWPHY_C_16F == tCoef.first.get().type())
        {
            using TStorageOut = scalar_from_complex<data_type_traits<LWPHY_C_16F>::type>::type;
            eq_coef_comp_kernel_launch<TStorageIn, TStorageOut, TCompute>(nBSAnts,
                                                                          nLayers,
                                                                          Nh,
                                                                          Nprb,
                                                                          tH,
                                                                          tRwwIlw,
                                                                          tCoef,
                                                                          tReeDiagIlw,
                                                                          tDbg,
                                                                          strm);
        }
        else
        {
            printf("channel_eq::eqCoefCompute(): No kernel available to launch with requested data type (1)\n");
        }
    }
    else if((LWPHY_C_16F == tH.first.get().type()) && (LWPHY_C_16F == tCoef.first.get().type()))
    {
        using TStorageIn  = scalar_from_complex<data_type_traits<LWPHY_C_16F>::type>::type;
        using TStorageOut = scalar_from_complex<data_type_traits<LWPHY_C_16F>::type>::type;
        eq_coef_comp_kernel_launch<TStorageIn, TStorageOut, TCompute>(nBSAnts,
                                                                      nLayers,
                                                                      Nh,
                                                                      Nprb,
                                                                      tH,
                                                                      tRwwIlw,
                                                                      tCoef,
                                                                      tReeDiagIlw,
                                                                      tDbg,
                                                                      strm);
    }
    else
    {
        printf("channel_eq::eqCoefCompute(): No kernel available to launch with requested data type (2)\n");
    }
    //printf("channel_eq::eqCoefCompute end()\n");
}

void eqSoftDemap(uint32_t           nBSAnts,
                 uint32_t           nLayers,
                 uint32_t           Nh,
                 uint32_t           Nd,
                 uint32_t           Nprb,
                 const_tensor_pair& tDataSymbLoc,
                 const_tensor_pair& tQamInfo,
                 const_tensor_pair& tCoef,
                 const_tensor_pair& tReeDiagIlw,
                 const_tensor_pair& tDataRx,
                 tensor_pair&       tDataEq,
                 tensor_pair&       tLlr,
                 tensor_pair&       tDbg,
                 lwdaStream_t       strm)
{
    //printf("channel_eq::eqSoftDemap begin()\n");
    using TCompute = float;
    if(LWPHY_C_32F == tCoef.first.get().type())
    {
        using TStorageIn = scalar_from_complex<data_type_traits<LWPHY_C_32F>::type>::type;
        if(LWPHY_C_32F == tDataRx.first.get().type())
        {
            using TDataRx = scalar_from_complex<data_type_traits<LWPHY_C_32F>::type>::type;
            if(LWPHY_R_32F == tLlr.first.get().type())
            {
                using TStorageOut = scalar_from_complex<data_type_traits<LWPHY_C_32F>::type>::type;
                eq_soft_demap_kernel_launch<TStorageIn, TDataRx, TStorageOut, TCompute>(nBSAnts,
                                                                                        nLayers,
                                                                                        Nh,
                                                                                        Nd,
                                                                                        Nprb,
                                                                                        tDataSymbLoc,
                                                                                        tQamInfo,
                                                                                        tCoef,
                                                                                        tReeDiagIlw,
                                                                                        tDataRx,
                                                                                        tDataEq,
                                                                                        tLlr,
                                                                                        tDbg,
                                                                                        strm);
            }
#if 0 // Enable for FP16 LLRs 
           else if(LWPHY_R_16F == tLlr.first.get().type()) 
           {
               using TStorageOut = scalar_from_complex<data_type_traits<LWPHY_C_16F>::type>::type;
               eq_soft_demap_kernel_launch<TStorageIn, TDataRx, TStorageOut, TCompute>(nBSAnts,
                                                                                       nLayers,
                                                                                       Nh,
                                                                                       Nd,
                                                                                       Nprb,
                                                                                       tDataSymbLoc,
                                                                                       tQamInfo,
                                                                                       tCoef,
                                                                                       tReeDiagIlw,
                                                                                       tDataRx,
                                                                                       tDataEq,
                                                                                       tLlr,
                                                                                       tDbg,
                                                                                       strm);
           }
#endif
            else
            {
                printf("channel_eq::eqSoftDemap: No kernel available to launch with requested data type (1)\n");
            }
        }
#if 0 // Enable for FP16 LLRs 
        else if((LWPHY_C_16F == tDataRx.first.get().type()) && (LWPHY_R_16F == tLlr.first.get().type())) 
        {
           using TDataRx = scalar_from_complex<data_type_traits<LWPHY_C_16F>::type>::type;
           using TStorageOut = scalar_from_complex<data_type_traits<LWPHY_C_16F>::type>::type;
           eq_soft_demap_kernel_launch<TStorageIn, TDataRx, TStorageOut, TCompute>(nBSAnts,
                                                                                   nLayers,
                                                                                   Nh,
                                                                                   Nd,
                                                                                   Nprb,
                                                                                   tDataSymbLoc,
                                                                                   tQamInfo,
                                                                                   tCoef,
                                                                                   tReeDiagIlw,
                                                                                   tDataRx,
                                                                                   tDataEq,
                                                                                   tLlr,
                                                                                   tDbg,
                                                                                   strm);
        }
#endif
        // supporting this mode until de-rate matching needs FP32 LLRs
        else if((LWPHY_C_16F == tDataRx.first.get().type()) && (LWPHY_R_32F == tLlr.first.get().type()))
        {
            using TDataRx     = scalar_from_complex<data_type_traits<LWPHY_C_16F>::type>::type;
            using TStorageOut = scalar_from_complex<data_type_traits<LWPHY_C_32F>::type>::type;
            eq_soft_demap_kernel_launch<TStorageIn, TDataRx, TStorageOut, TCompute>(nBSAnts,
                                                                                    nLayers,
                                                                                    Nh,
                                                                                    Nd,
                                                                                    Nprb,
                                                                                    tDataSymbLoc,
                                                                                    tQamInfo,
                                                                                    tCoef,
                                                                                    tReeDiagIlw,
                                                                                    tDataRx,
                                                                                    tDataEq,
                                                                                    tLlr,
                                                                                    tDbg,
                                                                                    strm);
        }
        else
        {
            printf("channel_eq::eqSoftDemap: No kernel available to launch with requested data type (2)\n");
        }
    }
    else if((LWPHY_C_16F == tCoef.first.get().type()) && (LWPHY_C_16F == tDataRx.first.get().type()) &&
            (LWPHY_R_16F == tLlr.first.get().type()))
    {
        using TStorageIn  = scalar_from_complex<data_type_traits<LWPHY_C_16F>::type>::type;
        using TDataRx     = scalar_from_complex<data_type_traits<LWPHY_C_16F>::type>::type;
        using TStorageOut = scalar_from_complex<data_type_traits<LWPHY_C_16F>::type>::type;
        eq_soft_demap_kernel_launch<TStorageIn, TDataRx, TStorageOut, TCompute>(nBSAnts,
                                                                                nLayers,
                                                                                Nh,
                                                                                Nd,
                                                                                Nprb,
                                                                                tDataSymbLoc,
                                                                                tQamInfo,
                                                                                tCoef,
                                                                                tReeDiagIlw,
                                                                                tDataRx,
                                                                                tDataEq,
                                                                                tLlr,
                                                                                tDbg,
                                                                                strm);
    }
    else
    {
        printf("channel_eq::eqSoftDemap: No kernel available to launch with requested data type (3)\n");
    }
    //printf("channel_eq::eqSoftDemap end()\n");
}

void equalize(uint32_t           nBSAnts,
              uint32_t           nLayers,
              uint32_t           Nh,
              uint32_t           Nf,
              uint32_t           Nd,
              QAM_t              qam,
              const_tensor_pair& tData_sym_loc,
              const_tensor_pair& tData_rx,
              const_tensor_pair& tH,
              const_tensor_pair& tNoise_pwr,
              tensor_pair&       tData_eq,
              tensor_pair&       tRee_diag,
              tensor_pair&       tLLR,
              lwdaStream_t       strm)
{
    //printf("channel_eq::equalize() begin()\n");
    using TCompute = float;
    if(LWPHY_C_32F == tH.first.get().type())
    {
        using TStorageIn = scalar_from_complex<data_type_traits<LWPHY_C_32F>::type>::type;
        if(LWPHY_C_32F == tData_rx.first.get().type())
        {
            using TDataRx = scalar_from_complex<data_type_traits<LWPHY_C_32F>::type>::type;
            if(LWPHY_R_32F == tLLR.first.get().type())
            {
                using TStorageOut = scalar_from_complex<data_type_traits<LWPHY_C_32F>::type>::type;
                eq_kernel_launch<TStorageIn, TDataRx, TStorageOut, TCompute>(nBSAnts,
                                                                             nLayers,
                                                                             Nh,
                                                                             Nf,
                                                                             Nd,
                                                                             qam,
                                                                             tData_sym_loc,
                                                                             tData_rx,
                                                                             tH,
                                                                             tNoise_pwr,
                                                                             tData_eq,
                                                                             tRee_diag,
                                                                             tLLR,
                                                                             strm);
            }
            else if(LWPHY_R_16F == tLLR.first.get().type())
            {
                using TStorageOut = scalar_from_complex<data_type_traits<LWPHY_C_16F>::type>::type;
                eq_kernel_launch<TStorageIn, TDataRx, TStorageOut, TCompute>(nBSAnts,
                                                                             nLayers,
                                                                             Nh,
                                                                             Nf,
                                                                             Nd,
                                                                             qam,
                                                                             tData_sym_loc,
                                                                             tData_rx,
                                                                             tH,
                                                                             tNoise_pwr,
                                                                             tData_eq,
                                                                             tRee_diag,
                                                                             tLLR,
                                                                             strm);
            }
            else
            {
                printf("Channel Eq: No kernel available to launch with requested data type (1)\n");
            }
        }
        else if((LWPHY_C_16F == tData_rx.first.get().type()) && (LWPHY_R_16F == tLLR.first.get().type()))
        {
            using TDataRx     = scalar_from_complex<data_type_traits<LWPHY_C_16F>::type>::type;
            using TStorageOut = scalar_from_complex<data_type_traits<LWPHY_C_16F>::type>::type;
            eq_kernel_launch<TStorageIn, TDataRx, TStorageOut, TCompute>(nBSAnts,
                                                                         nLayers,
                                                                         Nh,
                                                                         Nf,
                                                                         Nd,
                                                                         qam,
                                                                         tData_sym_loc,
                                                                         tData_rx,
                                                                         tH,
                                                                         tNoise_pwr,
                                                                         tData_eq,
                                                                         tRee_diag,
                                                                         tLLR,
                                                                         strm);
        }
        else
        {
            printf("Channel Eq: No kernel available to launch with requested data type (2)\n");
        }
    }
    else if((LWPHY_C_16F == tH.first.get().type()) && (LWPHY_C_16F == tData_rx.first.get().type()) &&
            (LWPHY_R_16F == tLLR.first.get().type()))
    {
        using TStorageIn  = scalar_from_complex<data_type_traits<LWPHY_C_16F>::type>::type;
        using TDataRx     = scalar_from_complex<data_type_traits<LWPHY_C_16F>::type>::type;
        using TStorageOut = scalar_from_complex<data_type_traits<LWPHY_C_16F>::type>::type;
        eq_kernel_launch<TStorageIn, TDataRx, TStorageOut, TCompute>(nBSAnts,
                                                                     nLayers,
                                                                     Nh,
                                                                     Nf,
                                                                     Nd,
                                                                     qam,
                                                                     tData_sym_loc,
                                                                     tData_rx,
                                                                     tH,
                                                                     tNoise_pwr,
                                                                     tData_eq,
                                                                     tRee_diag,
                                                                     tLLR,
                                                                     strm);
    }
    else
    {
        printf("Channel Eq: No kernel available to launch with requested data type (3)\n");
    }
    //printf("channel_eq::equalize() end()\n");
}

} // namespace channel_eq
