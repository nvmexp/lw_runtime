//
// Generated by LLVM LWPTX Back-End
//

.version 3.2
.target sm_20
.address_size 64

	// .globl	stlr_create_for_stellarcheck

.visible .func stlr_create_for_stellarcheck(
	.param .b64 stlr_create_for_stellarcheck_param_0,
	.param .b64 stlr_create_for_stellarcheck_param_1,
	.param .b64 stlr_create_for_stellarcheck_param_2
)
{
	.reg .s32 	%r<2>;
	.reg .s64 	%rd<3>;

	ld.param.u64 	%rd1, [stlr_create_for_stellarcheck_param_0];
	ld.param.u64 	%rd2, [stlr_create_for_stellarcheck_param_2];
	ld.u32 	%r1, [%rd2];
	st.u32 	[%rd1], %r1;
	ret;
}

	// .globl	stlr_light_closures_type
.visible .func stlr_light_closures_type(
	.param .b64 stlr_light_closures_type_param_0,
	.param .b64 stlr_light_closures_type_param_1,
	.param .b64 stlr_light_closures_type_param_2
)
{
	.reg .s64 	%rd<13>;

	ld.param.u64 	%rd1, [stlr_light_closures_type_param_0];
	ld.param.u64 	%rd2, [stlr_light_closures_type_param_2];
	ld.u64 	%rd3, [%rd2];
	ld.u64 	%rd4, [%rd2+8];
	ld.u64 	%rd5, [%rd2+16];
	ld.u64 	%rd6, [%rd2+24];
	ld.u64 	%rd7, [%rd2+32];
	ld.u64 	%rd8, [%rd2+40];
	ld.u64 	%rd9, [%rd2+48];
	ld.u64 	%rd10, [%rd2+56];
	ld.u64 	%rd11, [%rd2+64];
	ld.u64 	%rd12, [%rd2+72];
	st.u64 	[%rd1+72], %rd12;
	st.u64 	[%rd1+64], %rd11;
	st.u64 	[%rd1+56], %rd10;
	st.u64 	[%rd1+48], %rd9;
	st.u64 	[%rd1+40], %rd8;
	st.u64 	[%rd1+32], %rd7;
	st.u64 	[%rd1+24], %rd6;
	st.u64 	[%rd1+16], %rd5;
	st.u64 	[%rd1+8], %rd4;
	st.u64 	[%rd1], %rd3;
	ret;
}

	// .globl	stlr_intersect
.visible .func stlr_intersect(
	.param .b64 stlr_intersect_param_0,
	.param .b64 stlr_intersect_param_1,
	.param .b64 stlr_intersect_param_2,
	.param .b64 stlr_intersect_param_3,
	.param .b64 stlr_intersect_param_4
)
{
	.reg .s32 	%r<3>;
	.reg .s64 	%rd<3>;

	ld.param.u64 	%rd1, [stlr_intersect_param_0];
	mov.u32 	%r1, 0;
	st.u32 	[%rd1+8], %r1;
	mov.u64 	%rd2, 0;
	st.u64 	[%rd1], %rd2;
	st.u32 	[%rd1+24], %r1;
	st.u32 	[%rd1+20], %r1;
	mov.u32 	%r2, 2139095040;
	st.u32 	[%rd1+16], %r2;
	ret;
}

	// .globl	stlr_illuminate
.visible .func stlr_illuminate(
	.param .b64 stlr_illuminate_param_0,
	.param .b64 stlr_illuminate_param_1,
	.param .b64 stlr_illuminate_param_2,
	.param .b64 stlr_illuminate_param_3,
	.param .b64 stlr_illuminate_param_4
)
{
	.local .align 8 .b8 	__local_depot3[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<117>;
	.reg .f32 	%f<765>;
	.reg .s32 	%r<823>;
	.reg .s64 	%rd<313>;

	mov.u64 	%rd312, __local_depot3;
	cvta.local.u64 	%SP, %rd312;
	ld.param.u64 	%rd11, [stlr_illuminate_param_4];
	ld.param.u64 	%rd10, [stlr_illuminate_param_2];
	ld.param.u64 	%rd86, [stlr_illuminate_param_1];
	ld.param.u64 	%rd8, [stlr_illuminate_param_0];
	ld.u32 	%r83, [%rd10];
	ld.param.u64 	%rd42, [stlr_illuminate_param_3];
	// inline asm
	mov.b64 {_,%r63}, %rd86;
	// inline asm
	mov.u32 	%r582, 1;
	mov.u64 	%rd96, 0;
	// inline asm
	call (%rd13), _rt_buffer_get_id_64, (%r63, %r582, %r582, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	ld.u64 	%rd18, [%rd13+560];
	mul.wide.s32 	%rd43, %r83, 672;
	// inline asm
	mov.b64 {_,%r67}, %rd18;
	// inline asm
	// inline asm
	call (%rd19), _rt_buffer_get_id_64, (%r67, %r582, %r582, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd44, %rd19, %rd43;
	ld.v4.f32 	{%f1, %f2, %f3, %f173}, [%rd44+528];
	ld.v4.f32 	{%f4, %f5, %f6, %f174}, [%rd44+544];
	ld.v4.f32 	{%f7, %f8, %f9, %f175}, [%rd44+560];
	ld.v4.f32 	{%f176, %f177, %f178, %f179}, [%rd44+576];
	mul.ftz.f32 	%f180, %f9, %f2;
	mul.ftz.f32 	%f181, %f6, %f8;
	mul.ftz.f32 	%f182, %f3, %f5;
	neg.ftz.f32 	%f183, %f182;
	fma.rn.ftz.f32 	%f184, %f2, %f6, %f183;
	neg.ftz.f32 	%f185, %f181;
	fma.rn.ftz.f32 	%f186, %f5, %f9, %f185;
	neg.ftz.f32 	%f187, %f180;
	fma.rn.ftz.f32 	%f188, %f8, %f3, %f187;
	mul.ftz.f32 	%f189, %f4, %f9;
	mul.ftz.f32 	%f190, %f7, %f3;
	mul.ftz.f32 	%f191, %f1, %f6;
	neg.ftz.f32 	%f192, %f191;
	fma.rn.ftz.f32 	%f193, %f3, %f4, %f192;
	neg.ftz.f32 	%f194, %f190;
	fma.rn.ftz.f32 	%f195, %f9, %f1, %f194;
	neg.ftz.f32 	%f196, %f189;
	fma.rn.ftz.f32 	%f197, %f6, %f7, %f196;
	mul.ftz.f32 	%f198, %f5, %f7;
	mul.ftz.f32 	%f199, %f8, %f1;
	mul.ftz.f32 	%f200, %f2, %f4;
	neg.ftz.f32 	%f201, %f200;
	fma.rn.ftz.f32 	%f202, %f1, %f5, %f201;
	neg.ftz.f32 	%f203, %f199;
	fma.rn.ftz.f32 	%f204, %f7, %f2, %f203;
	neg.ftz.f32 	%f205, %f198;
	fma.rn.ftz.f32 	%f206, %f4, %f8, %f205;
	mul.ftz.f32 	%f207, %f4, %f188;
	fma.rn.ftz.f32 	%f208, %f1, %f186, %f207;
	fma.rn.ftz.f32 	%f209, %f7, %f184, %f208;
	mov.f32 	%f210, 0f3F800000;
	div.approx.ftz.f32 	%f211, %f210, %f209;
	mul.ftz.f32 	%f212, %f211, %f184;
	mul.ftz.f32 	%f213, %f211, %f188;
	mul.ftz.f32 	%f214, %f211, %f186;
	mul.ftz.f32 	%f215, %f211, %f197;
	mul.ftz.f32 	%f216, %f211, %f195;
	mul.ftz.f32 	%f217, %f211, %f193;
	ld.f32 	%f221, [%rd42+4];
	ld.f32 	%f222, [%rd42];
	ld.f32 	%f223, [%rd42+8];
	sub.ftz.f32 	%f225, %f222, %f176;
	sub.ftz.f32 	%f226, %f221, %f177;
	ld.f32 	%f227, [%rd42+24];
	ld.f32 	%f228, [%rd42+16];
	ld.f32 	%f229, [%rd42+20];
	ld.f32 	%f230, [%rd42+40];
	ld.f32 	%f231, [%rd42+32];
	ld.f32 	%f232, [%rd42+36];
	mul.ftz.f32 	%f233, %f226, %f217;
	mul.ftz.f32 	%f234, %f226, %f216;
	mul.ftz.f32 	%f235, %f226, %f215;
	mul.ftz.f32 	%f236, %f229, %f217;
	mul.ftz.f32 	%f237, %f229, %f216;
	mul.ftz.f32 	%f238, %f229, %f215;
	mul.ftz.f32 	%f239, %f217, %f232;
	mul.ftz.f32 	%f240, %f216, %f232;
	mul.ftz.f32 	%f241, %f215, %f232;
	ld.u32 	%r84, [%rd44+656];
	ld.u64 	%rd24, [%rd13+536];
	mul.wide.s32 	%rd45, %r84, 8;
	// inline asm
	mov.b64 {_,%r71}, %rd24;
	// inline asm
	// inline asm
	call (%rd25), _rt_buffer_get_id_64, (%r71, %r582, %r582, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd46, %rd45, %rd25;
	ld.u64 	%rd3, [%rd46+512];
	ld.u32 	%r85, [%rd44+668];
	ld.u64 	%rd30, [%rd13+528];
	mul.wide.s32 	%rd47, %r85, 16;
	// inline asm
	mov.b64 {_,%r75}, %rd30;
	// inline asm
	// inline asm
	call (%rd31), _rt_buffer_get_id_64, (%r75, %r582, %r582, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd48, %rd47, %rd31;
	ld.u64 	%rd36, [%rd48+512];
	// inline asm
	mov.b64 {_,%r79}, %rd36;
	// inline asm
	// inline asm
	call (%rd37), _rt_buffer_get_id_64, (%r79, %r582, %r582, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	ld.u32 	%r86, [%rd37+508];
	mul.wide.s32 	%rd49, %r86, 1431655766;
	shr.u64 	%rd50, %rd49, 63;
	cvt.u32.u64	%r87, %rd50;
	shr.u64 	%rd51, %rd49, 32;
	cvt.u32.u64	%r88, %rd51;
	add.s32 	%r1, %r88, %r87;
	ld.u32 	%r89, [%rd11];
	setp.eq.s32	%p1, %r89, 1;
	@%p1 bra 	LBB3_11;
	bra.uni 	LBB3_1;
LBB3_11:
	ld.u32 	%r811, [%rd11+36];
	cvt.rn.f32.s32	%f261, %r811;
	rcp.approx.ftz.f32 	%f262, %f261;
	mul.ftz.f32 	%f31, %f262, 0f3F000000;
	ld.u32 	%r810, [%rd11+32];
	setp.gt.s32	%p8, %r811, 1;
	selp.u32	%r235, 1, 0, %p8;
	setp.gt.s32	%p9, %r811, 2;
	selp.b32	%r236, 2, 1, %p8;
	selp.b32	%r237, %r236, %r235, %p9;
	setp.gt.s32	%p10, %r811, 4;
	selp.u32	%r238, 1, 0, %p10;
	setp.gt.s32	%p11, %r811, 8;
	selp.u32	%r239, 1, 0, %p11;
	setp.gt.s32	%p12, %r811, 16;
	selp.u32	%r240, 1, 0, %p12;
	setp.gt.s32	%p13, %r811, 32;
	selp.u32	%r241, 1, 0, %p13;
	setp.gt.s32	%p14, %r811, 64;
	selp.u32	%r242, 1, 0, %p14;
	setp.gt.s32	%p15, %r811, 128;
	selp.u32	%r243, 1, 0, %p15;
	setp.gt.s32	%p16, %r811, 256;
	selp.u32	%r244, 1, 0, %p16;
	setp.gt.s32	%p17, %r811, 512;
	selp.u32	%r245, 1, 0, %p17;
	setp.gt.s32	%p18, %r811, 1024;
	selp.u32	%r246, 1, 0, %p18;
	setp.gt.s32	%p19, %r811, 2048;
	selp.u32	%r247, 1, 0, %p19;
	setp.gt.s32	%p20, %r811, 4096;
	selp.u32	%r248, 1, 0, %p20;
	setp.gt.s32	%p21, %r811, 8192;
	selp.u32	%r249, 1, 0, %p21;
	setp.gt.s32	%p22, %r811, 16384;
	selp.u32	%r250, 1, 0, %p22;
	setp.gt.s32	%p23, %r811, 32768;
	selp.u32	%r251, 1, 0, %p23;
	setp.gt.s32	%p24, %r811, 65536;
	selp.u32	%r252, 1, 0, %p24;
	setp.gt.s32	%p25, %r811, 131072;
	selp.u32	%r253, 1, 0, %p25;
	setp.gt.s32	%p26, %r811, 262144;
	selp.u32	%r254, 1, 0, %p26;
	setp.gt.s32	%p27, %r811, 524288;
	selp.u32	%r255, 1, 0, %p27;
	setp.gt.s32	%p28, %r811, 1048576;
	selp.u32	%r256, 1, 0, %p28;
	setp.gt.s32	%p29, %r811, 2097152;
	selp.u32	%r257, 1, 0, %p29;
	setp.gt.s32	%p30, %r811, 4194304;
	selp.u32	%r258, 1, 0, %p30;
	setp.gt.s32	%p31, %r811, 8388608;
	selp.u32	%r259, 1, 0, %p31;
	setp.gt.s32	%p32, %r811, 16777216;
	selp.u32	%r260, 1, 0, %p32;
	setp.gt.s32	%p33, %r811, 33554432;
	selp.u32	%r261, 1, 0, %p33;
	setp.gt.s32	%p34, %r811, 67108864;
	selp.u32	%r262, 1, 0, %p34;
	setp.gt.s32	%p35, %r811, 134217728;
	selp.u32	%r263, 1, 0, %p35;
	setp.gt.s32	%p36, %r811, 268435456;
	selp.u32	%r264, 1, 0, %p36;
	setp.gt.s32	%p37, %r811, 536870912;
	selp.u32	%r265, 1, 0, %p37;
	setp.gt.s32	%p38, %r811, 1073741824;
	selp.u32	%r266, 1, 0, %p38;
	add.s32 	%r267, %r239, %r238;
	add.s32 	%r268, %r267, %r240;
	add.s32 	%r269, %r268, %r241;
	add.s32 	%r270, %r269, %r242;
	add.s32 	%r271, %r270, %r243;
	add.s32 	%r272, %r271, %r244;
	add.s32 	%r273, %r272, %r245;
	add.s32 	%r274, %r273, %r246;
	add.s32 	%r275, %r274, %r247;
	add.s32 	%r276, %r275, %r248;
	add.s32 	%r277, %r276, %r249;
	add.s32 	%r278, %r277, %r250;
	add.s32 	%r279, %r278, %r251;
	add.s32 	%r280, %r279, %r252;
	add.s32 	%r281, %r280, %r253;
	add.s32 	%r282, %r281, %r254;
	add.s32 	%r283, %r282, %r255;
	add.s32 	%r284, %r283, %r256;
	add.s32 	%r285, %r284, %r257;
	add.s32 	%r286, %r285, %r258;
	add.s32 	%r287, %r286, %r259;
	add.s32 	%r288, %r287, %r260;
	add.s32 	%r289, %r288, %r261;
	add.s32 	%r290, %r289, %r262;
	add.s32 	%r291, %r290, %r263;
	add.s32 	%r292, %r291, %r264;
	add.s32 	%r293, %r292, %r265;
	add.s32 	%r294, %r293, %r266;
	add.s32 	%r295, %r294, %r237;
	mov.u32 	%r296, 0;
	max.s32 	%r807, %r295, %r296;
	mov.f32 	%f738, 0f00000000;
	setp.lt.s32	%p39, %r807, 1;
	@%p39 bra 	LBB3_80;
	mov.f32 	%f736, 0f3F800000;
	mov.u32 	%r808, %r810;
LBB3_13:
	and.b32  	%r297, %r808, 1;
	shr.u32 	%r808, %r808, 1;
	mul.ftz.f32 	%f736, %f736, 0f3F000000;
	cvt.rn.f32.s32	%f263, %r297;
	fma.rn.ftz.f32 	%f738, %f736, %f263, %f738;
	add.s32 	%r807, %r807, -1;
	setp.ne.s32	%p40, %r807, 0;
	@%p40 bra 	LBB3_13;
	bra.uni 	LBB3_14;
LBB3_1:
	ld.u32 	%r2, [%rd11+16];
	setp.lt.s32	%p2, %r2, 8;
	ld.u32 	%r806, [%rd11+20];
	@%p2 bra 	LBB3_3;
	bra.uni 	LBB3_2;
LBB3_3:
	mov.f32 	%f735, 0f00000000;
	setp.lt.s32	%p3, %r806, 1;
	@%p3 bra 	LBB3_78;
	add.s64 	%rd1, %rd13, 512;
	and.b32  	%r215, %r2, 63;
	ld.u64 	%rd52, [%rd1];
	mul.wide.u32 	%rd58, %r215, 4;
	// inline asm
	mov.b64 {_,%r211}, %rd52;
	// inline asm
	// inline asm
	call (%rd53), _rt_buffer_get_id_64, (%r211, %r582, %r582, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd59, %rd58, %rd53;
	ld.u32 	%r4, [%rd59+512];
	cvt.rn.f32.s32	%f253, %r4;
	rcp.approx.ftz.f32 	%f20, %f253;
	mov.f32 	%f732, %f20;
LBB3_5:
	setp.ne.s32	%p4, %r4, 0;
	selp.b32	%r216, %r4, %r806, %p4;
	rem.s32 	%r217, %r806, %r216;
	cvt.rn.f32.s32	%f254, %r217;
	fma.rn.ftz.f32 	%f735, %f732, %f254, %f735;
	mul.ftz.f32 	%f732, %f20, %f732;
	selp.b32	%r218, %r4, -2147483648, %p4;
	div.s32 	%r806, %r806, %r218;
	setp.gt.s32	%p5, %r806, 0;
	@%p5 bra 	LBB3_5;
	bra.uni 	LBB3_6;
LBB3_2:
	shl.b32 	%r90, %r806, 4;
	add.s32 	%r91, %r90, -1556008596;
	add.s32 	%r92, %r806, -1640531527;
	xor.b32  	%r93, %r91, %r92;
	shr.u32 	%r94, %r806, 5;
	add.s32 	%r95, %r94, -939442524;
	xor.b32  	%r96, %r93, %r95;
	add.s32 	%r97, %r96, %r2;
	shl.b32 	%r98, %r97, 4;
	add.s32 	%r99, %r98, -1383041155;
	add.s32 	%r100, %r97, -1640531527;
	xor.b32  	%r101, %r99, %r100;
	shr.u32 	%r102, %r97, 5;
	add.s32 	%r103, %r102, 2123724318;
	xor.b32  	%r104, %r101, %r103;
	add.s32 	%r105, %r104, %r806;
	shl.b32 	%r106, %r105, 4;
	add.s32 	%r107, %r106, -1556008596;
	add.s32 	%r108, %r105, 1013904242;
	xor.b32  	%r109, %r107, %r108;
	shr.u32 	%r110, %r105, 5;
	add.s32 	%r111, %r110, -939442524;
	xor.b32  	%r112, %r109, %r111;
	add.s32 	%r113, %r112, %r97;
	shl.b32 	%r114, %r113, 4;
	add.s32 	%r115, %r114, -1383041155;
	add.s32 	%r116, %r113, 1013904242;
	xor.b32  	%r117, %r115, %r116;
	shr.u32 	%r118, %r113, 5;
	add.s32 	%r119, %r118, 2123724318;
	xor.b32  	%r120, %r117, %r119;
	add.s32 	%r121, %r120, %r105;
	shl.b32 	%r122, %r121, 4;
	add.s32 	%r123, %r122, -1556008596;
	add.s32 	%r124, %r121, -626627285;
	xor.b32  	%r125, %r123, %r124;
	shr.u32 	%r126, %r121, 5;
	add.s32 	%r127, %r126, -939442524;
	xor.b32  	%r128, %r125, %r127;
	add.s32 	%r129, %r128, %r113;
	shl.b32 	%r130, %r129, 4;
	add.s32 	%r131, %r130, -1383041155;
	add.s32 	%r132, %r129, -626627285;
	xor.b32  	%r133, %r131, %r132;
	shr.u32 	%r134, %r129, 5;
	add.s32 	%r135, %r134, 2123724318;
	xor.b32  	%r136, %r133, %r135;
	add.s32 	%r137, %r136, %r121;
	shl.b32 	%r138, %r137, 4;
	add.s32 	%r139, %r138, -1556008596;
	add.s32 	%r140, %r137, 2027808484;
	xor.b32  	%r141, %r139, %r140;
	shr.u32 	%r142, %r137, 5;
	add.s32 	%r143, %r142, -939442524;
	xor.b32  	%r144, %r141, %r143;
	add.s32 	%r145, %r144, %r129;
	shl.b32 	%r146, %r145, 4;
	add.s32 	%r147, %r146, -1383041155;
	add.s32 	%r148, %r145, 2027808484;
	xor.b32  	%r149, %r147, %r148;
	shr.u32 	%r150, %r145, 5;
	add.s32 	%r151, %r150, 2123724318;
	xor.b32  	%r152, %r149, %r151;
	add.s32 	%r153, %r152, %r137;
	shl.b32 	%r154, %r153, 4;
	add.s32 	%r155, %r154, -1556008596;
	add.s32 	%r156, %r153, 387276957;
	xor.b32  	%r157, %r155, %r156;
	shr.u32 	%r158, %r153, 5;
	add.s32 	%r159, %r158, -939442524;
	xor.b32  	%r160, %r157, %r159;
	add.s32 	%r161, %r160, %r145;
	shl.b32 	%r162, %r161, 4;
	add.s32 	%r163, %r162, -1383041155;
	add.s32 	%r164, %r161, 387276957;
	xor.b32  	%r165, %r163, %r164;
	shr.u32 	%r166, %r161, 5;
	add.s32 	%r167, %r166, 2123724318;
	xor.b32  	%r168, %r165, %r167;
	add.s32 	%r169, %r168, %r153;
	shl.b32 	%r170, %r169, 4;
	add.s32 	%r171, %r170, -1556008596;
	add.s32 	%r172, %r169, -1253254570;
	xor.b32  	%r173, %r171, %r172;
	shr.u32 	%r174, %r169, 5;
	add.s32 	%r175, %r174, -939442524;
	xor.b32  	%r176, %r173, %r175;
	add.s32 	%r177, %r176, %r161;
	shl.b32 	%r178, %r177, 4;
	add.s32 	%r179, %r178, -1383041155;
	add.s32 	%r180, %r177, -1253254570;
	xor.b32  	%r181, %r179, %r180;
	shr.u32 	%r182, %r177, 5;
	add.s32 	%r183, %r182, 2123724318;
	xor.b32  	%r184, %r181, %r183;
	add.s32 	%r185, %r184, %r169;
	shl.b32 	%r186, %r185, 4;
	add.s32 	%r187, %r186, -1556008596;
	add.s32 	%r188, %r185, 1401181199;
	xor.b32  	%r189, %r187, %r188;
	shr.u32 	%r190, %r185, 5;
	add.s32 	%r191, %r190, -939442524;
	xor.b32  	%r192, %r189, %r191;
	add.s32 	%r193, %r192, %r177;
	shl.b32 	%r194, %r193, 4;
	add.s32 	%r195, %r194, -1383041155;
	add.s32 	%r196, %r193, 1401181199;
	xor.b32  	%r197, %r195, %r196;
	shr.u32 	%r198, %r193, 5;
	add.s32 	%r199, %r198, 2123724318;
	xor.b32  	%r200, %r197, %r199;
	add.s32 	%r201, %r200, %r185;
	shl.b32 	%r202, %r201, 4;
	add.s32 	%r203, %r202, 591475052;
	add.s32 	%r204, %r201, 1908133320;
	xor.b32  	%r205, %r203, %r204;
	shr.u32 	%r206, %r201, 5;
	add.s32 	%r207, %r206, 1208041124;
	xor.b32  	%r208, %r205, %r207;
	add.s32 	%r209, %r208, %r193;
	and.b32  	%r210, %r209, 2147483647;
	cvt.rn.f32.s32	%f251, %r210;
	mul.ftz.f32 	%f735, %f251, 0f30000000;
	bra.uni 	LBB3_10;
LBB3_80:
LBB3_14:
	add.ftz.f32 	%f264, %f31, %f738;
	ld.v2.f32 	{%f740, %f741}, [%rd11+40];
	add.ftz.f32 	%f265, %f264, %f740;
	cvt.rmi.ftz.f32.f32	%f266, %f265;
	sub.ftz.f32 	%f267, %f265, %f266;
	mov.f32 	%f268, 0f3727C5AC;
	max.ftz.f32 	%f269, %f267, %f268;
	mov.f32 	%f270, 0f3F7FFF58;
	min.ftz.f32 	%f735, %f269, %f270;
	ld.u32 	%r809, [%rd11+16];
	bra.uni 	LBB3_15;
LBB3_78:
LBB3_6:
	setp.eq.s32	%p6, %r2, 1;
	@%p6 bra 	LBB3_9;
	bra.uni 	LBB3_7;
LBB3_9:
	ld.v2.u32 	{%r219, %r220}, [%rd11+24];
	mul.wide.s32 	%rd60, %r220, -2032597691;
	shr.u64 	%rd61, %rd60, 32;
	cvt.u32.u64	%r221, %rd61;
	add.s32 	%r222, %r221, %r220;
	shr.u32 	%r223, %r222, 31;
	shr.s32 	%r224, %r222, 7;
	add.s32 	%r225, %r224, %r223;
	mul.lo.s32 	%r226, %r225, 243;
	sub.s32 	%r227, %r220, %r226;
	cvt.rn.f32.s32	%f255, %r227;
	neg.ftz.f32 	%f256, %f255;
	fma.rn.ftz.f32 	%f735, %f735, 0f43730000, %f256;
	bra.uni 	LBB3_10;
LBB3_7:
	setp.ne.s32	%p7, %r2, 0;
	@%p7 bra 	LBB3_79;
	ld.v2.u32 	{%r228, %r229}, [%rd11+24];
	shr.s32 	%r230, %r228, 31;
	shr.u32 	%r231, %r230, 24;
	add.s32 	%r232, %r228, %r231;
	and.b32  	%r233, %r232, -256;
	sub.s32 	%r234, %r228, %r233;
	cvt.rn.f32.s32	%f257, %r234;
	neg.ftz.f32 	%f258, %f257;
	fma.rn.ftz.f32 	%f735, %f735, 0f43800000, %f258;
	bra.uni 	LBB3_10;
LBB3_79:
LBB3_10:
	add.s32 	%r809, %r2, 1;
	ld.u32 	%r810, [%rd11+32];
	ld.u32 	%r811, [%rd11+36];
	ld.v2.f32 	{%f740, %f741}, [%rd11+40];
LBB3_15:
	mul.ftz.f32 	%f218, %f211, %f206;
	mul.ftz.f32 	%f219, %f211, %f204;
	mul.ftz.f32 	%f220, %f211, %f202;
	sub.ftz.f32 	%f224, %f223, %f178;
	fma.rn.ftz.f32 	%f242, %f225, %f214, %f235;
	fma.rn.ftz.f32 	%f243, %f225, %f213, %f234;
	fma.rn.ftz.f32 	%f244, %f225, %f212, %f233;
	fma.rn.ftz.f32 	%f245, %f228, %f214, %f238;
	fma.rn.ftz.f32 	%f246, %f228, %f213, %f237;
	fma.rn.ftz.f32 	%f247, %f228, %f212, %f236;
	fma.rn.ftz.f32 	%f248, %f214, %f231, %f241;
	fma.rn.ftz.f32 	%f249, %f213, %f231, %f240;
	fma.rn.ftz.f32 	%f250, %f212, %f231, %f239;
	ld.v2.u32 	{%r50, %r51}, [%rd11+24];
	ld.u32 	%r49, [%rd11+20];
	ld.v2.f32 	{%f43, %f44}, [%rd11+8];
	ld.u32 	%r47, [%rd11];
	setp.lt.s32	%p41, %r1, 1;
	@%p41 bra 	LBB3_16;
	// inline asm
	mov.b64 {_,%r300}, %rd3;
	// inline asm
	mov.u32 	%r301, 1;
	// inline asm
	call (%rd63), _rt_buffer_get_id_64, (%r300, %r301, %r301, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	mov.u32 	%r814, 0;
	mov.u32 	%r812, %r1;
LBB3_18:
	add.s32 	%r304, %r814, %r812;
	shr.u32 	%r305, %r304, 31;
	add.s32 	%r306, %r304, %r305;
	shr.s32 	%r307, %r306, 1;
	add.s32 	%r308, %r307, 1;
	mul.wide.s32 	%rd68, %r308, 4;
	add.s64 	%rd69, %rd68, %rd63;
	ld.f32 	%f271, [%rd69+512];
	setp.lt.ftz.f32	%p42, %f735, %f271;
	selp.b32	%r812, %r307, %r812, %p42;
	selp.b32	%r814, %r814, %r308, %p42;
	setp.lt.s32	%p43, %r814, %r812;
	@%p43 bra 	LBB3_18;
	bra.uni 	LBB3_19;
LBB3_16:
	mov.u32 	%r814, 0;
LBB3_19:
	fma.rn.ftz.f32 	%f12, %f224, %f220, %f244;
	fma.rn.ftz.f32 	%f11, %f224, %f219, %f243;
	fma.rn.ftz.f32 	%f10, %f224, %f218, %f242;
	fma.rn.ftz.f32 	%f15, %f227, %f220, %f247;
	fma.rn.ftz.f32 	%f14, %f227, %f219, %f246;
	fma.rn.ftz.f32 	%f13, %f227, %f218, %f245;
	fma.rn.ftz.f32 	%f18, %f220, %f230, %f250;
	fma.rn.ftz.f32 	%f17, %f219, %f230, %f249;
	fma.rn.ftz.f32 	%f16, %f218, %f230, %f248;
	cvt.s64.s32	%rd2, %r84;
	cvt.s64.s32	%rd4, %r85;
	add.s32 	%r309, %r1, -1;
	setp.gt.s32	%p44, %r814, %r309;
	selp.b32	%r30, %r309, %r814, %p44;
	setp.eq.s32	%p45, %r47, 1;
	@%p45 bra 	LBB3_39;
	bra.uni 	LBB3_20;
LBB3_39:
	cvt.rn.f32.s32	%f290, %r811;
	rcp.approx.ftz.f32 	%f291, %f290;
	mul.ftz.f32 	%f70, %f291, 0f3F000000;
	cvt.rn.f32.s32	%f292, %r810;
	fma.rn.ftz.f32 	%f71, %f292, %f291, %f70;
	setp.gt.s32	%p58, %r811, 1;
	selp.u32	%r600, 1, 0, %p58;
	setp.gt.s32	%p59, %r811, 2;
	selp.b32	%r601, 2, 1, %p58;
	selp.b32	%r602, %r601, %r600, %p59;
	setp.gt.s32	%p60, %r811, 4;
	selp.u32	%r603, 1, 0, %p60;
	setp.gt.s32	%p61, %r811, 8;
	selp.u32	%r604, 1, 0, %p61;
	setp.gt.s32	%p62, %r811, 16;
	selp.u32	%r605, 1, 0, %p62;
	setp.gt.s32	%p63, %r811, 32;
	selp.u32	%r606, 1, 0, %p63;
	setp.gt.s32	%p64, %r811, 64;
	selp.u32	%r607, 1, 0, %p64;
	setp.gt.s32	%p65, %r811, 128;
	selp.u32	%r608, 1, 0, %p65;
	setp.gt.s32	%p66, %r811, 256;
	selp.u32	%r609, 1, 0, %p66;
	setp.gt.s32	%p67, %r811, 512;
	selp.u32	%r610, 1, 0, %p67;
	setp.gt.s32	%p68, %r811, 1024;
	selp.u32	%r611, 1, 0, %p68;
	setp.gt.s32	%p69, %r811, 2048;
	selp.u32	%r612, 1, 0, %p69;
	setp.gt.s32	%p70, %r811, 4096;
	selp.u32	%r613, 1, 0, %p70;
	setp.gt.s32	%p71, %r811, 8192;
	selp.u32	%r614, 1, 0, %p71;
	setp.gt.s32	%p72, %r811, 16384;
	selp.u32	%r615, 1, 0, %p72;
	setp.gt.s32	%p73, %r811, 32768;
	selp.u32	%r616, 1, 0, %p73;
	setp.gt.s32	%p74, %r811, 65536;
	selp.u32	%r617, 1, 0, %p74;
	setp.gt.s32	%p75, %r811, 131072;
	selp.u32	%r618, 1, 0, %p75;
	setp.gt.s32	%p76, %r811, 262144;
	selp.u32	%r619, 1, 0, %p76;
	setp.gt.s32	%p77, %r811, 524288;
	selp.u32	%r620, 1, 0, %p77;
	setp.gt.s32	%p78, %r811, 1048576;
	selp.u32	%r621, 1, 0, %p78;
	setp.gt.s32	%p79, %r811, 2097152;
	selp.u32	%r622, 1, 0, %p79;
	setp.gt.s32	%p80, %r811, 4194304;
	selp.u32	%r623, 1, 0, %p80;
	setp.gt.s32	%p81, %r811, 8388608;
	selp.u32	%r624, 1, 0, %p81;
	setp.gt.s32	%p82, %r811, 16777216;
	selp.u32	%r625, 1, 0, %p82;
	setp.gt.s32	%p83, %r811, 33554432;
	selp.u32	%r626, 1, 0, %p83;
	setp.gt.s32	%p84, %r811, 67108864;
	selp.u32	%r627, 1, 0, %p84;
	setp.gt.s32	%p85, %r811, 134217728;
	selp.u32	%r628, 1, 0, %p85;
	setp.gt.s32	%p86, %r811, 268435456;
	selp.u32	%r629, 1, 0, %p86;
	setp.gt.s32	%p87, %r811, 536870912;
	selp.u32	%r630, 1, 0, %p87;
	setp.gt.s32	%p88, %r811, 1073741824;
	selp.u32	%r631, 1, 0, %p88;
	add.s32 	%r632, %r604, %r603;
	add.s32 	%r633, %r632, %r605;
	add.s32 	%r634, %r633, %r606;
	add.s32 	%r635, %r634, %r607;
	add.s32 	%r636, %r635, %r608;
	add.s32 	%r637, %r636, %r609;
	add.s32 	%r638, %r637, %r610;
	add.s32 	%r639, %r638, %r611;
	add.s32 	%r640, %r639, %r612;
	add.s32 	%r641, %r640, %r613;
	add.s32 	%r642, %r641, %r614;
	add.s32 	%r643, %r642, %r615;
	add.s32 	%r644, %r643, %r616;
	add.s32 	%r645, %r644, %r617;
	add.s32 	%r646, %r645, %r618;
	add.s32 	%r647, %r646, %r619;
	add.s32 	%r648, %r647, %r620;
	add.s32 	%r649, %r648, %r621;
	add.s32 	%r650, %r649, %r622;
	add.s32 	%r651, %r650, %r623;
	add.s32 	%r652, %r651, %r624;
	add.s32 	%r653, %r652, %r625;
	add.s32 	%r654, %r653, %r626;
	add.s32 	%r655, %r654, %r627;
	add.s32 	%r656, %r655, %r628;
	add.s32 	%r657, %r656, %r629;
	add.s32 	%r658, %r657, %r630;
	add.s32 	%r659, %r658, %r631;
	add.s32 	%r660, %r659, %r602;
	mov.u32 	%r661, 0;
	max.s32 	%r818, %r660, %r661;
	mov.f32 	%f754, 0f00000000;
	setp.lt.s32	%p89, %r818, 1;
	@%p89 bra 	LBB3_86;
	mov.f32 	%f752, %f210;
	mov.u32 	%r819, %r810;
LBB3_41:
	and.b32  	%r662, %r819, 1;
	shr.u32 	%r819, %r819, 1;
	mul.ftz.f32 	%f752, %f752, 0f3F000000;
	cvt.rn.f32.s32	%f293, %r662;
	fma.rn.ftz.f32 	%f754, %f752, %f293, %f754;
	add.s32 	%r818, %r818, -1;
	setp.ne.s32	%p90, %r818, 0;
	@%p90 bra 	LBB3_41;
	bra.uni 	LBB3_42;
LBB3_20:
	setp.lt.s32	%p46, %r809, 8;
	@%p46 bra 	LBB3_22;
	bra.uni 	LBB3_21;
LBB3_22:
	mov.f32 	%f750, 0f00000000;
	setp.lt.s32	%p47, %r49, 1;
	@%p47 bra 	LBB3_81;
	and.b32  	%r439, %r809, 63;
	// inline asm
	mov.b64 {_,%r431}, %rd86;
	// inline asm
	// inline asm
	call (%rd71), _rt_buffer_get_id_64, (%r431, %r582, %r582, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	ld.u64 	%rd76, [%rd71+512];
	mul.wide.u32 	%rd82, %r439, 4;
	// inline asm
	mov.b64 {_,%r435}, %rd76;
	// inline asm
	// inline asm
	call (%rd77), _rt_buffer_get_id_64, (%r435, %r582, %r582, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd83, %rd82, %rd77;
	ld.u32 	%r31, [%rd83+512];
	cvt.rn.f32.s32	%f274, %r31;
	rcp.approx.ftz.f32 	%f46, %f274;
	mov.f32 	%f742, %f46;
	mov.u32 	%r815, %r49;
LBB3_24:
	setp.ne.s32	%p48, %r31, 0;
	selp.b32	%r440, %r31, %r815, %p48;
	rem.s32 	%r441, %r815, %r440;
	cvt.rn.f32.s32	%f275, %r441;
	fma.rn.ftz.f32 	%f750, %f742, %f275, %f750;
	mul.ftz.f32 	%f742, %f46, %f742;
	selp.b32	%r442, %r31, -2147483648, %p48;
	div.s32 	%r815, %r815, %r442;
	setp.gt.s32	%p49, %r815, 0;
	@%p49 bra 	LBB3_24;
	bra.uni 	LBB3_25;
LBB3_21:
	shl.b32 	%r310, %r49, 4;
	add.s32 	%r311, %r310, -1556008596;
	add.s32 	%r312, %r49, -1640531527;
	xor.b32  	%r313, %r311, %r312;
	shr.u32 	%r314, %r49, 5;
	add.s32 	%r315, %r314, -939442524;
	xor.b32  	%r316, %r313, %r315;
	add.s32 	%r317, %r316, %r809;
	shl.b32 	%r318, %r317, 4;
	add.s32 	%r319, %r318, -1383041155;
	add.s32 	%r320, %r317, -1640531527;
	xor.b32  	%r321, %r319, %r320;
	shr.u32 	%r322, %r317, 5;
	add.s32 	%r323, %r322, 2123724318;
	xor.b32  	%r324, %r321, %r323;
	add.s32 	%r325, %r324, %r49;
	shl.b32 	%r326, %r325, 4;
	add.s32 	%r327, %r326, -1556008596;
	add.s32 	%r328, %r325, 1013904242;
	xor.b32  	%r329, %r327, %r328;
	shr.u32 	%r330, %r325, 5;
	add.s32 	%r331, %r330, -939442524;
	xor.b32  	%r332, %r329, %r331;
	add.s32 	%r333, %r332, %r317;
	shl.b32 	%r334, %r333, 4;
	add.s32 	%r335, %r334, -1383041155;
	add.s32 	%r336, %r333, 1013904242;
	xor.b32  	%r337, %r335, %r336;
	shr.u32 	%r338, %r333, 5;
	add.s32 	%r339, %r338, 2123724318;
	xor.b32  	%r340, %r337, %r339;
	add.s32 	%r341, %r340, %r325;
	shl.b32 	%r342, %r341, 4;
	add.s32 	%r343, %r342, -1556008596;
	add.s32 	%r344, %r341, -626627285;
	xor.b32  	%r345, %r343, %r344;
	shr.u32 	%r346, %r341, 5;
	add.s32 	%r347, %r346, -939442524;
	xor.b32  	%r348, %r345, %r347;
	add.s32 	%r349, %r348, %r333;
	shl.b32 	%r350, %r349, 4;
	add.s32 	%r351, %r350, -1383041155;
	add.s32 	%r352, %r349, -626627285;
	xor.b32  	%r353, %r351, %r352;
	shr.u32 	%r354, %r349, 5;
	add.s32 	%r355, %r354, 2123724318;
	xor.b32  	%r356, %r353, %r355;
	add.s32 	%r357, %r356, %r341;
	shl.b32 	%r358, %r357, 4;
	add.s32 	%r359, %r358, -1556008596;
	add.s32 	%r360, %r357, 2027808484;
	xor.b32  	%r361, %r359, %r360;
	shr.u32 	%r362, %r357, 5;
	add.s32 	%r363, %r362, -939442524;
	xor.b32  	%r364, %r361, %r363;
	add.s32 	%r365, %r364, %r349;
	shl.b32 	%r366, %r365, 4;
	add.s32 	%r367, %r366, -1383041155;
	add.s32 	%r368, %r365, 2027808484;
	xor.b32  	%r369, %r367, %r368;
	shr.u32 	%r370, %r365, 5;
	add.s32 	%r371, %r370, 2123724318;
	xor.b32  	%r372, %r369, %r371;
	add.s32 	%r373, %r372, %r357;
	shl.b32 	%r374, %r373, 4;
	add.s32 	%r375, %r374, -1556008596;
	add.s32 	%r376, %r373, 387276957;
	xor.b32  	%r377, %r375, %r376;
	shr.u32 	%r378, %r373, 5;
	add.s32 	%r379, %r378, -939442524;
	xor.b32  	%r380, %r377, %r379;
	add.s32 	%r381, %r380, %r365;
	shl.b32 	%r382, %r381, 4;
	add.s32 	%r383, %r382, -1383041155;
	add.s32 	%r384, %r381, 387276957;
	xor.b32  	%r385, %r383, %r384;
	shr.u32 	%r386, %r381, 5;
	add.s32 	%r387, %r386, 2123724318;
	xor.b32  	%r388, %r385, %r387;
	add.s32 	%r389, %r388, %r373;
	shl.b32 	%r390, %r389, 4;
	add.s32 	%r391, %r390, -1556008596;
	add.s32 	%r392, %r389, -1253254570;
	xor.b32  	%r393, %r391, %r392;
	shr.u32 	%r394, %r389, 5;
	add.s32 	%r395, %r394, -939442524;
	xor.b32  	%r396, %r393, %r395;
	add.s32 	%r397, %r396, %r381;
	shl.b32 	%r398, %r397, 4;
	add.s32 	%r399, %r398, -1383041155;
	add.s32 	%r400, %r397, -1253254570;
	xor.b32  	%r401, %r399, %r400;
	shr.u32 	%r402, %r397, 5;
	add.s32 	%r403, %r402, 2123724318;
	xor.b32  	%r404, %r401, %r403;
	add.s32 	%r405, %r404, %r389;
	shl.b32 	%r406, %r405, 4;
	add.s32 	%r407, %r406, -1556008596;
	add.s32 	%r408, %r405, 1401181199;
	xor.b32  	%r409, %r407, %r408;
	shr.u32 	%r410, %r405, 5;
	add.s32 	%r411, %r410, -939442524;
	xor.b32  	%r412, %r409, %r411;
	add.s32 	%r413, %r412, %r397;
	shl.b32 	%r414, %r413, 4;
	add.s32 	%r415, %r414, -1383041155;
	add.s32 	%r416, %r413, 1401181199;
	xor.b32  	%r417, %r415, %r416;
	shr.u32 	%r418, %r413, 5;
	add.s32 	%r419, %r418, 2123724318;
	xor.b32  	%r420, %r417, %r419;
	add.s32 	%r421, %r420, %r405;
	shl.b32 	%r422, %r421, 4;
	add.s32 	%r423, %r422, 591475052;
	add.s32 	%r424, %r421, 1908133320;
	xor.b32  	%r425, %r423, %r424;
	shr.u32 	%r426, %r421, 5;
	add.s32 	%r427, %r426, 1208041124;
	xor.b32  	%r428, %r425, %r427;
	add.s32 	%r429, %r428, %r413;
	and.b32  	%r430, %r429, 2147483647;
	cvt.rn.f32.s32	%f272, %r430;
	mul.ftz.f32 	%f750, %f272, 0f30000000;
	bra.uni 	LBB3_29;
LBB3_86:
LBB3_42:
	add.ftz.f32 	%f294, %f70, %f754;
	add.ftz.f32 	%f295, %f741, %f294;
	add.ftz.f32 	%f296, %f740, %f71;
	cvt.rmi.ftz.f32.f32	%f297, %f296;
	cvt.rmi.ftz.f32.f32	%f298, %f295;
	sub.ftz.f32 	%f299, %f295, %f298;
	sub.ftz.f32 	%f300, %f296, %f297;
	mov.f32 	%f301, 0f3727C5AC;
	max.ftz.f32 	%f302, %f300, %f301;
	mov.f32 	%f303, 0f3F7FFF58;
	min.ftz.f32 	%f750, %f302, %f303;
	max.ftz.f32 	%f304, %f299, %f301;
	min.ftz.f32 	%f751, %f304, %f303;
	bra.uni 	LBB3_43;
LBB3_81:
LBB3_25:
	setp.eq.s32	%p50, %r809, 1;
	@%p50 bra 	LBB3_28;
	bra.uni 	LBB3_26;
LBB3_28:
	mul.wide.s32 	%rd84, %r51, -2032597691;
	shr.u64 	%rd85, %rd84, 32;
	cvt.u32.u64	%r443, %rd85;
	add.s32 	%r444, %r443, %r51;
	shr.u32 	%r445, %r444, 31;
	shr.s32 	%r446, %r444, 7;
	add.s32 	%r447, %r446, %r445;
	mul.lo.s32 	%r448, %r447, 243;
	sub.s32 	%r449, %r51, %r448;
	cvt.rn.f32.s32	%f276, %r449;
	neg.ftz.f32 	%f277, %f276;
	fma.rn.ftz.f32 	%f750, %f750, 0f43730000, %f277;
	add.s32 	%r816, %r809, 1;
	bra.uni 	LBB3_31;
LBB3_26:
	setp.ne.s32	%p51, %r809, 0;
	@%p51 bra 	LBB3_82;
	shr.s32 	%r450, %r50, 31;
	shr.u32 	%r451, %r450, 24;
	add.s32 	%r452, %r50, %r451;
	and.b32  	%r453, %r452, -256;
	sub.s32 	%r454, %r50, %r453;
	cvt.rn.f32.s32	%f278, %r454;
	neg.ftz.f32 	%f279, %f278;
	fma.rn.ftz.f32 	%f750, %f750, 0f43800000, %f279;
	add.s32 	%r816, %r809, 1;
	bra.uni 	LBB3_31;
LBB3_82:
LBB3_29:
	add.s32 	%r816, %r809, 1;
	setp.lt.s32	%p52, %r816, 8;
	@%p52 bra 	LBB3_83;
	bra.uni 	LBB3_30;
LBB3_83:
LBB3_31:
	mov.f32 	%f751, 0f00000000;
	setp.lt.s32	%p53, %r49, 1;
	@%p53 bra 	LBB3_84;
	and.b32  	%r584, %r816, 63;
	// inline asm
	mov.b64 {_,%r576}, %rd86;
	// inline asm
	// inline asm
	call (%rd87), _rt_buffer_get_id_64, (%r576, %r582, %r582, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	ld.u64 	%rd92, [%rd87+512];
	mul.wide.u32 	%rd98, %r584, 4;
	// inline asm
	mov.b64 {_,%r580}, %rd92;
	// inline asm
	// inline asm
	call (%rd93), _rt_buffer_get_id_64, (%r580, %r582, %r582, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd99, %rd98, %rd93;
	ld.u32 	%r37, [%rd99+512];
	cvt.rn.f32.s32	%f282, %r37;
	rcp.approx.ftz.f32 	%f58, %f282;
	mov.f32 	%f747, %f58;
	mov.u32 	%r817, %r49;
LBB3_33:
	setp.ne.s32	%p54, %r37, 0;
	selp.b32	%r585, %r37, %r817, %p54;
	rem.s32 	%r586, %r817, %r585;
	cvt.rn.f32.s32	%f283, %r586;
	fma.rn.ftz.f32 	%f751, %f747, %f283, %f751;
	mul.ftz.f32 	%f747, %f58, %f747;
	selp.b32	%r587, %r37, -2147483648, %p54;
	div.s32 	%r817, %r817, %r587;
	setp.gt.s32	%p55, %r817, 0;
	@%p55 bra 	LBB3_33;
	bra.uni 	LBB3_34;
LBB3_30:
	shl.b32 	%r455, %r49, 4;
	add.s32 	%r456, %r455, -1556008596;
	add.s32 	%r457, %r49, -1640531527;
	xor.b32  	%r458, %r456, %r457;
	shr.u32 	%r459, %r49, 5;
	add.s32 	%r460, %r459, -939442524;
	xor.b32  	%r461, %r458, %r460;
	add.s32 	%r462, %r461, %r816;
	shl.b32 	%r463, %r462, 4;
	add.s32 	%r464, %r463, -1383041155;
	add.s32 	%r465, %r462, -1640531527;
	xor.b32  	%r466, %r464, %r465;
	shr.u32 	%r467, %r462, 5;
	add.s32 	%r468, %r467, 2123724318;
	xor.b32  	%r469, %r466, %r468;
	add.s32 	%r470, %r469, %r49;
	shl.b32 	%r471, %r470, 4;
	add.s32 	%r472, %r471, -1556008596;
	add.s32 	%r473, %r470, 1013904242;
	xor.b32  	%r474, %r472, %r473;
	shr.u32 	%r475, %r470, 5;
	add.s32 	%r476, %r475, -939442524;
	xor.b32  	%r477, %r474, %r476;
	add.s32 	%r478, %r477, %r462;
	shl.b32 	%r479, %r478, 4;
	add.s32 	%r480, %r479, -1383041155;
	add.s32 	%r481, %r478, 1013904242;
	xor.b32  	%r482, %r480, %r481;
	shr.u32 	%r483, %r478, 5;
	add.s32 	%r484, %r483, 2123724318;
	xor.b32  	%r485, %r482, %r484;
	add.s32 	%r486, %r485, %r470;
	shl.b32 	%r487, %r486, 4;
	add.s32 	%r488, %r487, -1556008596;
	add.s32 	%r489, %r486, -626627285;
	xor.b32  	%r490, %r488, %r489;
	shr.u32 	%r491, %r486, 5;
	add.s32 	%r492, %r491, -939442524;
	xor.b32  	%r493, %r490, %r492;
	add.s32 	%r494, %r493, %r478;
	shl.b32 	%r495, %r494, 4;
	add.s32 	%r496, %r495, -1383041155;
	add.s32 	%r497, %r494, -626627285;
	xor.b32  	%r498, %r496, %r497;
	shr.u32 	%r499, %r494, 5;
	add.s32 	%r500, %r499, 2123724318;
	xor.b32  	%r501, %r498, %r500;
	add.s32 	%r502, %r501, %r486;
	shl.b32 	%r503, %r502, 4;
	add.s32 	%r504, %r503, -1556008596;
	add.s32 	%r505, %r502, 2027808484;
	xor.b32  	%r506, %r504, %r505;
	shr.u32 	%r507, %r502, 5;
	add.s32 	%r508, %r507, -939442524;
	xor.b32  	%r509, %r506, %r508;
	add.s32 	%r510, %r509, %r494;
	shl.b32 	%r511, %r510, 4;
	add.s32 	%r512, %r511, -1383041155;
	add.s32 	%r513, %r510, 2027808484;
	xor.b32  	%r514, %r512, %r513;
	shr.u32 	%r515, %r510, 5;
	add.s32 	%r516, %r515, 2123724318;
	xor.b32  	%r517, %r514, %r516;
	add.s32 	%r518, %r517, %r502;
	shl.b32 	%r519, %r518, 4;
	add.s32 	%r520, %r519, -1556008596;
	add.s32 	%r521, %r518, 387276957;
	xor.b32  	%r522, %r520, %r521;
	shr.u32 	%r523, %r518, 5;
	add.s32 	%r524, %r523, -939442524;
	xor.b32  	%r525, %r522, %r524;
	add.s32 	%r526, %r525, %r510;
	shl.b32 	%r527, %r526, 4;
	add.s32 	%r528, %r527, -1383041155;
	add.s32 	%r529, %r526, 387276957;
	xor.b32  	%r530, %r528, %r529;
	shr.u32 	%r531, %r526, 5;
	add.s32 	%r532, %r531, 2123724318;
	xor.b32  	%r533, %r530, %r532;
	add.s32 	%r534, %r533, %r518;
	shl.b32 	%r535, %r534, 4;
	add.s32 	%r536, %r535, -1556008596;
	add.s32 	%r537, %r534, -1253254570;
	xor.b32  	%r538, %r536, %r537;
	shr.u32 	%r539, %r534, 5;
	add.s32 	%r540, %r539, -939442524;
	xor.b32  	%r541, %r538, %r540;
	add.s32 	%r542, %r541, %r526;
	shl.b32 	%r543, %r542, 4;
	add.s32 	%r544, %r543, -1383041155;
	add.s32 	%r545, %r542, -1253254570;
	xor.b32  	%r546, %r544, %r545;
	shr.u32 	%r547, %r542, 5;
	add.s32 	%r548, %r547, 2123724318;
	xor.b32  	%r549, %r546, %r548;
	add.s32 	%r550, %r549, %r534;
	shl.b32 	%r551, %r550, 4;
	add.s32 	%r552, %r551, -1556008596;
	add.s32 	%r553, %r550, 1401181199;
	xor.b32  	%r554, %r552, %r553;
	shr.u32 	%r555, %r550, 5;
	add.s32 	%r556, %r555, -939442524;
	xor.b32  	%r557, %r554, %r556;
	add.s32 	%r558, %r557, %r542;
	shl.b32 	%r559, %r558, 4;
	add.s32 	%r560, %r559, -1383041155;
	add.s32 	%r561, %r558, 1401181199;
	xor.b32  	%r562, %r560, %r561;
	shr.u32 	%r563, %r558, 5;
	add.s32 	%r564, %r563, 2123724318;
	xor.b32  	%r565, %r562, %r564;
	add.s32 	%r566, %r565, %r550;
	shl.b32 	%r567, %r566, 4;
	add.s32 	%r568, %r567, 591475052;
	add.s32 	%r569, %r566, 1908133320;
	xor.b32  	%r570, %r568, %r569;
	shr.u32 	%r571, %r566, 5;
	add.s32 	%r572, %r571, 1208041124;
	xor.b32  	%r573, %r570, %r572;
	add.s32 	%r574, %r573, %r558;
	and.b32  	%r575, %r574, 2147483647;
	cvt.rn.f32.s32	%f280, %r575;
	mul.ftz.f32 	%f751, %f280, 0f30000000;
	bra.uni 	LBB3_38;
LBB3_84:
LBB3_34:
	setp.ne.s32	%p56, %r816, 0;
	@%p56 bra 	LBB3_36;
	bra.uni 	LBB3_35;
LBB3_36:
	setp.ne.s32	%p57, %r809, 0;
	@%p57 bra 	LBB3_85;
	bra.uni 	LBB3_37;
LBB3_85:
	bra.uni 	LBB3_38;
LBB3_35:
	shr.s32 	%r595, %r50, 31;
	shr.u32 	%r596, %r595, 24;
	add.s32 	%r597, %r50, %r596;
	and.b32  	%r598, %r597, -256;
	sub.s32 	%r599, %r50, %r598;
	cvt.rn.f32.s32	%f286, %r599;
	neg.ftz.f32 	%f287, %f286;
	fma.rn.ftz.f32 	%f751, %f751, 0f43800000, %f287;
	bra.uni 	LBB3_38;
LBB3_37:
	mul.wide.s32 	%rd100, %r51, -2032597691;
	shr.u64 	%rd101, %rd100, 32;
	cvt.u32.u64	%r588, %rd101;
	add.s32 	%r589, %r588, %r51;
	shr.u32 	%r590, %r589, 31;
	shr.s32 	%r591, %r589, 7;
	add.s32 	%r592, %r591, %r590;
	mul.lo.s32 	%r593, %r592, 243;
	sub.s32 	%r594, %r51, %r593;
	cvt.rn.f32.s32	%f284, %r594;
	neg.ftz.f32 	%f285, %f284;
	fma.rn.ftz.f32 	%f751, %f751, 0f43730000, %f285;
LBB3_38:
	add.s32 	%r809, %r809, 2;
LBB3_43:
	sqrt.approx.ftz.f32 	%f305, %f750;
	sub.ftz.f32 	%f307, %f210, %f305;
	sub.ftz.f32 	%f308, %f210, %f751;
	mul.ftz.f32 	%f309, %f308, %f305;
	mul.ftz.f32 	%f310, %f751, %f305;
	mul.lo.s32 	%r693, %r30, 3;
	mul.wide.s32 	%rd144, %r693, 4;
	// inline asm
	mov.b64 {_,%r663}, %rd36;
	// inline asm
	// inline asm
	call (%rd103), _rt_buffer_get_id_64, (%r663, %r582, %r582, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd145, %rd103, 512;
	add.s32 	%r694, %r693, 2;
	mul.wide.s32 	%rd146, %r694, 4;
	add.s64 	%rd147, %rd146, %rd145;
	add.s32 	%r695, %r693, 1;
	mul.wide.s32 	%rd148, %r695, 4;
	add.s64 	%rd149, %rd148, %rd145;
	add.s64 	%rd150, %rd144, %rd145;
	ld.u32 	%r696, [%rd150];
	ld.u32 	%r697, [%rd149];
	ld.u32 	%r698, [%rd147];
	// inline asm
	mov.b64 {_,%r667}, %rd86;
	// inline asm
	// inline asm
	call (%rd109), _rt_buffer_get_id_64, (%r667, %r582, %r582, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	ld.u64 	%rd114, [%rd109+528];
	shl.b64 	%rd151, %rd4, 4;
	// inline asm
	mov.b64 {_,%r671}, %rd114;
	// inline asm
	// inline asm
	call (%rd115), _rt_buffer_get_id_64, (%r671, %r582, %r582, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd152, %rd151, %rd115;
	ld.u64 	%rd120, [%rd152+520];
	mul.wide.s32 	%rd153, %r696, 52;
	// inline asm
	mov.b64 {_,%r675}, %rd120;
	// inline asm
	// inline asm
	call (%rd121), _rt_buffer_get_id_64, (%r675, %r582, %r582, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd154, %rd121, 512;
	mul.wide.s32 	%rd155, %r698, 52;
	add.s64 	%rd156, %rd155, %rd154;
	mul.wide.s32 	%rd157, %r697, 52;
	add.s64 	%rd158, %rd157, %rd154;
	add.s64 	%rd159, %rd153, %rd154;
	ld.f32 	%f311, [%rd159];
	ld.f32 	%f312, [%rd159+4];
	ld.f32 	%f313, [%rd159+8];
	ld.f32 	%f314, [%rd158];
	ld.f32 	%f315, [%rd158+4];
	ld.f32 	%f316, [%rd158+8];
	ld.f32 	%f317, [%rd156];
	ld.f32 	%f318, [%rd156+4];
	ld.f32 	%f319, [%rd156+8];
	mul.ftz.f32 	%f320, %f309, %f316;
	mul.ftz.f32 	%f321, %f309, %f314;
	mul.ftz.f32 	%f322, %f309, %f315;
	fma.rn.ftz.f32 	%f323, %f307, %f312, %f322;
	fma.rn.ftz.f32 	%f324, %f307, %f311, %f321;
	fma.rn.ftz.f32 	%f325, %f307, %f313, %f320;
	fma.rn.ftz.f32 	%f326, %f310, %f319, %f325;
	fma.rn.ftz.f32 	%f327, %f310, %f317, %f324;
	fma.rn.ftz.f32 	%f328, %f310, %f318, %f323;
	sub.ftz.f32 	%f329, %f314, %f311;
	sub.ftz.f32 	%f330, %f315, %f312;
	sub.ftz.f32 	%f331, %f316, %f313;
	sub.ftz.f32 	%f332, %f317, %f311;
	sub.ftz.f32 	%f333, %f319, %f313;
	sub.ftz.f32 	%f334, %f318, %f312;
	mul.ftz.f32 	%f335, %f331, %f334;
	neg.ftz.f32 	%f336, %f335;
	fma.rn.ftz.f32 	%f337, %f330, %f333, %f336;
	mul.ftz.f32 	%f338, %f329, %f333;
	neg.ftz.f32 	%f339, %f338;
	fma.rn.ftz.f32 	%f340, %f331, %f332, %f339;
	mul.ftz.f32 	%f341, %f330, %f332;
	neg.ftz.f32 	%f342, %f341;
	fma.rn.ftz.f32 	%f343, %f329, %f334, %f342;
	mul.ftz.f32 	%f344, %f340, %f340;
	fma.rn.ftz.f32 	%f345, %f337, %f337, %f344;
	fma.rn.ftz.f32 	%f346, %f343, %f343, %f345;
	rsqrt.approx.ftz.f32 	%f347, %f346;
	mul.ftz.f32 	%f348, %f347, %f337;
	mul.ftz.f32 	%f349, %f347, %f343;
	mul.ftz.f32 	%f350, %f347, %f340;
	neg.ftz.f32 	%f351, %f350;
	mul.ftz.f32 	%f352, %f349, %f348;
	neg.ftz.f32 	%f353, %f352;
	fma.rn.ftz.f32 	%f354, %f350, %f351, %f353;
	mul.ftz.f32 	%f355, %f349, %f349;
	neg.ftz.f32 	%f356, %f355;
	fma.rn.ftz.f32 	%f357, %f348, %f350, %f356;
	mul.ftz.f32 	%f358, %f350, %f349;
	fma.rn.ftz.f32 	%f359, %f348, %f348, %f358;
	mul.ftz.f32 	%f360, %f357, %f357;
	fma.rn.ftz.f32 	%f361, %f354, %f354, %f360;
	fma.rn.ftz.f32 	%f362, %f359, %f359, %f361;
	rsqrt.approx.ftz.f32 	%f363, %f362;
	mul.ftz.f32 	%f364, %f363, %f357;
	mul.ftz.f32 	%f365, %f363, %f359;
	mul.ftz.f32 	%f366, %f363, %f354;
	ld.f32 	%f367, [%rd159+36];
	ld.f32 	%f368, [%rd159+40];
	ld.f32 	%f369, [%rd158+36];
	ld.f32 	%f370, [%rd158+40];
	ld.f32 	%f371, [%rd156+36];
	ld.f32 	%f372, [%rd156+40];
	mul.ftz.f32 	%f373, %f309, %f370;
	mul.ftz.f32 	%f374, %f309, %f369;
	ld.u64 	%rd126, [%rd109+552];
	mul.lo.s64 	%rd160, %rd2, 160;
	// inline asm
	mov.b64 {_,%r679}, %rd126;
	// inline asm
	// inline asm
	call (%rd127), _rt_buffer_get_id_64, (%r679, %r582, %r582, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd161, %rd127, %rd160;
	ld.v4.f32 	{%f377, %f378, %f379, %f380}, [%rd161+608];
	ld.v4.f32 	{%f381, %f382, %f383, %f384}, [%rd161+624];
	ld.v4.f32 	{%f385, %f386, %f387, %f388}, [%rd161+640];
	ld.v4.f32 	{%f389, %f390, %f391, %f392}, [%rd161+544];
	ld.v4.f32 	{%f393, %f394, %f395, %f396}, [%rd161+560];
	mul.ftz.f32 	%f397, %f328, %f395;
	mul.ftz.f32 	%f398, %f328, %f393;
	mul.ftz.f32 	%f399, %f328, %f394;
	fma.rn.ftz.f32 	%f400, %f327, %f390, %f399;
	fma.rn.ftz.f32 	%f401, %f327, %f389, %f398;
	fma.rn.ftz.f32 	%f402, %f327, %f391, %f397;
	ld.v4.f32 	{%f403, %f404, %f405, %f406}, [%rd161+576];
	fma.rn.ftz.f32 	%f407, %f326, %f405, %f402;
	fma.rn.ftz.f32 	%f408, %f326, %f403, %f401;
	fma.rn.ftz.f32 	%f409, %f326, %f404, %f400;
	ld.v4.f32 	{%f410, %f411, %f412, %f413}, [%rd161+592];
	add.ftz.f32 	%f414, %f411, %f409;
	add.ftz.f32 	%f415, %f410, %f408;
	add.ftz.f32 	%f416, %f412, %f407;
	mul.ftz.f32 	%f417, %f350, %f386;
	mul.ftz.f32 	%f418, %f350, %f378;
	mul.ftz.f32 	%f419, %f350, %f382;
	fma.rn.ftz.f32 	%f420, %f348, %f381, %f419;
	fma.rn.ftz.f32 	%f421, %f348, %f377, %f418;
	fma.rn.ftz.f32 	%f422, %f348, %f385, %f417;
	fma.rn.ftz.f32 	%f423, %f349, %f387, %f422;
	fma.rn.ftz.f32 	%f424, %f349, %f379, %f421;
	fma.rn.ftz.f32 	%f425, %f349, %f383, %f420;
	mul.ftz.f32 	%f426, %f425, %f425;
	fma.rn.ftz.f32 	%f427, %f424, %f424, %f426;
	fma.rn.ftz.f32 	%f428, %f423, %f423, %f427;
	rsqrt.approx.ftz.f32 	%f429, %f428;
	mul.ftz.f32 	%f430, %f424, %f429;
	mul.ftz.f32 	%f431, %f425, %f429;
	mul.ftz.f32 	%f432, %f423, %f429;
	mul.ftz.f32 	%f433, %f364, %f395;
	mul.ftz.f32 	%f434, %f364, %f393;
	mul.ftz.f32 	%f435, %f364, %f394;
	fma.rn.ftz.f32 	%f436, %f366, %f390, %f435;
	fma.rn.ftz.f32 	%f437, %f366, %f389, %f434;
	fma.rn.ftz.f32 	%f438, %f366, %f391, %f433;
	fma.rn.ftz.f32 	%f439, %f365, %f405, %f438;
	fma.rn.ftz.f32 	%f440, %f365, %f403, %f437;
	fma.rn.ftz.f32 	%f441, %f365, %f404, %f436;
	mul.ftz.f32 	%f442, %f441, %f441;
	fma.rn.ftz.f32 	%f443, %f440, %f440, %f442;
	fma.rn.ftz.f32 	%f444, %f439, %f439, %f443;
	rsqrt.approx.ftz.f32 	%f445, %f444;
	mul.ftz.f32 	%f446, %f440, %f445;
	mul.ftz.f32 	%f447, %f439, %f445;
	mul.ftz.f32 	%f448, %f441, %f445;
	mul.ftz.f32 	%f449, %f432, %f448;
	neg.ftz.f32 	%f450, %f449;
	fma.rn.ftz.f32 	%f451, %f431, %f447, %f450;
	mul.ftz.f32 	%f452, %f430, %f447;
	neg.ftz.f32 	%f453, %f452;
	fma.rn.ftz.f32 	%f454, %f432, %f446, %f453;
	mul.ftz.f32 	%f455, %f431, %f446;
	neg.ftz.f32 	%f456, %f455;
	fma.rn.ftz.f32 	%f457, %f430, %f448, %f456;
	mul.ftz.f32 	%f458, %f454, %f454;
	fma.rn.ftz.f32 	%f459, %f451, %f451, %f458;
	fma.rn.ftz.f32 	%f460, %f457, %f457, %f459;
	rsqrt.approx.ftz.f32 	%f461, %f460;
	mul.ftz.f32 	%f462, %f461, %f451;
	mul.ftz.f32 	%f463, %f461, %f454;
	mul.ftz.f32 	%f464, %f461, %f457;
	ld.u64 	%rd132, [%rd109+544];
	shl.b64 	%rd162, %rd2, 6;
	// inline asm
	mov.b64 {_,%r683}, %rd132;
	// inline asm
	// inline asm
	call (%rd133), _rt_buffer_get_id_64, (%r683, %r582, %r582, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd163, %rd162, %rd133;
	ld.f32 	%f465, [%rd163+512];
	sub.ftz.f32 	%f467, %f416, %f12;
	sub.ftz.f32 	%f468, %f415, %f10;
	sub.ftz.f32 	%f469, %f414, %f11;
	neg.ftz.f32 	%f470, %f15;
	neg.ftz.f32 	%f471, %f13;
	neg.ftz.f32 	%f472, %f14;
	neg.ftz.f32 	%f473, %f18;
	neg.ftz.f32 	%f474, %f16;
	neg.ftz.f32 	%f475, %f17;
	mul.ftz.f32 	%f476, %f469, %f469;
	mul.ftz.f32 	%f477, %f469, %f472;
	fma.rn.ftz.f32 	%f478, %f469, %f472, %f477;
	mul.ftz.f32 	%f479, %f469, %f475;
	fma.rn.ftz.f32 	%f480, %f469, %f475, %f479;
	fma.rn.ftz.f32 	%f481, %f468, %f468, %f476;
	fma.rn.ftz.f32 	%f482, %f468, %f471, %f478;
	fma.rn.ftz.f32 	%f483, %f468, %f471, %f482;
	fma.rn.ftz.f32 	%f484, %f468, %f474, %f480;
	fma.rn.ftz.f32 	%f485, %f468, %f474, %f484;
	fma.rn.ftz.f32 	%f486, %f467, %f467, %f481;
	fma.rn.ftz.f32 	%f487, %f467, %f470, %f483;
	fma.rn.ftz.f32 	%f488, %f467, %f470, %f487;
	fma.rn.ftz.f32 	%f489, %f467, %f473, %f485;
	fma.rn.ftz.f32 	%f490, %f467, %f473, %f489;
	rsqrt.approx.ftz.f32 	%f491, %f486;
	mul.ftz.f32 	%f492, %f491, 0f3F000000;
	sqrt.approx.ftz.f32 	%f87, %f486;
	mul.ftz.f32 	%f88, %f488, %f492;
	mul.ftz.f32 	%f89, %f490, %f492;
	rcp.approx.ftz.f32 	%f493, %f87;
	neg.ftz.f32 	%f494, %f88;
	mul.ftz.f32 	%f495, %f87, %f87;
	div.approx.ftz.f32 	%f496, %f494, %f495;
	neg.ftz.f32 	%f497, %f89;
	div.approx.ftz.f32 	%f498, %f497, %f495;
	mul.ftz.f32 	%f92, %f467, %f493;
	mul.ftz.f32 	%f90, %f468, %f493;
	mul.ftz.f32 	%f91, %f469, %f493;
	mul.ftz.f32 	%f499, %f493, %f14;
	mul.ftz.f32 	%f500, %f493, %f13;
	mul.ftz.f32 	%f501, %f493, %f15;
	neg.ftz.f32 	%f502, %f501;
	fma.rn.ftz.f32 	%f95, %f467, %f496, %f502;
	neg.ftz.f32 	%f503, %f500;
	fma.rn.ftz.f32 	%f93, %f468, %f496, %f503;
	neg.ftz.f32 	%f504, %f499;
	fma.rn.ftz.f32 	%f94, %f469, %f496, %f504;
	mul.ftz.f32 	%f505, %f493, %f17;
	mul.ftz.f32 	%f506, %f493, %f16;
	mul.ftz.f32 	%f507, %f493, %f18;
	neg.ftz.f32 	%f508, %f507;
	fma.rn.ftz.f32 	%f98, %f467, %f498, %f508;
	neg.ftz.f32 	%f509, %f506;
	fma.rn.ftz.f32 	%f96, %f468, %f498, %f509;
	neg.ftz.f32 	%f510, %f505;
	fma.rn.ftz.f32 	%f97, %f469, %f498, %f510;
	mul.ftz.f32 	%f511, %f431, %f91;
	mul.ftz.f32 	%f512, %f431, %f94;
	mul.ftz.f32 	%f513, %f431, %f97;
	fma.rn.ftz.f32 	%f514, %f430, %f90, %f511;
	fma.rn.ftz.f32 	%f515, %f430, %f93, %f512;
	fma.rn.ftz.f32 	%f516, %f430, %f96, %f513;
	fma.rn.ftz.f32 	%f517, %f432, %f92, %f514;
	fma.rn.ftz.f32 	%f518, %f432, %f95, %f515;
	fma.rn.ftz.f32 	%f519, %f432, %f98, %f516;
	setp.lt.ftz.f32	%p91, %f517, 0f00000000;
	neg.ftz.f32 	%f520, %f517;
	neg.ftz.f32 	%f521, %f518;
	neg.ftz.f32 	%f522, %f519;
	selp.f32	%f523, %f520, %f517, %p91;
	selp.f32	%f524, %f521, %f518, %p91;
	selp.f32	%f525, %f522, %f519, %p91;
	mul.ftz.f32 	%f527, %f486, %f524;
	neg.ftz.f32 	%f528, %f527;
	fma.rn.ftz.f32 	%f529, %f488, %f523, %f528;
	mul.ftz.f32 	%f530, %f486, %f486;
	mul.ftz.f32 	%f532, %f486, %f525;
	neg.ftz.f32 	%f533, %f532;
	fma.rn.ftz.f32 	%f534, %f490, %f523, %f533;
	mul.ftz.f32 	%f539, %f431, %f464;
	mul.ftz.f32 	%f540, %f463, %f447;
	neg.ftz.f32 	%f541, %f540;
	fma.rn.ftz.f32 	%f542, %f464, %f448, %f541;
	neg.ftz.f32 	%f543, %f539;
	fma.rn.ftz.f32 	%f544, %f432, %f463, %f543;
	fma.rn.ftz.f32 	%f545, %f447, %f431, %f450;
	mul.ftz.f32 	%f546, %f464, %f446;
	mul.ftz.f32 	%f553, %f462, %f448;
	mul.ftz.f32 	%f560, %f462, %f545;
	fma.rn.ftz.f32 	%f561, %f446, %f544, %f560;
	ld.u32 	%r699, [%rd10];
	ld.u64 	%rd138, [%rd109+560];
	cvt.s64.s32	%rd7, %r699;
	mul.wide.s32 	%rd164, %r699, 672;
	// inline asm
	mov.b64 {_,%r687}, %rd138;
	// inline asm
	// inline asm
	call (%rd139), _rt_buffer_get_id_64, (%r687, %r582, %r582, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd165, %rd139, %rd164;
	ld.f32 	%f761, [%rd165+920];
	ld.f32 	%f760, [%rd165+916];
	ld.f32 	%f759, [%rd165+912];
	ld.f32 	%f114, [%rd165+956];
	ld.u32 	%r54, [%rd165+752];
	ld.f32 	%f115, [%rd165+756];
	ld.v2.f32 	{%f116, %f117}, [%rd165+768];
	ld.v2.f32 	{%f118, %f119}, [%rd165+760];
	ld.u32 	%r692, [%rd165+776];
	mov.u32 	%r822, 0;
	setp.eq.s32	%p92, %r692, 3;
	@%p92 bra 	LBB3_48;
	setp.eq.s32	%p93, %r692, 2;
	@%p93 bra 	LBB3_47;
	bra.uni 	LBB3_45;
LBB3_47:
	mov.u32 	%r821, 2;
	bra.uni 	LBB3_49;
LBB3_48:
	mov.u32 	%r821, 3;
	bra.uni 	LBB3_49;
LBB3_45:
	setp.ne.s32	%p94, %r692, 1;
	mov.u32 	%r821, %r822;
	@%p94 bra 	LBB3_49;
	mov.u32 	%r821, %r582;
LBB3_49:
	div.approx.ftz.f32 	%f466, %f210, %f465;
	div.approx.ftz.f32 	%f526, %f523, %f486;
	div.approx.ftz.f32 	%f531, %f529, %f530;
	div.approx.ftz.f32 	%f535, %f534, %f530;
	neg.ftz.f32 	%f550, %f546;
	neg.ftz.f32 	%f557, %f553;
	fma.rn.ftz.f32 	%f562, %f430, %f542, %f561;
	fma.rn.ftz.f32 	%f375, %f307, %f367, %f374;
	fma.rn.ftz.f32 	%f376, %f307, %f368, %f373;
	// inline asm
	mov.b64 {_,%r703}, %rd86;
	// inline asm
	// inline asm
	call (%rd167), _rt_buffer_get_id_64, (%r703, %r582, %r582, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	ld.u64 	%rd172, [%rd167+560];
	mul.lo.s64 	%rd178, %rd7, 672;
	// inline asm
	mov.b64 {_,%r707}, %rd172;
	// inline asm
	// inline asm
	call (%rd173), _rt_buffer_get_id_64, (%r707, %r582, %r582, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd179, %rd178, %rd173;
	ld.u32 	%r56, [%rd179+1008];
	ld.f32 	%f120, [%rd179+1012];
	ld.v2.f32 	{%f121, %f122}, [%rd179+1024];
	ld.v2.f32 	{%f123, %f124}, [%rd179+1016];
	ld.u32 	%r712, [%rd179+1032];
	setp.eq.s32	%p95, %r712, 3;
	@%p95 bra 	LBB3_54;
	setp.eq.s32	%p96, %r712, 2;
	@%p96 bra 	LBB3_53;
	bra.uni 	LBB3_51;
LBB3_53:
	mov.u32 	%r822, 2;
	bra.uni 	LBB3_55;
LBB3_54:
	mov.u32 	%r822, 3;
	bra.uni 	LBB3_55;
LBB3_51:
	setp.ne.s32	%p97, %r712, 1;
	@%p97 bra 	LBB3_55;
	mov.u32 	%r822, 1;
LBB3_55:
	mul.ftz.f32 	%f536, %f466, %f531;
	mul.ftz.f32 	%f537, %f526, %f526;
	mul.ftz.f32 	%f538, %f466, %f535;
	fma.rn.ftz.f32 	%f551, %f462, %f447, %f550;
	fma.rn.ftz.f32 	%f558, %f463, %f446, %f557;
	div.approx.ftz.f32 	%f563, %f210, %f562;
	fma.rn.ftz.f32 	%f86, %f310, %f372, %f376;
	fma.rn.ftz.f32 	%f85, %f310, %f371, %f375;
	and.b32  	%r716, %r54, 1;
	setp.eq.b32	%p98, %r716, 1;
	@!%p98 bra 	LBB3_66;
	bra.uni 	LBB3_56;
LBB3_56:
	div.approx.ftz.f32 	%f573, %f85, %f116;
	div.approx.ftz.f32 	%f574, %f86, %f117;
	cos.approx.ftz.f32 	%f575, %f115;
	sin.approx.ftz.f32 	%f576, %f115;
	mul.ftz.f32 	%f577, %f574, %f576;
	neg.ftz.f32 	%f578, %f577;
	fma.rn.ftz.f32 	%f579, %f573, %f575, %f578;
	neg.ftz.f32 	%f580, %f119;
	fma.rn.ftz.f32 	%f581, %f573, %f576, %f580;
	fma.rn.ftz.f32 	%f569, %f574, %f575, %f581;
	sub.ftz.f32 	%f568, %f579, %f118;
	// inline asm
	mov.b64 {_,%r717}, %rd86;
	// inline asm
	mov.u32 	%r742, 1;
	// inline asm
	call (%rd181), _rt_buffer_get_id_64, (%r717, %r742, %r742, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	ld.u64 	%rd186, [%rd181+560];
	add.s64 	%rd217, %rd178, 176;
	// inline asm
	mov.b64 {_,%r721}, %rd186;
	// inline asm
	// inline asm
	call (%rd187), _rt_buffer_get_id_64, (%r721, %r742, %r742, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd218, %rd187, %rd217;
	ld.f32 	%f125, [%rd218+544];
	ld.f32 	%f126, [%rd218+548];
	ld.f32 	%f127, [%rd218+552];
	ld.u32 	%r744, [%rd218+512];
	ld.u64 	%rd219, [%rd218+520];
	ld.u64 	%rd192, [%rd181+520];
	mul.wide.s32 	%rd220, %r744, 32;
	// inline asm
	mov.b64 {_,%r725}, %rd192;
	// inline asm
	// inline asm
	call (%rd193), _rt_buffer_get_id_64, (%r725, %r742, %r742, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd221, %rd220, %rd193;
	ld.v2.u32 	{%r58, %r59}, [%rd221+528];
	st.u64 	[%SP+0], %rd219;
	cvt.u32.u64	%r729, %rd219;
	mov.u32 	%r730, 2;
	mov.u32 	%r731, 0;
	mov.f32 	%f571, 0f00000000;
	// inline asm
	call (%f564,%f565,%f566,%f567), _rt_texture_get_level_id, (%r729, %r730, %f568, %f569, %f571, %r731, %f571);
	// inline asm
	// inline asm
	mov.b64 {_,%r732}, %rd86;
	// inline asm
	// inline asm
	call (%rd199), _rt_buffer_get_id_64, (%r732, %r742, %r742, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	ld.u64 	%rd204, [%rd199+520];
	// inline asm
	mov.b64 {_,%r736}, %rd204;
	// inline asm
	// inline asm
	call (%rd205), _rt_buffer_get_id_64, (%r736, %r742, %r742, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd222, %rd205, %rd220;
	ld.u32 	%r745, [%rd222+512];
	setp.lt.s32	%p99, %r745, 4;
	ld.u64 	%rd210, [%rd199+560];
	// inline asm
	mov.b64 {_,%r740}, %rd210;
	// inline asm
	// inline asm
	call (%rd211), _rt_buffer_get_id_64, (%r740, %r742, %r742, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd223, %rd217, %rd211;
	ld.u32 	%r746, [%rd223+528];
	setp.ne.s32	%p100, %r746, 1;
	mov.f32 	%f757, %f571;
	@%p100 bra 	LBB3_58;
	cvt.rn.f32.s32	%f582, %r58;
	mov.f32 	%f583, 0f3F000000;
	div.approx.ftz.f32 	%f584, %f583, %f582;
	sub.ftz.f32 	%f586, %f210, %f584;
	max.ftz.f32 	%f587, %f568, %f584;
	min.ftz.f32 	%f588, %f587, %f586;
	sub.ftz.f32 	%f589, %f568, %f588;
	abs.ftz.f32 	%f590, %f589;
	mul.ftz.f32 	%f591, %f582, %f590;
	mov.f32 	%f592, 0f00000000;
	max.ftz.f32 	%f593, %f591, %f592;
	min.ftz.f32 	%f757, %f593, %f210;
LBB3_58:
	selp.f32	%f132, %f564, %f566, %p99;
	selp.f32	%f131, %f564, %f565, %p99;
	// inline asm
	mov.b64 {_,%r747}, %rd86;
	// inline asm
	// inline asm
	call (%rd225), _rt_buffer_get_id_64, (%r747, %r742, %r742, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	ld.u64 	%rd230, [%rd225+560];
	// inline asm
	mov.b64 {_,%r751}, %rd230;
	// inline asm
	// inline asm
	call (%rd231), _rt_buffer_get_id_64, (%r751, %r742, %r742, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd237, %rd178, %rd231;
	ld.u32 	%r755, [%rd237+708];
	setp.ne.s32	%p101, %r755, 1;
	mov.f32 	%f758, %f571;
	@%p101 bra 	LBB3_60;
	cvt.rn.f32.s32	%f595, %r59;
	mov.f32 	%f596, 0f3F000000;
	div.approx.ftz.f32 	%f597, %f596, %f595;
	sub.ftz.f32 	%f599, %f210, %f597;
	max.ftz.f32 	%f600, %f569, %f597;
	min.ftz.f32 	%f601, %f600, %f599;
	sub.ftz.f32 	%f602, %f569, %f601;
	abs.ftz.f32 	%f603, %f602;
	mul.ftz.f32 	%f604, %f595, %f603;
	mov.f32 	%f605, 0f00000000;
	max.ftz.f32 	%f606, %f604, %f605;
	min.ftz.f32 	%f758, %f606, %f210;
LBB3_60:
	sub.ftz.f32 	%f607, %f125, %f564;
	fma.rn.ftz.f32 	%f608, %f757, %f607, %f564;
	sub.ftz.f32 	%f609, %f126, %f131;
	fma.rn.ftz.f32 	%f610, %f757, %f609, %f131;
	sub.ftz.f32 	%f611, %f127, %f132;
	fma.rn.ftz.f32 	%f612, %f757, %f611, %f132;
	sub.ftz.f32 	%f613, %f125, %f608;
	fma.rn.ftz.f32 	%f614, %f758, %f613, %f608;
	sub.ftz.f32 	%f615, %f126, %f610;
	fma.rn.ftz.f32 	%f616, %f758, %f615, %f610;
	sub.ftz.f32 	%f617, %f127, %f612;
	fma.rn.ftz.f32 	%f618, %f758, %f617, %f612;
	mul.ftz.f32 	%f619, %f618, 0f3D93D07D;
	mov.f32 	%f620, 0f3F371437;
	fma.rn.ftz.f32 	%f621, %f620, %f616, %f619;
	mov.f32 	%f622, 0f3E59C6ED;
	fma.rn.ftz.f32 	%f623, %f622, %f614, %f621;
	sub.ftz.f32 	%f624, %f614, %f623;
	sub.ftz.f32 	%f625, %f616, %f623;
	sub.ftz.f32 	%f626, %f618, %f623;
	add.ftz.f32 	%f627, %f623, %f626;
	add.ftz.f32 	%f628, %f623, %f625;
	add.ftz.f32 	%f629, %f623, %f624;
	max.ftz.f32 	%f138, %f571, %f629;
	max.ftz.f32 	%f139, %f571, %f628;
	max.ftz.f32 	%f140, %f571, %f627;
	setp.eq.s32	%p102, %r821, 3;
	@%p102 bra 	LBB3_65;
	setp.eq.s32	%p103, %r821, 2;
	@%p103 bra 	LBB3_64;
	bra.uni 	LBB3_62;
LBB3_64:
	add.ftz.f32 	%f759, %f759, %f138;
	add.ftz.f32 	%f760, %f760, %f139;
	add.ftz.f32 	%f761, %f761, %f140;
	bra.uni 	LBB3_66;
LBB3_65:
	mul.ftz.f32 	%f759, %f759, %f138;
	mul.ftz.f32 	%f760, %f760, %f139;
	mul.ftz.f32 	%f761, %f761, %f140;
	bra.uni 	LBB3_66;
LBB3_62:
	setp.ne.s32	%p104, %r821, 1;
	@%p104 bra 	LBB3_87;
	mul.ftz.f32 	%f759, %f759, %f138;
	mul.ftz.f32 	%f760, %f760, %f139;
	mul.ftz.f32 	%f761, %f761, %f140;
	bra.uni 	LBB3_66;
LBB3_87:
	mov.f32 	%f759, %f138;
	mov.f32 	%f760, %f139;
	mov.f32 	%f761, %f140;
LBB3_66:
	div.approx.ftz.f32 	%f99, %f466, %f526;
	div.approx.ftz.f32 	%f100, %f536, %f537;
	div.approx.ftz.f32 	%f101, %f538, %f537;
	mul.ftz.f32 	%f104, %f542, %f563;
	mul.ftz.f32 	%f107, %f551, %f563;
	mul.ftz.f32 	%f110, %f558, %f563;
	sub.ftz.f32 	%f764, %f210, %f114;
	and.b32  	%r756, %r56, 1;
	setp.eq.b32	%p105, %r756, 1;
	@!%p105 bra 	LBB3_77;
	bra.uni 	LBB3_67;
LBB3_67:
	div.approx.ftz.f32 	%f641, %f85, %f121;
	div.approx.ftz.f32 	%f642, %f86, %f122;
	cos.approx.ftz.f32 	%f643, %f120;
	sin.approx.ftz.f32 	%f644, %f120;
	mul.ftz.f32 	%f645, %f642, %f644;
	neg.ftz.f32 	%f646, %f645;
	fma.rn.ftz.f32 	%f647, %f641, %f643, %f646;
	neg.ftz.f32 	%f648, %f124;
	fma.rn.ftz.f32 	%f649, %f641, %f644, %f648;
	fma.rn.ftz.f32 	%f637, %f642, %f643, %f649;
	sub.ftz.f32 	%f636, %f647, %f123;
	// inline asm
	mov.b64 {_,%r757}, %rd86;
	// inline asm
	mov.u32 	%r782, 1;
	// inline asm
	call (%rd239), _rt_buffer_get_id_64, (%r757, %r782, %r782, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	ld.u64 	%rd244, [%rd239+560];
	add.s64 	%rd275, %rd178, 448;
	// inline asm
	mov.b64 {_,%r761}, %rd244;
	// inline asm
	// inline asm
	call (%rd245), _rt_buffer_get_id_64, (%r761, %r782, %r782, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd276, %rd245, %rd275;
	ld.u32 	%r784, [%rd276+512];
	ld.u64 	%rd277, [%rd276+520];
	ld.u64 	%rd250, [%rd239+520];
	mul.wide.s32 	%rd278, %r784, 32;
	// inline asm
	mov.b64 {_,%r765}, %rd250;
	// inline asm
	// inline asm
	call (%rd251), _rt_buffer_get_id_64, (%r765, %r782, %r782, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd279, %rd251, %rd278;
	ld.u32 	%r60, [%rd279+536];
	ld.v4.f32 	{%f154, %f155, %f156, %f157}, [%rd276+544];
	ld.v2.u32 	{%r61, %r62}, [%rd279+528];
	st.u64 	[%SP+8], %rd277;
	cvt.u32.u64	%r769, %rd277;
	mov.u32 	%r770, 2;
	mov.u32 	%r771, 0;
	mov.f32 	%f763, 0f00000000;
	// inline asm
	call (%f632,%f633,%f634,%f635), _rt_texture_get_level_id, (%r769, %r770, %f636, %f637, %f763, %r771, %f763);
	// inline asm
	// inline asm
	mov.b64 {_,%r772}, %rd86;
	// inline asm
	// inline asm
	call (%rd257), _rt_buffer_get_id_64, (%r772, %r782, %r782, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	ld.u64 	%rd262, [%rd257+520];
	// inline asm
	mov.b64 {_,%r776}, %rd262;
	// inline asm
	// inline asm
	call (%rd263), _rt_buffer_get_id_64, (%r776, %r782, %r782, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd280, %rd263, %rd278;
	ld.u32 	%r785, [%rd280+512];
	setp.lt.s32	%p106, %r785, 4;
	ld.u64 	%rd268, [%rd257+560];
	// inline asm
	mov.b64 {_,%r780}, %rd268;
	// inline asm
	// inline asm
	call (%rd269), _rt_buffer_get_id_64, (%r780, %r782, %r782, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd281, %rd275, %rd269;
	ld.u32 	%r786, [%rd281+528];
	setp.ne.s32	%p107, %r786, 1;
	mov.f32 	%f762, %f763;
	@%p107 bra 	LBB3_69;
	cvt.rn.f32.s32	%f650, %r61;
	mov.f32 	%f651, 0f3F000000;
	div.approx.ftz.f32 	%f652, %f651, %f650;
	sub.ftz.f32 	%f654, %f210, %f652;
	max.ftz.f32 	%f655, %f636, %f652;
	min.ftz.f32 	%f656, %f655, %f654;
	sub.ftz.f32 	%f657, %f636, %f656;
	abs.ftz.f32 	%f658, %f657;
	mul.ftz.f32 	%f659, %f650, %f658;
	mov.f32 	%f660, 0f00000000;
	max.ftz.f32 	%f661, %f659, %f660;
	min.ftz.f32 	%f762, %f661, %f210;
LBB3_69:
	selp.f32	%f163, 0f3F800000, %f635, %p106;
	selp.f32	%f161, %f632, %f633, %p106;
	// inline asm
	mov.b64 {_,%r787}, %rd86;
	// inline asm
	// inline asm
	call (%rd283), _rt_buffer_get_id_64, (%r787, %r782, %r782, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	ld.u64 	%rd288, [%rd283+560];
	// inline asm
	mov.b64 {_,%r791}, %rd288;
	// inline asm
	// inline asm
	call (%rd289), _rt_buffer_get_id_64, (%r791, %r782, %r782, %rd96, %rd96, %rd96, %rd96);
	// inline asm
	add.s64 	%rd295, %rd178, %rd289;
	ld.u32 	%r795, [%rd295+980];
	setp.ne.s32	%p108, %r795, 1;
	@%p108 bra 	LBB3_71;
	cvt.rn.f32.s32	%f663, %r62;
	mov.f32 	%f664, 0f3F000000;
	div.approx.ftz.f32 	%f665, %f664, %f663;
	sub.ftz.f32 	%f667, %f210, %f665;
	max.ftz.f32 	%f668, %f637, %f665;
	min.ftz.f32 	%f669, %f668, %f667;
	sub.ftz.f32 	%f670, %f637, %f669;
	abs.ftz.f32 	%f671, %f670;
	mul.ftz.f32 	%f672, %f663, %f671;
	mov.f32 	%f673, 0f00000000;
	max.ftz.f32 	%f674, %f672, %f673;
	min.ftz.f32 	%f763, %f674, %f210;
LBB3_71:
	sub.ftz.f32 	%f675, %f154, %f632;
	fma.rn.ftz.f32 	%f676, %f762, %f675, %f632;
	sub.ftz.f32 	%f677, %f155, %f161;
	fma.rn.ftz.f32 	%f678, %f762, %f677, %f161;
	sub.ftz.f32 	%f679, %f157, %f163;
	fma.rn.ftz.f32 	%f680, %f762, %f679, %f163;
	sub.ftz.f32 	%f681, %f154, %f676;
	fma.rn.ftz.f32 	%f682, %f763, %f681, %f676;
	sub.ftz.f32 	%f683, %f155, %f678;
	fma.rn.ftz.f32 	%f684, %f763, %f683, %f678;
	sub.ftz.f32 	%f685, %f157, %f680;
	fma.rn.ftz.f32 	%f686, %f763, %f685, %f680;
	setp.gt.s32	%p109, %r60, 0;
	setp.gt.s32	%p110, %r60, 1;
	selp.f32	%f687, %f684, 0f00000000, %p110;
	selp.f32	%f688, %f682, 0f00000000, %p109;
	setp.gt.s32	%p111, %r60, 3;
	selp.f32	%f689, %f686, 0f00000000, %p111;
	setp.eq.s32	%p112, %r60, 2;
	selp.f32	%f690, %f689, %f688, %p111;
	selp.f32	%f168, %f687, %f690, %p112;
	setp.eq.s32	%p113, %r822, 3;
	@%p113 bra 	LBB3_76;
	setp.eq.s32	%p114, %r822, 2;
	@%p114 bra 	LBB3_75;
	bra.uni 	LBB3_73;
LBB3_75:
	add.ftz.f32 	%f764, %f764, %f168;
	bra.uni 	LBB3_77;
LBB3_76:
	mul.ftz.f32 	%f764, %f764, %f168;
	bra.uni 	LBB3_77;
LBB3_73:
	setp.ne.s32	%p115, %r822, 1;
	@%p115 bra 	LBB3_88;
	mul.ftz.f32 	%f764, %f764, %f168;
	bra.uni 	LBB3_77;
LBB3_88:
	mov.f32 	%f764, %f168;
LBB3_77:
	neg.ftz.f32 	%f691, %f92;
	neg.ftz.f32 	%f692, %f90;
	neg.ftz.f32 	%f693, %f91;
	mul.ftz.f32 	%f694, %f693, %f107;
	fma.rn.ftz.f32 	%f695, %f692, %f104, %f694;
	fma.rn.ftz.f32 	%f696, %f691, %f110, %f695;
	setp.ge.ftz.f32	%p116, %f696, 0f00000000;
	selp.f32	%f697, %f761, 0f00000000, %p116;
	selp.f32	%f698, %f760, 0f00000000, %p116;
	selp.f32	%f699, %f759, 0f00000000, %p116;
	mul.ftz.f32 	%f700, %f699, %f764;
	mov.b32 	 %r796, %f700;
	cvt.u64.u32	%rd296, %r796;
	mul.ftz.f32 	%f701, %f698, %f764;
	mov.b32 	 %r797, %f701;
	cvt.u64.u32	%rd297, %r797;
	shl.b64 	%rd298, %rd297, 32;
	or.b64  	%rd299, %rd296, %rd298;
	mul.ftz.f32 	%f702, %f697, %f764;
	mul.ftz.f32 	%f703, %f4, %f91;
	mul.ftz.f32 	%f704, %f5, %f91;
	mul.ftz.f32 	%f705, %f6, %f91;
	mul.ftz.f32 	%f706, %f4, %f94;
	mul.ftz.f32 	%f707, %f5, %f94;
	mul.ftz.f32 	%f708, %f6, %f94;
	mul.ftz.f32 	%f709, %f4, %f97;
	mul.ftz.f32 	%f710, %f5, %f97;
	mul.ftz.f32 	%f711, %f6, %f97;
	fma.rn.ftz.f32 	%f712, %f3, %f90, %f705;
	fma.rn.ftz.f32 	%f713, %f2, %f90, %f704;
	fma.rn.ftz.f32 	%f714, %f1, %f90, %f703;
	fma.rn.ftz.f32 	%f715, %f3, %f93, %f708;
	fma.rn.ftz.f32 	%f716, %f2, %f93, %f707;
	fma.rn.ftz.f32 	%f717, %f1, %f93, %f706;
	fma.rn.ftz.f32 	%f718, %f3, %f96, %f711;
	fma.rn.ftz.f32 	%f719, %f2, %f96, %f710;
	fma.rn.ftz.f32 	%f720, %f1, %f96, %f709;
	fma.rn.ftz.f32 	%f721, %f7, %f92, %f714;
	mov.b32 	 %r798, %f721;
	cvt.u64.u32	%rd300, %r798;
	fma.rn.ftz.f32 	%f722, %f8, %f92, %f713;
	mov.b32 	 %r799, %f722;
	cvt.u64.u32	%rd301, %r799;
	shl.b64 	%rd302, %rd301, 32;
	or.b64  	%rd303, %rd300, %rd302;
	fma.rn.ftz.f32 	%f723, %f9, %f92, %f712;
	fma.rn.ftz.f32 	%f724, %f7, %f95, %f717;
	mov.b32 	 %r800, %f724;
	cvt.u64.u32	%rd304, %r800;
	fma.rn.ftz.f32 	%f725, %f8, %f95, %f716;
	mov.b32 	 %r801, %f725;
	cvt.u64.u32	%rd305, %r801;
	shl.b64 	%rd306, %rd305, 32;
	or.b64  	%rd307, %rd304, %rd306;
	fma.rn.ftz.f32 	%f726, %f9, %f95, %f715;
	fma.rn.ftz.f32 	%f727, %f7, %f98, %f720;
	mov.b32 	 %r802, %f727;
	cvt.u64.u32	%rd308, %r802;
	fma.rn.ftz.f32 	%f728, %f8, %f98, %f719;
	mov.b32 	 %r803, %f728;
	cvt.u64.u32	%rd309, %r803;
	shl.b64 	%rd310, %rd309, 32;
	or.b64  	%rd311, %rd308, %rd310;
	fma.rn.ftz.f32 	%f729, %f9, %f98, %f718;
	st.v2.f32 	[%rd8+96], {%f43, %f44};
	st.v2.u32 	[%rd8+112], {%r50, %r51};
	st.v2.f32 	[%rd8+128], {%f740, %f741};
	st.u32 	[%rd8+88], %r47;
	st.f32 	[%rd8+76], %f99;
	st.f32 	[%rd8+64], %f87;
	st.f32 	[%rd8+80], %f100;
	st.f32 	[%rd8+68], %f88;
	st.f32 	[%rd8+84], %f101;
	st.f32 	[%rd8+72], %f89;
	st.u32 	[%rd8+104], %r809;
	st.u32 	[%rd8+108], %r49;
	st.f32 	[%rd8+56], %f729;
	st.f32 	[%rd8+40], %f726;
	st.f32 	[%rd8+24], %f723;
	st.u32 	[%rd8+120], %r810;
	st.u64 	[%rd8+48], %rd311;
	st.u64 	[%rd8+32], %rd307;
	st.u64 	[%rd8+16], %rd303;
	st.f32 	[%rd8+8], %f702;
	st.u32 	[%rd8+124], %r811;
	st.u64 	[%rd8], %rd299;
	ret;
}

	// .globl	stlr_pdf
.visible .func stlr_pdf(
	.param .b64 stlr_pdf_param_0,
	.param .b64 stlr_pdf_param_1,
	.param .b64 stlr_pdf_param_2,
	.param .b64 stlr_pdf_param_3,
	.param .b64 stlr_pdf_param_4
)
{
	.reg .f32 	%f<3>;
	.reg .s32 	%r<16>;
	.reg .s64 	%rd<25>;

	ld.param.u64 	%rd19, [stlr_pdf_param_0];
	ld.param.u64 	%rd20, [stlr_pdf_param_2];
	ld.u32 	%r13, [%rd20];
	ld.param.u64 	%rd1, [stlr_pdf_param_1];
	// inline asm
	mov.b64 {_,%r2}, %rd1;
	// inline asm
	mov.u32 	%r11, 1;
	mov.u64 	%rd17, 0;
	// inline asm
	call (%rd2), _rt_buffer_get_id_64, (%r2, %r11, %r11, %rd17, %rd17, %rd17, %rd17);
	// inline asm
	ld.u64 	%rd7, [%rd2+560];
	mul.wide.s32 	%rd21, %r13, 672;
	// inline asm
	mov.b64 {_,%r6}, %rd7;
	// inline asm
	// inline asm
	call (%rd8), _rt_buffer_get_id_64, (%r6, %r11, %r11, %rd17, %rd17, %rd17, %rd17);
	// inline asm
	add.s64 	%rd22, %rd21, %rd8;
	ld.u32 	%r14, [%rd22+656];
	ld.u64 	%rd13, [%rd2+544];
	mul.wide.s32 	%rd23, %r14, 64;
	// inline asm
	mov.b64 {_,%r10}, %rd13;
	// inline asm
	// inline asm
	call (%rd14), _rt_buffer_get_id_64, (%r10, %r11, %r11, %rd17, %rd17, %rd17, %rd17);
	// inline asm
	add.s64 	%rd24, %rd23, %rd14;
	ld.f32 	%f1, [%rd24+512];
	rcp.approx.ftz.f32 	%f2, %f1;
	mov.u32 	%r15, 0;
	st.u32 	[%rd19+8], %r15;
	st.u32 	[%rd19+4], %r15;
	st.f32 	[%rd19], %f2;
	ret;
}

	// .globl	stlr_sample
.visible .func stlr_sample(
	.param .b64 stlr_sample_param_0,
	.param .b64 stlr_sample_param_1,
	.param .b64 stlr_sample_param_2,
	.param .b64 stlr_sample_param_3,
	.param .b64 stlr_sample_param_4,
	.param .b64 stlr_sample_param_5
)
{
	.local .align 8 .b8 	__local_depot5[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<203>;
	.reg .f32 	%f<713>;
	.reg .s32 	%r<1426>;
	.reg .s64 	%rd<377>;

	mov.u64 	%rd376, __local_depot5;
	cvta.local.u64 	%SP, %rd376;
	ld.param.u64 	%rd11, [stlr_sample_param_5];
	ld.param.u64 	%rd10, [stlr_sample_param_2];
	ld.param.u64 	%rd9, [stlr_sample_param_1];
	ld.param.u64 	%rd8, [stlr_sample_param_0];
	ld.u32 	%r103, [%rd10];
	// inline asm
	mov.b64 {_,%r83}, %rd9;
	// inline asm
	mov.u32 	%r602, 1;
	mov.u64 	%rd337, 0;
	// inline asm
	call (%rd13), _rt_buffer_get_id_64, (%r83, %r602, %r602, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	ld.u64 	%rd18, [%rd13+560];
	mul.wide.s32 	%rd42, %r103, 672;
	// inline asm
	mov.b64 {_,%r87}, %rd18;
	// inline asm
	// inline asm
	call (%rd19), _rt_buffer_get_id_64, (%r87, %r602, %r602, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd43, %rd42, %rd19;
	ld.u32 	%r104, [%rd43+656];
	ld.u64 	%rd24, [%rd13+536];
	mul.wide.s32 	%rd44, %r104, 8;
	// inline asm
	mov.b64 {_,%r91}, %rd24;
	// inline asm
	// inline asm
	call (%rd25), _rt_buffer_get_id_64, (%r91, %r602, %r602, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd45, %rd44, %rd25;
	ld.u64 	%rd3, [%rd45+512];
	ld.u32 	%r105, [%rd43+668];
	ld.u64 	%rd30, [%rd13+528];
	mul.wide.s32 	%rd46, %r105, 16;
	// inline asm
	mov.b64 {_,%r95}, %rd30;
	// inline asm
	// inline asm
	call (%rd31), _rt_buffer_get_id_64, (%r95, %r602, %r602, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd47, %rd46, %rd31;
	ld.u64 	%rd36, [%rd47+512];
	// inline asm
	mov.b64 {_,%r99}, %rd36;
	// inline asm
	// inline asm
	call (%rd37), _rt_buffer_get_id_64, (%r99, %r602, %r602, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	ld.u32 	%r106, [%rd37+508];
	mul.wide.s32 	%rd48, %r106, 1431655766;
	shr.u64 	%rd49, %rd48, 63;
	cvt.u32.u64	%r107, %rd49;
	shr.u64 	%rd50, %rd48, 32;
	cvt.u32.u64	%r108, %rd50;
	add.s32 	%r1, %r108, %r107;
	ld.u32 	%r109, [%rd11];
	setp.eq.s32	%p1, %r109, 1;
	@%p1 bra 	LBB5_11;
	bra.uni 	LBB5_1;
LBB5_11:
	ld.u32 	%r1403, [%rd11+36];
	cvt.rn.f32.s32	%f234, %r1403;
	rcp.approx.ftz.f32 	%f235, %f234;
	mul.ftz.f32 	%f13, %f235, 0f3F000000;
	ld.u32 	%r1402, [%rd11+32];
	setp.gt.s32	%p8, %r1403, 1;
	selp.u32	%r255, 1, 0, %p8;
	setp.gt.s32	%p9, %r1403, 2;
	selp.b32	%r256, 2, 1, %p8;
	selp.b32	%r257, %r256, %r255, %p9;
	setp.gt.s32	%p10, %r1403, 4;
	selp.u32	%r258, 1, 0, %p10;
	setp.gt.s32	%p11, %r1403, 8;
	selp.u32	%r259, 1, 0, %p11;
	setp.gt.s32	%p12, %r1403, 16;
	selp.u32	%r260, 1, 0, %p12;
	setp.gt.s32	%p13, %r1403, 32;
	selp.u32	%r261, 1, 0, %p13;
	setp.gt.s32	%p14, %r1403, 64;
	selp.u32	%r262, 1, 0, %p14;
	setp.gt.s32	%p15, %r1403, 128;
	selp.u32	%r263, 1, 0, %p15;
	setp.gt.s32	%p16, %r1403, 256;
	selp.u32	%r264, 1, 0, %p16;
	setp.gt.s32	%p17, %r1403, 512;
	selp.u32	%r265, 1, 0, %p17;
	setp.gt.s32	%p18, %r1403, 1024;
	selp.u32	%r266, 1, 0, %p18;
	setp.gt.s32	%p19, %r1403, 2048;
	selp.u32	%r267, 1, 0, %p19;
	setp.gt.s32	%p20, %r1403, 4096;
	selp.u32	%r268, 1, 0, %p20;
	setp.gt.s32	%p21, %r1403, 8192;
	selp.u32	%r269, 1, 0, %p21;
	setp.gt.s32	%p22, %r1403, 16384;
	selp.u32	%r270, 1, 0, %p22;
	setp.gt.s32	%p23, %r1403, 32768;
	selp.u32	%r271, 1, 0, %p23;
	setp.gt.s32	%p24, %r1403, 65536;
	selp.u32	%r272, 1, 0, %p24;
	setp.gt.s32	%p25, %r1403, 131072;
	selp.u32	%r273, 1, 0, %p25;
	setp.gt.s32	%p26, %r1403, 262144;
	selp.u32	%r274, 1, 0, %p26;
	setp.gt.s32	%p27, %r1403, 524288;
	selp.u32	%r275, 1, 0, %p27;
	setp.gt.s32	%p28, %r1403, 1048576;
	selp.u32	%r276, 1, 0, %p28;
	setp.gt.s32	%p29, %r1403, 2097152;
	selp.u32	%r277, 1, 0, %p29;
	setp.gt.s32	%p30, %r1403, 4194304;
	selp.u32	%r278, 1, 0, %p30;
	setp.gt.s32	%p31, %r1403, 8388608;
	selp.u32	%r279, 1, 0, %p31;
	setp.gt.s32	%p32, %r1403, 16777216;
	selp.u32	%r280, 1, 0, %p32;
	setp.gt.s32	%p33, %r1403, 33554432;
	selp.u32	%r281, 1, 0, %p33;
	setp.gt.s32	%p34, %r1403, 67108864;
	selp.u32	%r282, 1, 0, %p34;
	setp.gt.s32	%p35, %r1403, 134217728;
	selp.u32	%r283, 1, 0, %p35;
	setp.gt.s32	%p36, %r1403, 268435456;
	selp.u32	%r284, 1, 0, %p36;
	setp.gt.s32	%p37, %r1403, 536870912;
	selp.u32	%r285, 1, 0, %p37;
	setp.gt.s32	%p38, %r1403, 1073741824;
	selp.u32	%r286, 1, 0, %p38;
	add.s32 	%r287, %r259, %r258;
	add.s32 	%r288, %r287, %r260;
	add.s32 	%r289, %r288, %r261;
	add.s32 	%r290, %r289, %r262;
	add.s32 	%r291, %r290, %r263;
	add.s32 	%r292, %r291, %r264;
	add.s32 	%r293, %r292, %r265;
	add.s32 	%r294, %r293, %r266;
	add.s32 	%r295, %r294, %r267;
	add.s32 	%r296, %r295, %r268;
	add.s32 	%r297, %r296, %r269;
	add.s32 	%r298, %r297, %r270;
	add.s32 	%r299, %r298, %r271;
	add.s32 	%r300, %r299, %r272;
	add.s32 	%r301, %r300, %r273;
	add.s32 	%r302, %r301, %r274;
	add.s32 	%r303, %r302, %r275;
	add.s32 	%r304, %r303, %r276;
	add.s32 	%r305, %r304, %r277;
	add.s32 	%r306, %r305, %r278;
	add.s32 	%r307, %r306, %r279;
	add.s32 	%r308, %r307, %r280;
	add.s32 	%r309, %r308, %r281;
	add.s32 	%r310, %r309, %r282;
	add.s32 	%r311, %r310, %r283;
	add.s32 	%r312, %r311, %r284;
	add.s32 	%r313, %r312, %r285;
	add.s32 	%r314, %r313, %r286;
	add.s32 	%r315, %r314, %r257;
	mov.u32 	%r316, 0;
	max.s32 	%r1399, %r315, %r316;
	mov.f32 	%f656, 0f00000000;
	setp.lt.s32	%p39, %r1399, 1;
	@%p39 bra 	LBB5_122;
	mov.f32 	%f654, 0f3F800000;
	mov.u32 	%r1400, %r1402;
LBB5_13:
	and.b32  	%r317, %r1400, 1;
	shr.u32 	%r1400, %r1400, 1;
	mul.ftz.f32 	%f654, %f654, 0f3F000000;
	cvt.rn.f32.s32	%f236, %r317;
	fma.rn.ftz.f32 	%f656, %f654, %f236, %f656;
	add.s32 	%r1399, %r1399, -1;
	setp.ne.s32	%p40, %r1399, 0;
	@%p40 bra 	LBB5_13;
	bra.uni 	LBB5_14;
LBB5_1:
	ld.u32 	%r2, [%rd11+16];
	setp.lt.s32	%p2, %r2, 8;
	ld.u32 	%r1398, [%rd11+20];
	@%p2 bra 	LBB5_3;
	bra.uni 	LBB5_2;
LBB5_3:
	mov.f32 	%f653, 0f00000000;
	setp.lt.s32	%p3, %r1398, 1;
	@%p3 bra 	LBB5_120;
	add.s64 	%rd1, %rd13, 512;
	and.b32  	%r235, %r2, 63;
	ld.u64 	%rd51, [%rd1];
	mul.wide.u32 	%rd57, %r235, 4;
	// inline asm
	mov.b64 {_,%r231}, %rd51;
	// inline asm
	// inline asm
	call (%rd52), _rt_buffer_get_id_64, (%r231, %r602, %r602, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd58, %rd57, %rd52;
	ld.u32 	%r4, [%rd58+512];
	cvt.rn.f32.s32	%f226, %r4;
	rcp.approx.ftz.f32 	%f2, %f226;
	mov.f32 	%f650, %f2;
LBB5_5:
	setp.ne.s32	%p4, %r4, 0;
	selp.b32	%r236, %r4, %r1398, %p4;
	rem.s32 	%r237, %r1398, %r236;
	cvt.rn.f32.s32	%f227, %r237;
	fma.rn.ftz.f32 	%f653, %f650, %f227, %f653;
	mul.ftz.f32 	%f650, %f2, %f650;
	selp.b32	%r238, %r4, -2147483648, %p4;
	div.s32 	%r1398, %r1398, %r238;
	setp.gt.s32	%p5, %r1398, 0;
	@%p5 bra 	LBB5_5;
	bra.uni 	LBB5_6;
LBB5_2:
	shl.b32 	%r110, %r1398, 4;
	add.s32 	%r111, %r110, -1556008596;
	add.s32 	%r112, %r1398, -1640531527;
	xor.b32  	%r113, %r111, %r112;
	shr.u32 	%r114, %r1398, 5;
	add.s32 	%r115, %r114, -939442524;
	xor.b32  	%r116, %r113, %r115;
	add.s32 	%r117, %r116, %r2;
	shl.b32 	%r118, %r117, 4;
	add.s32 	%r119, %r118, -1383041155;
	add.s32 	%r120, %r117, -1640531527;
	xor.b32  	%r121, %r119, %r120;
	shr.u32 	%r122, %r117, 5;
	add.s32 	%r123, %r122, 2123724318;
	xor.b32  	%r124, %r121, %r123;
	add.s32 	%r125, %r124, %r1398;
	shl.b32 	%r126, %r125, 4;
	add.s32 	%r127, %r126, -1556008596;
	add.s32 	%r128, %r125, 1013904242;
	xor.b32  	%r129, %r127, %r128;
	shr.u32 	%r130, %r125, 5;
	add.s32 	%r131, %r130, -939442524;
	xor.b32  	%r132, %r129, %r131;
	add.s32 	%r133, %r132, %r117;
	shl.b32 	%r134, %r133, 4;
	add.s32 	%r135, %r134, -1383041155;
	add.s32 	%r136, %r133, 1013904242;
	xor.b32  	%r137, %r135, %r136;
	shr.u32 	%r138, %r133, 5;
	add.s32 	%r139, %r138, 2123724318;
	xor.b32  	%r140, %r137, %r139;
	add.s32 	%r141, %r140, %r125;
	shl.b32 	%r142, %r141, 4;
	add.s32 	%r143, %r142, -1556008596;
	add.s32 	%r144, %r141, -626627285;
	xor.b32  	%r145, %r143, %r144;
	shr.u32 	%r146, %r141, 5;
	add.s32 	%r147, %r146, -939442524;
	xor.b32  	%r148, %r145, %r147;
	add.s32 	%r149, %r148, %r133;
	shl.b32 	%r150, %r149, 4;
	add.s32 	%r151, %r150, -1383041155;
	add.s32 	%r152, %r149, -626627285;
	xor.b32  	%r153, %r151, %r152;
	shr.u32 	%r154, %r149, 5;
	add.s32 	%r155, %r154, 2123724318;
	xor.b32  	%r156, %r153, %r155;
	add.s32 	%r157, %r156, %r141;
	shl.b32 	%r158, %r157, 4;
	add.s32 	%r159, %r158, -1556008596;
	add.s32 	%r160, %r157, 2027808484;
	xor.b32  	%r161, %r159, %r160;
	shr.u32 	%r162, %r157, 5;
	add.s32 	%r163, %r162, -939442524;
	xor.b32  	%r164, %r161, %r163;
	add.s32 	%r165, %r164, %r149;
	shl.b32 	%r166, %r165, 4;
	add.s32 	%r167, %r166, -1383041155;
	add.s32 	%r168, %r165, 2027808484;
	xor.b32  	%r169, %r167, %r168;
	shr.u32 	%r170, %r165, 5;
	add.s32 	%r171, %r170, 2123724318;
	xor.b32  	%r172, %r169, %r171;
	add.s32 	%r173, %r172, %r157;
	shl.b32 	%r174, %r173, 4;
	add.s32 	%r175, %r174, -1556008596;
	add.s32 	%r176, %r173, 387276957;
	xor.b32  	%r177, %r175, %r176;
	shr.u32 	%r178, %r173, 5;
	add.s32 	%r179, %r178, -939442524;
	xor.b32  	%r180, %r177, %r179;
	add.s32 	%r181, %r180, %r165;
	shl.b32 	%r182, %r181, 4;
	add.s32 	%r183, %r182, -1383041155;
	add.s32 	%r184, %r181, 387276957;
	xor.b32  	%r185, %r183, %r184;
	shr.u32 	%r186, %r181, 5;
	add.s32 	%r187, %r186, 2123724318;
	xor.b32  	%r188, %r185, %r187;
	add.s32 	%r189, %r188, %r173;
	shl.b32 	%r190, %r189, 4;
	add.s32 	%r191, %r190, -1556008596;
	add.s32 	%r192, %r189, -1253254570;
	xor.b32  	%r193, %r191, %r192;
	shr.u32 	%r194, %r189, 5;
	add.s32 	%r195, %r194, -939442524;
	xor.b32  	%r196, %r193, %r195;
	add.s32 	%r197, %r196, %r181;
	shl.b32 	%r198, %r197, 4;
	add.s32 	%r199, %r198, -1383041155;
	add.s32 	%r200, %r197, -1253254570;
	xor.b32  	%r201, %r199, %r200;
	shr.u32 	%r202, %r197, 5;
	add.s32 	%r203, %r202, 2123724318;
	xor.b32  	%r204, %r201, %r203;
	add.s32 	%r205, %r204, %r189;
	shl.b32 	%r206, %r205, 4;
	add.s32 	%r207, %r206, -1556008596;
	add.s32 	%r208, %r205, 1401181199;
	xor.b32  	%r209, %r207, %r208;
	shr.u32 	%r210, %r205, 5;
	add.s32 	%r211, %r210, -939442524;
	xor.b32  	%r212, %r209, %r211;
	add.s32 	%r213, %r212, %r197;
	shl.b32 	%r214, %r213, 4;
	add.s32 	%r215, %r214, -1383041155;
	add.s32 	%r216, %r213, 1401181199;
	xor.b32  	%r217, %r215, %r216;
	shr.u32 	%r218, %r213, 5;
	add.s32 	%r219, %r218, 2123724318;
	xor.b32  	%r220, %r217, %r219;
	add.s32 	%r221, %r220, %r205;
	shl.b32 	%r222, %r221, 4;
	add.s32 	%r223, %r222, 591475052;
	add.s32 	%r224, %r221, 1908133320;
	xor.b32  	%r225, %r223, %r224;
	shr.u32 	%r226, %r221, 5;
	add.s32 	%r227, %r226, 1208041124;
	xor.b32  	%r228, %r225, %r227;
	add.s32 	%r229, %r228, %r213;
	and.b32  	%r230, %r229, 2147483647;
	cvt.rn.f32.s32	%f224, %r230;
	mul.ftz.f32 	%f653, %f224, 0f30000000;
	bra.uni 	LBB5_10;
LBB5_122:
LBB5_14:
	add.ftz.f32 	%f237, %f13, %f656;
	ld.v2.f32 	{%f658, %f659}, [%rd11+40];
	add.ftz.f32 	%f238, %f237, %f658;
	cvt.rmi.ftz.f32.f32	%f239, %f238;
	sub.ftz.f32 	%f240, %f238, %f239;
	mov.f32 	%f241, 0f3727C5AC;
	max.ftz.f32 	%f242, %f240, %f241;
	mov.f32 	%f243, 0f3F7FFF58;
	min.ftz.f32 	%f653, %f242, %f243;
	ld.u32 	%r1412, [%rd11+16];
	bra.uni 	LBB5_15;
LBB5_120:
LBB5_6:
	setp.eq.s32	%p6, %r2, 1;
	@%p6 bra 	LBB5_9;
	bra.uni 	LBB5_7;
LBB5_9:
	ld.v2.u32 	{%r239, %r240}, [%rd11+24];
	mul.wide.s32 	%rd59, %r240, -2032597691;
	shr.u64 	%rd60, %rd59, 32;
	cvt.u32.u64	%r241, %rd60;
	add.s32 	%r242, %r241, %r240;
	shr.u32 	%r243, %r242, 31;
	shr.s32 	%r244, %r242, 7;
	add.s32 	%r245, %r244, %r243;
	mul.lo.s32 	%r246, %r245, 243;
	sub.s32 	%r247, %r240, %r246;
	cvt.rn.f32.s32	%f228, %r247;
	neg.ftz.f32 	%f229, %f228;
	fma.rn.ftz.f32 	%f653, %f653, 0f43730000, %f229;
	bra.uni 	LBB5_10;
LBB5_7:
	setp.ne.s32	%p7, %r2, 0;
	@%p7 bra 	LBB5_121;
	ld.v2.u32 	{%r248, %r249}, [%rd11+24];
	shr.s32 	%r250, %r248, 31;
	shr.u32 	%r251, %r250, 24;
	add.s32 	%r252, %r248, %r251;
	and.b32  	%r253, %r252, -256;
	sub.s32 	%r254, %r248, %r253;
	cvt.rn.f32.s32	%f230, %r254;
	neg.ftz.f32 	%f231, %f230;
	fma.rn.ftz.f32 	%f653, %f653, 0f43800000, %f231;
	bra.uni 	LBB5_10;
LBB5_121:
LBB5_10:
	add.s32 	%r1412, %r2, 1;
	ld.u32 	%r1402, [%rd11+32];
	ld.u32 	%r1403, [%rd11+36];
	ld.v2.f32 	{%f658, %f659}, [%rd11+40];
LBB5_15:
	ld.v2.u32 	{%r21, %r22}, [%rd11+24];
	ld.u32 	%r23, [%rd11+20];
	ld.v2.f32 	{%f25, %f26}, [%rd11+8];
	ld.u32 	%r24, [%rd11];
	setp.lt.s32	%p41, %r1, 1;
	@%p41 bra 	LBB5_16;
	// inline asm
	mov.b64 {_,%r320}, %rd3;
	// inline asm
	mov.u32 	%r321, 1;
	// inline asm
	call (%rd62), _rt_buffer_get_id_64, (%r320, %r321, %r321, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	mov.u32 	%r1406, 0;
	mov.u32 	%r1404, %r1;
LBB5_18:
	add.s32 	%r324, %r1406, %r1404;
	shr.u32 	%r325, %r324, 31;
	add.s32 	%r326, %r324, %r325;
	shr.s32 	%r327, %r326, 1;
	add.s32 	%r328, %r327, 1;
	mul.wide.s32 	%rd67, %r328, 4;
	add.s64 	%rd68, %rd67, %rd62;
	ld.f32 	%f244, [%rd68+512];
	setp.lt.ftz.f32	%p42, %f653, %f244;
	selp.b32	%r1404, %r327, %r1404, %p42;
	selp.b32	%r1406, %r1406, %r328, %p42;
	setp.lt.s32	%p43, %r1406, %r1404;
	@%p43 bra 	LBB5_18;
	bra.uni 	LBB5_19;
LBB5_16:
	mov.u32 	%r1406, 0;
LBB5_19:
	cvt.s64.s32	%rd2, %r104;
	cvt.s64.s32	%rd4, %r105;
	add.s32 	%r329, %r1, -1;
	setp.gt.s32	%p44, %r1406, %r329;
	selp.b32	%r30, %r329, %r1406, %p44;
	setp.eq.s32	%p45, %r24, 1;
	@%p45 bra 	LBB5_39;
	bra.uni 	LBB5_20;
LBB5_39:
	cvt.rn.f32.s32	%f263, %r1403;
	rcp.approx.ftz.f32 	%f264, %f263;
	mul.ftz.f32 	%f52, %f264, 0f3F000000;
	cvt.rn.f32.s32	%f265, %r1402;
	fma.rn.ftz.f32 	%f53, %f265, %f264, %f52;
	setp.gt.s32	%p58, %r1403, 1;
	selp.u32	%r620, 1, 0, %p58;
	setp.gt.s32	%p59, %r1403, 2;
	selp.b32	%r621, 2, 1, %p58;
	selp.b32	%r622, %r621, %r620, %p59;
	setp.gt.s32	%p60, %r1403, 4;
	selp.u32	%r623, 1, 0, %p60;
	setp.gt.s32	%p61, %r1403, 8;
	selp.u32	%r624, 1, 0, %p61;
	setp.gt.s32	%p62, %r1403, 16;
	selp.u32	%r625, 1, 0, %p62;
	setp.gt.s32	%p63, %r1403, 32;
	selp.u32	%r626, 1, 0, %p63;
	setp.gt.s32	%p64, %r1403, 64;
	selp.u32	%r627, 1, 0, %p64;
	setp.gt.s32	%p65, %r1403, 128;
	selp.u32	%r628, 1, 0, %p65;
	setp.gt.s32	%p66, %r1403, 256;
	selp.u32	%r629, 1, 0, %p66;
	setp.gt.s32	%p67, %r1403, 512;
	selp.u32	%r630, 1, 0, %p67;
	setp.gt.s32	%p68, %r1403, 1024;
	selp.u32	%r631, 1, 0, %p68;
	setp.gt.s32	%p69, %r1403, 2048;
	selp.u32	%r632, 1, 0, %p69;
	setp.gt.s32	%p70, %r1403, 4096;
	selp.u32	%r633, 1, 0, %p70;
	setp.gt.s32	%p71, %r1403, 8192;
	selp.u32	%r634, 1, 0, %p71;
	setp.gt.s32	%p72, %r1403, 16384;
	selp.u32	%r635, 1, 0, %p72;
	setp.gt.s32	%p73, %r1403, 32768;
	selp.u32	%r636, 1, 0, %p73;
	setp.gt.s32	%p74, %r1403, 65536;
	selp.u32	%r637, 1, 0, %p74;
	setp.gt.s32	%p75, %r1403, 131072;
	selp.u32	%r638, 1, 0, %p75;
	setp.gt.s32	%p76, %r1403, 262144;
	selp.u32	%r639, 1, 0, %p76;
	setp.gt.s32	%p77, %r1403, 524288;
	selp.u32	%r640, 1, 0, %p77;
	setp.gt.s32	%p78, %r1403, 1048576;
	selp.u32	%r641, 1, 0, %p78;
	setp.gt.s32	%p79, %r1403, 2097152;
	selp.u32	%r642, 1, 0, %p79;
	setp.gt.s32	%p80, %r1403, 4194304;
	selp.u32	%r643, 1, 0, %p80;
	setp.gt.s32	%p81, %r1403, 8388608;
	selp.u32	%r644, 1, 0, %p81;
	setp.gt.s32	%p82, %r1403, 16777216;
	selp.u32	%r645, 1, 0, %p82;
	setp.gt.s32	%p83, %r1403, 33554432;
	selp.u32	%r646, 1, 0, %p83;
	setp.gt.s32	%p84, %r1403, 67108864;
	selp.u32	%r647, 1, 0, %p84;
	setp.gt.s32	%p85, %r1403, 134217728;
	selp.u32	%r648, 1, 0, %p85;
	setp.gt.s32	%p86, %r1403, 268435456;
	selp.u32	%r649, 1, 0, %p86;
	setp.gt.s32	%p87, %r1403, 536870912;
	selp.u32	%r650, 1, 0, %p87;
	setp.gt.s32	%p88, %r1403, 1073741824;
	selp.u32	%r651, 1, 0, %p88;
	add.s32 	%r652, %r624, %r623;
	add.s32 	%r653, %r652, %r625;
	add.s32 	%r654, %r653, %r626;
	add.s32 	%r655, %r654, %r627;
	add.s32 	%r656, %r655, %r628;
	add.s32 	%r657, %r656, %r629;
	add.s32 	%r658, %r657, %r630;
	add.s32 	%r659, %r658, %r631;
	add.s32 	%r660, %r659, %r632;
	add.s32 	%r661, %r660, %r633;
	add.s32 	%r662, %r661, %r634;
	add.s32 	%r663, %r662, %r635;
	add.s32 	%r664, %r663, %r636;
	add.s32 	%r665, %r664, %r637;
	add.s32 	%r666, %r665, %r638;
	add.s32 	%r667, %r666, %r639;
	add.s32 	%r668, %r667, %r640;
	add.s32 	%r669, %r668, %r641;
	add.s32 	%r670, %r669, %r642;
	add.s32 	%r671, %r670, %r643;
	add.s32 	%r672, %r671, %r644;
	add.s32 	%r673, %r672, %r645;
	add.s32 	%r674, %r673, %r646;
	add.s32 	%r675, %r674, %r647;
	add.s32 	%r676, %r675, %r648;
	add.s32 	%r677, %r676, %r649;
	add.s32 	%r678, %r677, %r650;
	add.s32 	%r679, %r678, %r651;
	add.s32 	%r680, %r679, %r622;
	mov.u32 	%r681, 0;
	max.s32 	%r1410, %r680, %r681;
	mov.f32 	%f672, 0f00000000;
	setp.lt.s32	%p89, %r1410, 1;
	@%p89 bra 	LBB5_128;
	mov.f32 	%f670, 0f3F800000;
	mov.u32 	%r1411, %r1402;
LBB5_41:
	and.b32  	%r682, %r1411, 1;
	shr.u32 	%r1411, %r1411, 1;
	mul.ftz.f32 	%f670, %f670, 0f3F000000;
	cvt.rn.f32.s32	%f266, %r682;
	fma.rn.ftz.f32 	%f672, %f670, %f266, %f672;
	add.s32 	%r1410, %r1410, -1;
	setp.ne.s32	%p90, %r1410, 0;
	@%p90 bra 	LBB5_41;
	bra.uni 	LBB5_42;
LBB5_20:
	setp.lt.s32	%p46, %r1412, 8;
	@%p46 bra 	LBB5_22;
	bra.uni 	LBB5_21;
LBB5_22:
	mov.f32 	%f668, 0f00000000;
	setp.lt.s32	%p47, %r23, 1;
	@%p47 bra 	LBB5_123;
	and.b32  	%r459, %r1412, 63;
	// inline asm
	mov.b64 {_,%r451}, %rd9;
	// inline asm
	// inline asm
	call (%rd70), _rt_buffer_get_id_64, (%r451, %r602, %r602, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	ld.u64 	%rd75, [%rd70+512];
	mul.wide.u32 	%rd81, %r459, 4;
	// inline asm
	mov.b64 {_,%r455}, %rd75;
	// inline asm
	// inline asm
	call (%rd76), _rt_buffer_get_id_64, (%r455, %r602, %r602, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd82, %rd81, %rd76;
	ld.u32 	%r31, [%rd82+512];
	cvt.rn.f32.s32	%f247, %r31;
	rcp.approx.ftz.f32 	%f28, %f247;
	mov.f32 	%f660, %f28;
	mov.u32 	%r1407, %r23;
LBB5_24:
	setp.ne.s32	%p48, %r31, 0;
	selp.b32	%r460, %r31, %r1407, %p48;
	rem.s32 	%r461, %r1407, %r460;
	cvt.rn.f32.s32	%f248, %r461;
	fma.rn.ftz.f32 	%f668, %f660, %f248, %f668;
	mul.ftz.f32 	%f660, %f28, %f660;
	selp.b32	%r462, %r31, -2147483648, %p48;
	div.s32 	%r1407, %r1407, %r462;
	setp.gt.s32	%p49, %r1407, 0;
	@%p49 bra 	LBB5_24;
	bra.uni 	LBB5_25;
LBB5_21:
	shl.b32 	%r330, %r23, 4;
	add.s32 	%r331, %r330, -1556008596;
	add.s32 	%r332, %r23, -1640531527;
	xor.b32  	%r333, %r331, %r332;
	shr.u32 	%r334, %r23, 5;
	add.s32 	%r335, %r334, -939442524;
	xor.b32  	%r336, %r333, %r335;
	add.s32 	%r337, %r336, %r1412;
	shl.b32 	%r338, %r337, 4;
	add.s32 	%r339, %r338, -1383041155;
	add.s32 	%r340, %r337, -1640531527;
	xor.b32  	%r341, %r339, %r340;
	shr.u32 	%r342, %r337, 5;
	add.s32 	%r343, %r342, 2123724318;
	xor.b32  	%r344, %r341, %r343;
	add.s32 	%r345, %r344, %r23;
	shl.b32 	%r346, %r345, 4;
	add.s32 	%r347, %r346, -1556008596;
	add.s32 	%r348, %r345, 1013904242;
	xor.b32  	%r349, %r347, %r348;
	shr.u32 	%r350, %r345, 5;
	add.s32 	%r351, %r350, -939442524;
	xor.b32  	%r352, %r349, %r351;
	add.s32 	%r353, %r352, %r337;
	shl.b32 	%r354, %r353, 4;
	add.s32 	%r355, %r354, -1383041155;
	add.s32 	%r356, %r353, 1013904242;
	xor.b32  	%r357, %r355, %r356;
	shr.u32 	%r358, %r353, 5;
	add.s32 	%r359, %r358, 2123724318;
	xor.b32  	%r360, %r357, %r359;
	add.s32 	%r361, %r360, %r345;
	shl.b32 	%r362, %r361, 4;
	add.s32 	%r363, %r362, -1556008596;
	add.s32 	%r364, %r361, -626627285;
	xor.b32  	%r365, %r363, %r364;
	shr.u32 	%r366, %r361, 5;
	add.s32 	%r367, %r366, -939442524;
	xor.b32  	%r368, %r365, %r367;
	add.s32 	%r369, %r368, %r353;
	shl.b32 	%r370, %r369, 4;
	add.s32 	%r371, %r370, -1383041155;
	add.s32 	%r372, %r369, -626627285;
	xor.b32  	%r373, %r371, %r372;
	shr.u32 	%r374, %r369, 5;
	add.s32 	%r375, %r374, 2123724318;
	xor.b32  	%r376, %r373, %r375;
	add.s32 	%r377, %r376, %r361;
	shl.b32 	%r378, %r377, 4;
	add.s32 	%r379, %r378, -1556008596;
	add.s32 	%r380, %r377, 2027808484;
	xor.b32  	%r381, %r379, %r380;
	shr.u32 	%r382, %r377, 5;
	add.s32 	%r383, %r382, -939442524;
	xor.b32  	%r384, %r381, %r383;
	add.s32 	%r385, %r384, %r369;
	shl.b32 	%r386, %r385, 4;
	add.s32 	%r387, %r386, -1383041155;
	add.s32 	%r388, %r385, 2027808484;
	xor.b32  	%r389, %r387, %r388;
	shr.u32 	%r390, %r385, 5;
	add.s32 	%r391, %r390, 2123724318;
	xor.b32  	%r392, %r389, %r391;
	add.s32 	%r393, %r392, %r377;
	shl.b32 	%r394, %r393, 4;
	add.s32 	%r395, %r394, -1556008596;
	add.s32 	%r396, %r393, 387276957;
	xor.b32  	%r397, %r395, %r396;
	shr.u32 	%r398, %r393, 5;
	add.s32 	%r399, %r398, -939442524;
	xor.b32  	%r400, %r397, %r399;
	add.s32 	%r401, %r400, %r385;
	shl.b32 	%r402, %r401, 4;
	add.s32 	%r403, %r402, -1383041155;
	add.s32 	%r404, %r401, 387276957;
	xor.b32  	%r405, %r403, %r404;
	shr.u32 	%r406, %r401, 5;
	add.s32 	%r407, %r406, 2123724318;
	xor.b32  	%r408, %r405, %r407;
	add.s32 	%r409, %r408, %r393;
	shl.b32 	%r410, %r409, 4;
	add.s32 	%r411, %r410, -1556008596;
	add.s32 	%r412, %r409, -1253254570;
	xor.b32  	%r413, %r411, %r412;
	shr.u32 	%r414, %r409, 5;
	add.s32 	%r415, %r414, -939442524;
	xor.b32  	%r416, %r413, %r415;
	add.s32 	%r417, %r416, %r401;
	shl.b32 	%r418, %r417, 4;
	add.s32 	%r419, %r418, -1383041155;
	add.s32 	%r420, %r417, -1253254570;
	xor.b32  	%r421, %r419, %r420;
	shr.u32 	%r422, %r417, 5;
	add.s32 	%r423, %r422, 2123724318;
	xor.b32  	%r424, %r421, %r423;
	add.s32 	%r425, %r424, %r409;
	shl.b32 	%r426, %r425, 4;
	add.s32 	%r427, %r426, -1556008596;
	add.s32 	%r428, %r425, 1401181199;
	xor.b32  	%r429, %r427, %r428;
	shr.u32 	%r430, %r425, 5;
	add.s32 	%r431, %r430, -939442524;
	xor.b32  	%r432, %r429, %r431;
	add.s32 	%r433, %r432, %r417;
	shl.b32 	%r434, %r433, 4;
	add.s32 	%r435, %r434, -1383041155;
	add.s32 	%r436, %r433, 1401181199;
	xor.b32  	%r437, %r435, %r436;
	shr.u32 	%r438, %r433, 5;
	add.s32 	%r439, %r438, 2123724318;
	xor.b32  	%r440, %r437, %r439;
	add.s32 	%r441, %r440, %r425;
	shl.b32 	%r442, %r441, 4;
	add.s32 	%r443, %r442, 591475052;
	add.s32 	%r444, %r441, 1908133320;
	xor.b32  	%r445, %r443, %r444;
	shr.u32 	%r446, %r441, 5;
	add.s32 	%r447, %r446, 1208041124;
	xor.b32  	%r448, %r445, %r447;
	add.s32 	%r449, %r448, %r433;
	and.b32  	%r450, %r449, 2147483647;
	cvt.rn.f32.s32	%f245, %r450;
	mul.ftz.f32 	%f668, %f245, 0f30000000;
	bra.uni 	LBB5_29;
LBB5_128:
LBB5_42:
	add.ftz.f32 	%f267, %f52, %f672;
	add.ftz.f32 	%f268, %f659, %f267;
	add.ftz.f32 	%f269, %f658, %f53;
	cvt.rmi.ftz.f32.f32	%f270, %f269;
	cvt.rmi.ftz.f32.f32	%f271, %f268;
	sub.ftz.f32 	%f272, %f268, %f271;
	sub.ftz.f32 	%f273, %f269, %f270;
	mov.f32 	%f274, 0f3727C5AC;
	max.ftz.f32 	%f275, %f273, %f274;
	mov.f32 	%f276, 0f3F7FFF58;
	min.ftz.f32 	%f668, %f275, %f276;
	max.ftz.f32 	%f277, %f272, %f274;
	min.ftz.f32 	%f669, %f277, %f276;
	bra.uni 	LBB5_43;
LBB5_123:
LBB5_25:
	setp.eq.s32	%p50, %r1412, 1;
	@%p50 bra 	LBB5_28;
	bra.uni 	LBB5_26;
LBB5_28:
	mul.wide.s32 	%rd83, %r22, -2032597691;
	shr.u64 	%rd84, %rd83, 32;
	cvt.u32.u64	%r463, %rd84;
	add.s32 	%r464, %r463, %r22;
	shr.u32 	%r465, %r464, 31;
	shr.s32 	%r466, %r464, 7;
	add.s32 	%r467, %r466, %r465;
	mul.lo.s32 	%r468, %r467, 243;
	sub.s32 	%r469, %r22, %r468;
	cvt.rn.f32.s32	%f249, %r469;
	neg.ftz.f32 	%f250, %f249;
	fma.rn.ftz.f32 	%f668, %f668, 0f43730000, %f250;
	add.s32 	%r1408, %r1412, 1;
	bra.uni 	LBB5_31;
LBB5_26:
	setp.ne.s32	%p51, %r1412, 0;
	@%p51 bra 	LBB5_124;
	shr.s32 	%r470, %r21, 31;
	shr.u32 	%r471, %r470, 24;
	add.s32 	%r472, %r21, %r471;
	and.b32  	%r473, %r472, -256;
	sub.s32 	%r474, %r21, %r473;
	cvt.rn.f32.s32	%f251, %r474;
	neg.ftz.f32 	%f252, %f251;
	fma.rn.ftz.f32 	%f668, %f668, 0f43800000, %f252;
	add.s32 	%r1408, %r1412, 1;
	bra.uni 	LBB5_31;
LBB5_124:
LBB5_29:
	add.s32 	%r1408, %r1412, 1;
	setp.lt.s32	%p52, %r1408, 8;
	@%p52 bra 	LBB5_125;
	bra.uni 	LBB5_30;
LBB5_125:
LBB5_31:
	mov.f32 	%f669, 0f00000000;
	setp.lt.s32	%p53, %r23, 1;
	@%p53 bra 	LBB5_126;
	and.b32  	%r604, %r1408, 63;
	// inline asm
	mov.b64 {_,%r596}, %rd9;
	// inline asm
	// inline asm
	call (%rd86), _rt_buffer_get_id_64, (%r596, %r602, %r602, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	ld.u64 	%rd91, [%rd86+512];
	mul.wide.u32 	%rd97, %r604, 4;
	// inline asm
	mov.b64 {_,%r600}, %rd91;
	// inline asm
	// inline asm
	call (%rd92), _rt_buffer_get_id_64, (%r600, %r602, %r602, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd98, %rd97, %rd92;
	ld.u32 	%r37, [%rd98+512];
	cvt.rn.f32.s32	%f255, %r37;
	rcp.approx.ftz.f32 	%f40, %f255;
	mov.f32 	%f665, %f40;
	mov.u32 	%r1409, %r23;
LBB5_33:
	setp.ne.s32	%p54, %r37, 0;
	selp.b32	%r605, %r37, %r1409, %p54;
	rem.s32 	%r606, %r1409, %r605;
	cvt.rn.f32.s32	%f256, %r606;
	fma.rn.ftz.f32 	%f669, %f665, %f256, %f669;
	mul.ftz.f32 	%f665, %f40, %f665;
	selp.b32	%r607, %r37, -2147483648, %p54;
	div.s32 	%r1409, %r1409, %r607;
	setp.gt.s32	%p55, %r1409, 0;
	@%p55 bra 	LBB5_33;
	bra.uni 	LBB5_34;
LBB5_30:
	shl.b32 	%r475, %r23, 4;
	add.s32 	%r476, %r475, -1556008596;
	add.s32 	%r477, %r23, -1640531527;
	xor.b32  	%r478, %r476, %r477;
	shr.u32 	%r479, %r23, 5;
	add.s32 	%r480, %r479, -939442524;
	xor.b32  	%r481, %r478, %r480;
	add.s32 	%r482, %r481, %r1408;
	shl.b32 	%r483, %r482, 4;
	add.s32 	%r484, %r483, -1383041155;
	add.s32 	%r485, %r482, -1640531527;
	xor.b32  	%r486, %r484, %r485;
	shr.u32 	%r487, %r482, 5;
	add.s32 	%r488, %r487, 2123724318;
	xor.b32  	%r489, %r486, %r488;
	add.s32 	%r490, %r489, %r23;
	shl.b32 	%r491, %r490, 4;
	add.s32 	%r492, %r491, -1556008596;
	add.s32 	%r493, %r490, 1013904242;
	xor.b32  	%r494, %r492, %r493;
	shr.u32 	%r495, %r490, 5;
	add.s32 	%r496, %r495, -939442524;
	xor.b32  	%r497, %r494, %r496;
	add.s32 	%r498, %r497, %r482;
	shl.b32 	%r499, %r498, 4;
	add.s32 	%r500, %r499, -1383041155;
	add.s32 	%r501, %r498, 1013904242;
	xor.b32  	%r502, %r500, %r501;
	shr.u32 	%r503, %r498, 5;
	add.s32 	%r504, %r503, 2123724318;
	xor.b32  	%r505, %r502, %r504;
	add.s32 	%r506, %r505, %r490;
	shl.b32 	%r507, %r506, 4;
	add.s32 	%r508, %r507, -1556008596;
	add.s32 	%r509, %r506, -626627285;
	xor.b32  	%r510, %r508, %r509;
	shr.u32 	%r511, %r506, 5;
	add.s32 	%r512, %r511, -939442524;
	xor.b32  	%r513, %r510, %r512;
	add.s32 	%r514, %r513, %r498;
	shl.b32 	%r515, %r514, 4;
	add.s32 	%r516, %r515, -1383041155;
	add.s32 	%r517, %r514, -626627285;
	xor.b32  	%r518, %r516, %r517;
	shr.u32 	%r519, %r514, 5;
	add.s32 	%r520, %r519, 2123724318;
	xor.b32  	%r521, %r518, %r520;
	add.s32 	%r522, %r521, %r506;
	shl.b32 	%r523, %r522, 4;
	add.s32 	%r524, %r523, -1556008596;
	add.s32 	%r525, %r522, 2027808484;
	xor.b32  	%r526, %r524, %r525;
	shr.u32 	%r527, %r522, 5;
	add.s32 	%r528, %r527, -939442524;
	xor.b32  	%r529, %r526, %r528;
	add.s32 	%r530, %r529, %r514;
	shl.b32 	%r531, %r530, 4;
	add.s32 	%r532, %r531, -1383041155;
	add.s32 	%r533, %r530, 2027808484;
	xor.b32  	%r534, %r532, %r533;
	shr.u32 	%r535, %r530, 5;
	add.s32 	%r536, %r535, 2123724318;
	xor.b32  	%r537, %r534, %r536;
	add.s32 	%r538, %r537, %r522;
	shl.b32 	%r539, %r538, 4;
	add.s32 	%r540, %r539, -1556008596;
	add.s32 	%r541, %r538, 387276957;
	xor.b32  	%r542, %r540, %r541;
	shr.u32 	%r543, %r538, 5;
	add.s32 	%r544, %r543, -939442524;
	xor.b32  	%r545, %r542, %r544;
	add.s32 	%r546, %r545, %r530;
	shl.b32 	%r547, %r546, 4;
	add.s32 	%r548, %r547, -1383041155;
	add.s32 	%r549, %r546, 387276957;
	xor.b32  	%r550, %r548, %r549;
	shr.u32 	%r551, %r546, 5;
	add.s32 	%r552, %r551, 2123724318;
	xor.b32  	%r553, %r550, %r552;
	add.s32 	%r554, %r553, %r538;
	shl.b32 	%r555, %r554, 4;
	add.s32 	%r556, %r555, -1556008596;
	add.s32 	%r557, %r554, -1253254570;
	xor.b32  	%r558, %r556, %r557;
	shr.u32 	%r559, %r554, 5;
	add.s32 	%r560, %r559, -939442524;
	xor.b32  	%r561, %r558, %r560;
	add.s32 	%r562, %r561, %r546;
	shl.b32 	%r563, %r562, 4;
	add.s32 	%r564, %r563, -1383041155;
	add.s32 	%r565, %r562, -1253254570;
	xor.b32  	%r566, %r564, %r565;
	shr.u32 	%r567, %r562, 5;
	add.s32 	%r568, %r567, 2123724318;
	xor.b32  	%r569, %r566, %r568;
	add.s32 	%r570, %r569, %r554;
	shl.b32 	%r571, %r570, 4;
	add.s32 	%r572, %r571, -1556008596;
	add.s32 	%r573, %r570, 1401181199;
	xor.b32  	%r574, %r572, %r573;
	shr.u32 	%r575, %r570, 5;
	add.s32 	%r576, %r575, -939442524;
	xor.b32  	%r577, %r574, %r576;
	add.s32 	%r578, %r577, %r562;
	shl.b32 	%r579, %r578, 4;
	add.s32 	%r580, %r579, -1383041155;
	add.s32 	%r581, %r578, 1401181199;
	xor.b32  	%r582, %r580, %r581;
	shr.u32 	%r583, %r578, 5;
	add.s32 	%r584, %r583, 2123724318;
	xor.b32  	%r585, %r582, %r584;
	add.s32 	%r586, %r585, %r570;
	shl.b32 	%r587, %r586, 4;
	add.s32 	%r588, %r587, 591475052;
	add.s32 	%r589, %r586, 1908133320;
	xor.b32  	%r590, %r588, %r589;
	shr.u32 	%r591, %r586, 5;
	add.s32 	%r592, %r591, 1208041124;
	xor.b32  	%r593, %r590, %r592;
	add.s32 	%r594, %r593, %r578;
	and.b32  	%r595, %r594, 2147483647;
	cvt.rn.f32.s32	%f253, %r595;
	mul.ftz.f32 	%f669, %f253, 0f30000000;
	bra.uni 	LBB5_38;
LBB5_126:
LBB5_34:
	setp.ne.s32	%p56, %r1408, 0;
	@%p56 bra 	LBB5_36;
	bra.uni 	LBB5_35;
LBB5_36:
	setp.ne.s32	%p57, %r1412, 0;
	@%p57 bra 	LBB5_127;
	bra.uni 	LBB5_37;
LBB5_127:
	bra.uni 	LBB5_38;
LBB5_35:
	shr.s32 	%r615, %r21, 31;
	shr.u32 	%r616, %r615, 24;
	add.s32 	%r617, %r21, %r616;
	and.b32  	%r618, %r617, -256;
	sub.s32 	%r619, %r21, %r618;
	cvt.rn.f32.s32	%f259, %r619;
	neg.ftz.f32 	%f260, %f259;
	fma.rn.ftz.f32 	%f669, %f669, 0f43800000, %f260;
	bra.uni 	LBB5_38;
LBB5_37:
	mul.wide.s32 	%rd99, %r22, -2032597691;
	shr.u64 	%rd100, %rd99, 32;
	cvt.u32.u64	%r608, %rd100;
	add.s32 	%r609, %r608, %r22;
	shr.u32 	%r610, %r609, 31;
	shr.s32 	%r611, %r609, 7;
	add.s32 	%r612, %r611, %r610;
	mul.lo.s32 	%r613, %r612, 243;
	sub.s32 	%r614, %r22, %r613;
	cvt.rn.f32.s32	%f257, %r614;
	neg.ftz.f32 	%f258, %f257;
	fma.rn.ftz.f32 	%f669, %f669, 0f43730000, %f258;
LBB5_38:
	add.s32 	%r1412, %r1412, 2;
LBB5_43:
	sqrt.approx.ftz.f32 	%f278, %f668;
	mov.f32 	%f708, 0f3F800000;
	sub.ftz.f32 	%f280, %f708, %f278;
	sub.ftz.f32 	%f281, %f708, %f669;
	mul.ftz.f32 	%f282, %f281, %f278;
	mul.ftz.f32 	%f283, %f669, %f278;
	mul.lo.s32 	%r713, %r30, 3;
	mul.wide.s32 	%rd143, %r713, 4;
	// inline asm
	mov.b64 {_,%r683}, %rd36;
	// inline asm
	// inline asm
	call (%rd102), _rt_buffer_get_id_64, (%r683, %r602, %r602, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd144, %rd102, 512;
	add.s32 	%r714, %r713, 2;
	mul.wide.s32 	%rd145, %r714, 4;
	add.s64 	%rd146, %rd145, %rd144;
	add.s32 	%r715, %r713, 1;
	mul.wide.s32 	%rd147, %r715, 4;
	add.s64 	%rd148, %rd147, %rd144;
	add.s64 	%rd149, %rd143, %rd144;
	ld.u32 	%r716, [%rd149];
	ld.u32 	%r717, [%rd148];
	ld.u32 	%r718, [%rd146];
	// inline asm
	mov.b64 {_,%r687}, %rd9;
	// inline asm
	// inline asm
	call (%rd108), _rt_buffer_get_id_64, (%r687, %r602, %r602, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	ld.u64 	%rd113, [%rd108+528];
	shl.b64 	%rd150, %rd4, 4;
	// inline asm
	mov.b64 {_,%r691}, %rd113;
	// inline asm
	// inline asm
	call (%rd114), _rt_buffer_get_id_64, (%r691, %r602, %r602, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd151, %rd150, %rd114;
	ld.u64 	%rd119, [%rd151+520];
	mul.wide.s32 	%rd152, %r716, 52;
	// inline asm
	mov.b64 {_,%r695}, %rd119;
	// inline asm
	// inline asm
	call (%rd120), _rt_buffer_get_id_64, (%r695, %r602, %r602, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd153, %rd120, 512;
	mul.wide.s32 	%rd154, %r718, 52;
	add.s64 	%rd155, %rd154, %rd153;
	mul.wide.s32 	%rd156, %r717, 52;
	add.s64 	%rd157, %rd156, %rd153;
	add.s64 	%rd158, %rd152, %rd153;
	ld.f32 	%f284, [%rd158];
	ld.f32 	%f285, [%rd158+4];
	ld.f32 	%f286, [%rd158+8];
	ld.f32 	%f287, [%rd157];
	ld.f32 	%f288, [%rd157+4];
	ld.f32 	%f289, [%rd157+8];
	ld.f32 	%f290, [%rd155];
	ld.f32 	%f291, [%rd155+4];
	ld.f32 	%f292, [%rd155+8];
	mul.ftz.f32 	%f294, %f282, %f287;
	mul.ftz.f32 	%f295, %f282, %f288;
	fma.rn.ftz.f32 	%f296, %f280, %f285, %f295;
	sub.ftz.f32 	%f302, %f287, %f284;
	sub.ftz.f32 	%f303, %f288, %f285;
	sub.ftz.f32 	%f304, %f289, %f286;
	sub.ftz.f32 	%f305, %f290, %f284;
	sub.ftz.f32 	%f306, %f292, %f286;
	sub.ftz.f32 	%f307, %f291, %f285;
	mul.ftz.f32 	%f308, %f304, %f307;
	neg.ftz.f32 	%f309, %f308;
	fma.rn.ftz.f32 	%f310, %f303, %f306, %f309;
	mul.ftz.f32 	%f311, %f302, %f306;
	neg.ftz.f32 	%f312, %f311;
	fma.rn.ftz.f32 	%f313, %f304, %f305, %f312;
	mul.ftz.f32 	%f314, %f303, %f305;
	neg.ftz.f32 	%f315, %f314;
	fma.rn.ftz.f32 	%f316, %f302, %f307, %f315;
	mul.ftz.f32 	%f317, %f313, %f313;
	fma.rn.ftz.f32 	%f318, %f310, %f310, %f317;
	fma.rn.ftz.f32 	%f319, %f316, %f316, %f318;
	rsqrt.approx.ftz.f32 	%f320, %f319;
	mul.ftz.f32 	%f321, %f320, %f310;
	mul.ftz.f32 	%f322, %f320, %f316;
	mul.ftz.f32 	%f323, %f320, %f313;
	ld.f32 	%f340, [%rd158+36];
	ld.f32 	%f341, [%rd158+40];
	ld.f32 	%f342, [%rd157+36];
	ld.f32 	%f343, [%rd157+40];
	ld.f32 	%f344, [%rd155+36];
	ld.f32 	%f345, [%rd155+40];
	mul.ftz.f32 	%f346, %f282, %f343;
	mul.ftz.f32 	%f347, %f282, %f342;
	ld.u64 	%rd125, [%rd108+552];
	mul.lo.s64 	%rd159, %rd2, 160;
	// inline asm
	mov.b64 {_,%r699}, %rd125;
	// inline asm
	// inline asm
	call (%rd126), _rt_buffer_get_id_64, (%r699, %r602, %r602, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd160, %rd126, %rd159;
	ld.v4.f32 	{%f350, %f351, %f352, %f353}, [%rd160+608];
	ld.v4.f32 	{%f354, %f355, %f356, %f357}, [%rd160+624];
	ld.v4.f32 	{%f358, %f359, %f360, %f361}, [%rd160+640];
	ld.v4.f32 	{%f362, %f363, %f364, %f365}, [%rd160+544];
	ld.v4.f32 	{%f366, %f367, %f368, %f369}, [%rd160+560];
	ld.v4.f32 	{%f376, %f377, %f378, %f379}, [%rd160+576];
	ld.v4.f32 	{%f383, %f384, %f385, %f386}, [%rd160+592];
	mul.ftz.f32 	%f387, %f323, %f359;
	mul.ftz.f32 	%f388, %f323, %f351;
	mul.ftz.f32 	%f389, %f323, %f355;
	fma.rn.ftz.f32 	%f390, %f321, %f354, %f389;
	fma.rn.ftz.f32 	%f391, %f321, %f350, %f388;
	fma.rn.ftz.f32 	%f392, %f321, %f358, %f387;
	fma.rn.ftz.f32 	%f394, %f322, %f352, %f391;
	fma.rn.ftz.f32 	%f395, %f322, %f356, %f390;
	mul.ftz.f32 	%f396, %f395, %f395;
	ld.u64 	%rd131, [%rd108+544];
	shl.b64 	%rd161, %rd2, 6;
	// inline asm
	mov.b64 {_,%r703}, %rd131;
	// inline asm
	// inline asm
	call (%rd132), _rt_buffer_get_id_64, (%r703, %r602, %r602, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd162, %rd161, %rd132;
	ld.f32 	%f426, [%rd162+512];
	ld.u32 	%r719, [%rd10];
	ld.u64 	%rd137, [%rd108+560];
	cvt.s64.s32	%rd7, %r719;
	mul.wide.s32 	%rd163, %r719, 672;
	// inline asm
	mov.b64 {_,%r707}, %rd137;
	// inline asm
	// inline asm
	call (%rd138), _rt_buffer_get_id_64, (%r707, %r602, %r602, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd164, %rd138, %rd163;
	ld.f32 	%f679, [%rd164+920];
	ld.f32 	%f678, [%rd164+916];
	ld.f32 	%f677, [%rd164+912];
	ld.f32 	%f96, [%rd164+956];
	ld.u32 	%r47, [%rd164+752];
	ld.f32 	%f97, [%rd164+756];
	ld.v2.f32 	{%f98, %f99}, [%rd164+768];
	ld.v2.f32 	{%f100, %f101}, [%rd164+760];
	ld.u32 	%r712, [%rd164+776];
	mov.u32 	%r1414, 0;
	setp.eq.s32	%p91, %r712, 3;
	@%p91 bra 	LBB5_48;
	setp.eq.s32	%p92, %r712, 2;
	@%p92 bra 	LBB5_47;
	bra.uni 	LBB5_45;
LBB5_47:
	mov.u32 	%r1413, 2;
	bra.uni 	LBB5_49;
LBB5_48:
	mov.u32 	%r1413, 3;
	bra.uni 	LBB5_49;
LBB5_45:
	setp.ne.s32	%p93, %r712, 1;
	mov.u32 	%r1413, %r1414;
	@%p93 bra 	LBB5_49;
	mov.u32 	%r1413, %r602;
LBB5_49:
	fma.rn.ftz.f32 	%f393, %f322, %f360, %f392;
	fma.rn.ftz.f32 	%f397, %f394, %f394, %f396;
	mul.ftz.f32 	%f293, %f282, %f289;
	fma.rn.ftz.f32 	%f297, %f280, %f284, %f294;
	fma.rn.ftz.f32 	%f301, %f283, %f291, %f296;
	fma.rn.ftz.f32 	%f348, %f280, %f340, %f347;
	fma.rn.ftz.f32 	%f349, %f280, %f341, %f346;
	// inline asm
	mov.b64 {_,%r723}, %rd9;
	// inline asm
	// inline asm
	call (%rd166), _rt_buffer_get_id_64, (%r723, %r602, %r602, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	ld.u64 	%rd171, [%rd166+560];
	mul.lo.s64 	%rd177, %rd7, 672;
	// inline asm
	mov.b64 {_,%r727}, %rd171;
	// inline asm
	// inline asm
	call (%rd172), _rt_buffer_get_id_64, (%r727, %r602, %r602, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd178, %rd177, %rd172;
	ld.u32 	%r49, [%rd178+1008];
	ld.f32 	%f102, [%rd178+1012];
	ld.v2.f32 	{%f103, %f104}, [%rd178+1024];
	ld.v2.f32 	{%f105, %f106}, [%rd178+1016];
	ld.u32 	%r732, [%rd178+1032];
	setp.eq.s32	%p94, %r732, 3;
	@%p94 bra 	LBB5_54;
	setp.eq.s32	%p95, %r732, 2;
	@%p95 bra 	LBB5_53;
	bra.uni 	LBB5_51;
LBB5_53:
	mov.u32 	%r1414, 2;
	bra.uni 	LBB5_55;
LBB5_54:
	mov.u32 	%r1414, 3;
	bra.uni 	LBB5_55;
LBB5_51:
	setp.ne.s32	%p96, %r732, 1;
	@%p96 bra 	LBB5_55;
	mov.u32 	%r1414, 1;
LBB5_55:
	fma.rn.ftz.f32 	%f398, %f393, %f393, %f397;
	fma.rn.ftz.f32 	%f298, %f280, %f286, %f293;
	fma.rn.ftz.f32 	%f300, %f283, %f290, %f297;
	mul.ftz.f32 	%f370, %f301, %f366;
	mul.ftz.f32 	%f371, %f301, %f367;
	mul.ftz.f32 	%f372, %f301, %f368;
	mov.f32 	%f434, 0f00000000;
	fma.rn.ftz.f32 	%f64, %f283, %f345, %f349;
	fma.rn.ftz.f32 	%f63, %f283, %f344, %f348;
	and.b32  	%r736, %r47, 1;
	setp.eq.b32	%p97, %r736, 1;
	@!%p97 bra 	LBB5_66;
	bra.uni 	LBB5_56;
LBB5_56:
	div.approx.ftz.f32 	%f436, %f63, %f98;
	div.approx.ftz.f32 	%f437, %f64, %f99;
	cos.approx.ftz.f32 	%f438, %f97;
	sin.approx.ftz.f32 	%f439, %f97;
	mul.ftz.f32 	%f440, %f437, %f439;
	neg.ftz.f32 	%f441, %f440;
	fma.rn.ftz.f32 	%f442, %f436, %f438, %f441;
	neg.ftz.f32 	%f443, %f101;
	fma.rn.ftz.f32 	%f444, %f436, %f439, %f443;
	fma.rn.ftz.f32 	%f432, %f437, %f438, %f444;
	sub.ftz.f32 	%f431, %f442, %f100;
	// inline asm
	mov.b64 {_,%r737}, %rd9;
	// inline asm
	mov.u32 	%r762, 1;
	// inline asm
	call (%rd180), _rt_buffer_get_id_64, (%r737, %r762, %r762, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	ld.u64 	%rd185, [%rd180+560];
	add.s64 	%rd216, %rd177, 176;
	// inline asm
	mov.b64 {_,%r741}, %rd185;
	// inline asm
	// inline asm
	call (%rd186), _rt_buffer_get_id_64, (%r741, %r762, %r762, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd217, %rd186, %rd216;
	ld.f32 	%f107, [%rd217+544];
	ld.f32 	%f108, [%rd217+548];
	ld.f32 	%f109, [%rd217+552];
	ld.u32 	%r764, [%rd217+512];
	ld.u64 	%rd218, [%rd217+520];
	ld.u64 	%rd191, [%rd180+520];
	mul.wide.s32 	%rd219, %r764, 32;
	// inline asm
	mov.b64 {_,%r745}, %rd191;
	// inline asm
	// inline asm
	call (%rd192), _rt_buffer_get_id_64, (%r745, %r762, %r762, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd220, %rd219, %rd192;
	ld.v2.u32 	{%r51, %r52}, [%rd220+528];
	st.u64 	[%SP+0], %rd218;
	cvt.u32.u64	%r749, %rd218;
	mov.u32 	%r750, 2;
	mov.u32 	%r751, 0;
	// inline asm
	call (%f427,%f428,%f429,%f430), _rt_texture_get_level_id, (%r749, %r750, %f431, %f432, %f434, %r751, %f434);
	// inline asm
	// inline asm
	mov.b64 {_,%r752}, %rd9;
	// inline asm
	// inline asm
	call (%rd198), _rt_buffer_get_id_64, (%r752, %r762, %r762, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	ld.u64 	%rd203, [%rd198+520];
	// inline asm
	mov.b64 {_,%r756}, %rd203;
	// inline asm
	// inline asm
	call (%rd204), _rt_buffer_get_id_64, (%r756, %r762, %r762, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd221, %rd204, %rd219;
	ld.u32 	%r765, [%rd221+512];
	setp.lt.s32	%p98, %r765, 4;
	ld.u64 	%rd209, [%rd198+560];
	// inline asm
	mov.b64 {_,%r760}, %rd209;
	// inline asm
	// inline asm
	call (%rd210), _rt_buffer_get_id_64, (%r760, %r762, %r762, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd222, %rd216, %rd210;
	ld.u32 	%r766, [%rd222+528];
	setp.ne.s32	%p99, %r766, 1;
	mov.f32 	%f675, %f434;
	@%p99 bra 	LBB5_58;
	cvt.rn.f32.s32	%f445, %r51;
	mov.f32 	%f446, 0f3F000000;
	div.approx.ftz.f32 	%f447, %f446, %f445;
	sub.ftz.f32 	%f449, %f708, %f447;
	max.ftz.f32 	%f450, %f431, %f447;
	min.ftz.f32 	%f451, %f450, %f449;
	sub.ftz.f32 	%f452, %f431, %f451;
	abs.ftz.f32 	%f453, %f452;
	mul.ftz.f32 	%f454, %f445, %f453;
	mov.f32 	%f455, 0f00000000;
	max.ftz.f32 	%f456, %f454, %f455;
	min.ftz.f32 	%f675, %f456, %f708;
LBB5_58:
	selp.f32	%f114, %f427, %f429, %p98;
	selp.f32	%f113, %f427, %f428, %p98;
	// inline asm
	mov.b64 {_,%r767}, %rd9;
	// inline asm
	// inline asm
	call (%rd224), _rt_buffer_get_id_64, (%r767, %r762, %r762, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	ld.u64 	%rd229, [%rd224+560];
	// inline asm
	mov.b64 {_,%r771}, %rd229;
	// inline asm
	// inline asm
	call (%rd230), _rt_buffer_get_id_64, (%r771, %r762, %r762, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd236, %rd177, %rd230;
	ld.u32 	%r775, [%rd236+708];
	setp.ne.s32	%p100, %r775, 1;
	mov.f32 	%f676, %f434;
	@%p100 bra 	LBB5_60;
	cvt.rn.f32.s32	%f458, %r52;
	mov.f32 	%f459, 0f3F000000;
	div.approx.ftz.f32 	%f460, %f459, %f458;
	sub.ftz.f32 	%f462, %f708, %f460;
	max.ftz.f32 	%f463, %f432, %f460;
	min.ftz.f32 	%f464, %f463, %f462;
	sub.ftz.f32 	%f465, %f432, %f464;
	abs.ftz.f32 	%f466, %f465;
	mul.ftz.f32 	%f467, %f458, %f466;
	mov.f32 	%f468, 0f00000000;
	max.ftz.f32 	%f469, %f467, %f468;
	min.ftz.f32 	%f676, %f469, %f708;
LBB5_60:
	sub.ftz.f32 	%f470, %f107, %f427;
	fma.rn.ftz.f32 	%f471, %f675, %f470, %f427;
	sub.ftz.f32 	%f472, %f108, %f113;
	fma.rn.ftz.f32 	%f473, %f675, %f472, %f113;
	sub.ftz.f32 	%f474, %f109, %f114;
	fma.rn.ftz.f32 	%f475, %f675, %f474, %f114;
	sub.ftz.f32 	%f476, %f107, %f471;
	fma.rn.ftz.f32 	%f477, %f676, %f476, %f471;
	sub.ftz.f32 	%f478, %f108, %f473;
	fma.rn.ftz.f32 	%f479, %f676, %f478, %f473;
	sub.ftz.f32 	%f480, %f109, %f475;
	fma.rn.ftz.f32 	%f481, %f676, %f480, %f475;
	mul.ftz.f32 	%f482, %f481, 0f3D93D07D;
	mov.f32 	%f483, 0f3F371437;
	fma.rn.ftz.f32 	%f484, %f483, %f479, %f482;
	mov.f32 	%f485, 0f3E59C6ED;
	fma.rn.ftz.f32 	%f486, %f485, %f477, %f484;
	sub.ftz.f32 	%f487, %f477, %f486;
	sub.ftz.f32 	%f488, %f479, %f486;
	sub.ftz.f32 	%f489, %f481, %f486;
	add.ftz.f32 	%f490, %f486, %f489;
	add.ftz.f32 	%f491, %f486, %f488;
	add.ftz.f32 	%f492, %f486, %f487;
	max.ftz.f32 	%f120, %f434, %f492;
	max.ftz.f32 	%f121, %f434, %f491;
	max.ftz.f32 	%f122, %f434, %f490;
	setp.eq.s32	%p101, %r1413, 3;
	@%p101 bra 	LBB5_65;
	setp.eq.s32	%p102, %r1413, 2;
	@%p102 bra 	LBB5_64;
	bra.uni 	LBB5_62;
LBB5_64:
	add.ftz.f32 	%f677, %f677, %f120;
	add.ftz.f32 	%f678, %f678, %f121;
	add.ftz.f32 	%f679, %f679, %f122;
	bra.uni 	LBB5_66;
LBB5_65:
	mul.ftz.f32 	%f677, %f677, %f120;
	mul.ftz.f32 	%f678, %f678, %f121;
	mul.ftz.f32 	%f679, %f679, %f122;
	bra.uni 	LBB5_66;
LBB5_62:
	setp.ne.s32	%p103, %r1413, 1;
	@%p103 bra 	LBB5_129;
	mul.ftz.f32 	%f677, %f677, %f120;
	mul.ftz.f32 	%f678, %f678, %f121;
	mul.ftz.f32 	%f679, %f679, %f122;
	bra.uni 	LBB5_66;
LBB5_129:
	mov.f32 	%f677, %f120;
	mov.f32 	%f678, %f121;
	mov.f32 	%f679, %f122;
LBB5_66:
	rsqrt.approx.ftz.f32 	%f399, %f398;
	fma.rn.ftz.f32 	%f299, %f283, %f292, %f298;
	fma.rn.ftz.f32 	%f373, %f300, %f364, %f372;
	fma.rn.ftz.f32 	%f374, %f300, %f363, %f371;
	fma.rn.ftz.f32 	%f375, %f300, %f362, %f370;
	sub.ftz.f32 	%f682, %f708, %f96;
	and.b32  	%r776, %r49, 1;
	setp.eq.b32	%p104, %r776, 1;
	@!%p104 bra 	LBB5_77;
	bra.uni 	LBB5_67;
LBB5_67:
	div.approx.ftz.f32 	%f504, %f63, %f103;
	div.approx.ftz.f32 	%f505, %f64, %f104;
	cos.approx.ftz.f32 	%f506, %f102;
	sin.approx.ftz.f32 	%f507, %f102;
	mul.ftz.f32 	%f508, %f505, %f507;
	neg.ftz.f32 	%f509, %f508;
	fma.rn.ftz.f32 	%f510, %f504, %f506, %f509;
	neg.ftz.f32 	%f511, %f106;
	fma.rn.ftz.f32 	%f512, %f504, %f507, %f511;
	fma.rn.ftz.f32 	%f500, %f505, %f506, %f512;
	sub.ftz.f32 	%f499, %f510, %f105;
	// inline asm
	mov.b64 {_,%r777}, %rd9;
	// inline asm
	mov.u32 	%r802, 1;
	// inline asm
	call (%rd238), _rt_buffer_get_id_64, (%r777, %r802, %r802, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	ld.u64 	%rd243, [%rd238+560];
	add.s64 	%rd274, %rd177, 448;
	// inline asm
	mov.b64 {_,%r781}, %rd243;
	// inline asm
	// inline asm
	call (%rd244), _rt_buffer_get_id_64, (%r781, %r802, %r802, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd275, %rd244, %rd274;
	ld.u32 	%r804, [%rd275+512];
	ld.u64 	%rd276, [%rd275+520];
	ld.u64 	%rd249, [%rd238+520];
	mul.wide.s32 	%rd277, %r804, 32;
	// inline asm
	mov.b64 {_,%r785}, %rd249;
	// inline asm
	// inline asm
	call (%rd250), _rt_buffer_get_id_64, (%r785, %r802, %r802, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd278, %rd250, %rd277;
	ld.u32 	%r53, [%rd278+536];
	ld.v4.f32 	{%f136, %f137, %f138, %f139}, [%rd275+544];
	ld.v2.u32 	{%r54, %r55}, [%rd278+528];
	st.u64 	[%SP+8], %rd276;
	cvt.u32.u64	%r789, %rd276;
	mov.u32 	%r790, 2;
	mov.u32 	%r791, 0;
	mov.f32 	%f681, 0f00000000;
	// inline asm
	call (%f495,%f496,%f497,%f498), _rt_texture_get_level_id, (%r789, %r790, %f499, %f500, %f681, %r791, %f681);
	// inline asm
	// inline asm
	mov.b64 {_,%r792}, %rd9;
	// inline asm
	// inline asm
	call (%rd256), _rt_buffer_get_id_64, (%r792, %r802, %r802, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	ld.u64 	%rd261, [%rd256+520];
	// inline asm
	mov.b64 {_,%r796}, %rd261;
	// inline asm
	// inline asm
	call (%rd262), _rt_buffer_get_id_64, (%r796, %r802, %r802, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd279, %rd262, %rd277;
	ld.u32 	%r805, [%rd279+512];
	setp.lt.s32	%p105, %r805, 4;
	ld.u64 	%rd267, [%rd256+560];
	// inline asm
	mov.b64 {_,%r800}, %rd267;
	// inline asm
	// inline asm
	call (%rd268), _rt_buffer_get_id_64, (%r800, %r802, %r802, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd280, %rd274, %rd268;
	ld.u32 	%r806, [%rd280+528];
	setp.ne.s32	%p106, %r806, 1;
	mov.f32 	%f680, %f681;
	@%p106 bra 	LBB5_69;
	cvt.rn.f32.s32	%f513, %r54;
	mov.f32 	%f514, 0f3F000000;
	div.approx.ftz.f32 	%f515, %f514, %f513;
	sub.ftz.f32 	%f517, %f708, %f515;
	max.ftz.f32 	%f518, %f499, %f515;
	min.ftz.f32 	%f519, %f518, %f517;
	sub.ftz.f32 	%f520, %f499, %f519;
	abs.ftz.f32 	%f521, %f520;
	mul.ftz.f32 	%f522, %f513, %f521;
	mov.f32 	%f523, 0f00000000;
	max.ftz.f32 	%f524, %f522, %f523;
	min.ftz.f32 	%f680, %f524, %f708;
LBB5_69:
	selp.f32	%f145, 0f3F800000, %f498, %p105;
	selp.f32	%f143, %f495, %f496, %p105;
	// inline asm
	mov.b64 {_,%r807}, %rd9;
	// inline asm
	// inline asm
	call (%rd282), _rt_buffer_get_id_64, (%r807, %r802, %r802, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	ld.u64 	%rd287, [%rd282+560];
	// inline asm
	mov.b64 {_,%r811}, %rd287;
	// inline asm
	// inline asm
	call (%rd288), _rt_buffer_get_id_64, (%r811, %r802, %r802, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd294, %rd177, %rd288;
	ld.u32 	%r815, [%rd294+980];
	setp.ne.s32	%p107, %r815, 1;
	@%p107 bra 	LBB5_71;
	cvt.rn.f32.s32	%f526, %r55;
	mov.f32 	%f527, 0f3F000000;
	div.approx.ftz.f32 	%f528, %f527, %f526;
	sub.ftz.f32 	%f530, %f708, %f528;
	max.ftz.f32 	%f531, %f500, %f528;
	min.ftz.f32 	%f532, %f531, %f530;
	sub.ftz.f32 	%f533, %f500, %f532;
	abs.ftz.f32 	%f534, %f533;
	mul.ftz.f32 	%f535, %f526, %f534;
	mov.f32 	%f536, 0f00000000;
	max.ftz.f32 	%f537, %f535, %f536;
	min.ftz.f32 	%f681, %f537, %f708;
LBB5_71:
	sub.ftz.f32 	%f538, %f136, %f495;
	fma.rn.ftz.f32 	%f539, %f680, %f538, %f495;
	sub.ftz.f32 	%f540, %f137, %f143;
	fma.rn.ftz.f32 	%f541, %f680, %f540, %f143;
	sub.ftz.f32 	%f542, %f139, %f145;
	fma.rn.ftz.f32 	%f543, %f680, %f542, %f145;
	sub.ftz.f32 	%f544, %f136, %f539;
	fma.rn.ftz.f32 	%f545, %f681, %f544, %f539;
	sub.ftz.f32 	%f546, %f137, %f541;
	fma.rn.ftz.f32 	%f547, %f681, %f546, %f541;
	sub.ftz.f32 	%f548, %f139, %f543;
	fma.rn.ftz.f32 	%f549, %f681, %f548, %f543;
	setp.gt.s32	%p108, %r53, 0;
	setp.gt.s32	%p109, %r53, 1;
	selp.f32	%f550, %f547, 0f00000000, %p109;
	selp.f32	%f551, %f545, 0f00000000, %p108;
	setp.gt.s32	%p110, %r53, 3;
	selp.f32	%f552, %f549, 0f00000000, %p110;
	setp.eq.s32	%p111, %r53, 2;
	selp.f32	%f553, %f552, %f551, %p110;
	selp.f32	%f150, %f550, %f553, %p111;
	setp.eq.s32	%p112, %r1414, 3;
	@%p112 bra 	LBB5_76;
	setp.eq.s32	%p113, %r1414, 2;
	@%p113 bra 	LBB5_75;
	bra.uni 	LBB5_73;
LBB5_75:
	add.ftz.f32 	%f682, %f682, %f150;
	bra.uni 	LBB5_77;
LBB5_76:
	mul.ftz.f32 	%f682, %f682, %f150;
	bra.uni 	LBB5_77;
LBB5_73:
	setp.ne.s32	%p114, %r1414, 1;
	@%p114 bra 	LBB5_130;
	mul.ftz.f32 	%f682, %f682, %f150;
	bra.uni 	LBB5_77;
LBB5_130:
	mov.f32 	%f682, %f150;
LBB5_77:
	mul.ftz.f32 	%f79, %f393, %f399;
	fma.rn.ftz.f32 	%f380, %f299, %f376, %f375;
	fma.rn.ftz.f32 	%f381, %f299, %f377, %f374;
	fma.rn.ftz.f32 	%f382, %f299, %f378, %f373;
	mul.ftz.f32 	%f554, %f679, 0f3D93D07D;
	mov.f32 	%f555, 0f3F371437;
	fma.rn.ftz.f32 	%f556, %f555, %f678, %f554;
	mov.f32 	%f557, 0f3E59C6ED;
	fma.rn.ftz.f32 	%f558, %f557, %f677, %f556;
	rcp.approx.ftz.f32 	%f559, %f558;
	mul.ftz.f32 	%f155, %f558, %f559;
	@%p45 bra 	LBB5_88;
	bra.uni 	LBB5_78;
LBB5_88:
	cvt.rn.f32.s32	%f570, %r1403;
	rcp.approx.ftz.f32 	%f571, %f570;
	mul.ftz.f32 	%f166, %f571, 0f3F000000;
	setp.gt.s32	%p122, %r1403, 1;
	selp.u32	%r961, 1, 0, %p122;
	setp.gt.s32	%p123, %r1403, 2;
	selp.b32	%r962, 2, 1, %p122;
	selp.b32	%r963, %r962, %r961, %p123;
	setp.gt.s32	%p124, %r1403, 4;
	selp.u32	%r964, 1, 0, %p124;
	setp.gt.s32	%p125, %r1403, 8;
	selp.u32	%r965, 1, 0, %p125;
	setp.gt.s32	%p126, %r1403, 16;
	selp.u32	%r966, 1, 0, %p126;
	setp.gt.s32	%p127, %r1403, 32;
	selp.u32	%r967, 1, 0, %p127;
	setp.gt.s32	%p128, %r1403, 64;
	selp.u32	%r968, 1, 0, %p128;
	setp.gt.s32	%p129, %r1403, 128;
	selp.u32	%r969, 1, 0, %p129;
	setp.gt.s32	%p130, %r1403, 256;
	selp.u32	%r970, 1, 0, %p130;
	setp.gt.s32	%p131, %r1403, 512;
	selp.u32	%r971, 1, 0, %p131;
	setp.gt.s32	%p132, %r1403, 1024;
	selp.u32	%r972, 1, 0, %p132;
	setp.gt.s32	%p133, %r1403, 2048;
	selp.u32	%r973, 1, 0, %p133;
	setp.gt.s32	%p134, %r1403, 4096;
	selp.u32	%r974, 1, 0, %p134;
	setp.gt.s32	%p135, %r1403, 8192;
	selp.u32	%r975, 1, 0, %p135;
	setp.gt.s32	%p136, %r1403, 16384;
	selp.u32	%r976, 1, 0, %p136;
	setp.gt.s32	%p137, %r1403, 32768;
	selp.u32	%r977, 1, 0, %p137;
	setp.gt.s32	%p138, %r1403, 65536;
	selp.u32	%r978, 1, 0, %p138;
	setp.gt.s32	%p139, %r1403, 131072;
	selp.u32	%r979, 1, 0, %p139;
	setp.gt.s32	%p140, %r1403, 262144;
	selp.u32	%r980, 1, 0, %p140;
	setp.gt.s32	%p141, %r1403, 524288;
	selp.u32	%r981, 1, 0, %p141;
	setp.gt.s32	%p142, %r1403, 1048576;
	selp.u32	%r982, 1, 0, %p142;
	setp.gt.s32	%p143, %r1403, 2097152;
	selp.u32	%r983, 1, 0, %p143;
	setp.gt.s32	%p144, %r1403, 4194304;
	selp.u32	%r984, 1, 0, %p144;
	setp.gt.s32	%p145, %r1403, 8388608;
	selp.u32	%r985, 1, 0, %p145;
	setp.gt.s32	%p146, %r1403, 16777216;
	selp.u32	%r986, 1, 0, %p146;
	setp.gt.s32	%p147, %r1403, 33554432;
	selp.u32	%r987, 1, 0, %p147;
	setp.gt.s32	%p148, %r1403, 67108864;
	selp.u32	%r988, 1, 0, %p148;
	setp.gt.s32	%p149, %r1403, 134217728;
	selp.u32	%r989, 1, 0, %p149;
	setp.gt.s32	%p150, %r1403, 268435456;
	selp.u32	%r990, 1, 0, %p150;
	setp.gt.s32	%p151, %r1403, 536870912;
	selp.u32	%r991, 1, 0, %p151;
	setp.gt.s32	%p152, %r1403, 1073741824;
	selp.u32	%r992, 1, 0, %p152;
	add.s32 	%r993, %r965, %r964;
	add.s32 	%r994, %r993, %r966;
	add.s32 	%r995, %r994, %r967;
	add.s32 	%r996, %r995, %r968;
	add.s32 	%r997, %r996, %r969;
	add.s32 	%r998, %r997, %r970;
	add.s32 	%r999, %r998, %r971;
	add.s32 	%r1000, %r999, %r972;
	add.s32 	%r1001, %r1000, %r973;
	add.s32 	%r1002, %r1001, %r974;
	add.s32 	%r1003, %r1002, %r975;
	add.s32 	%r1004, %r1003, %r976;
	add.s32 	%r1005, %r1004, %r977;
	add.s32 	%r1006, %r1005, %r978;
	add.s32 	%r1007, %r1006, %r979;
	add.s32 	%r1008, %r1007, %r980;
	add.s32 	%r1009, %r1008, %r981;
	add.s32 	%r1010, %r1009, %r982;
	add.s32 	%r1011, %r1010, %r983;
	add.s32 	%r1012, %r1011, %r984;
	add.s32 	%r1013, %r1012, %r985;
	add.s32 	%r1014, %r1013, %r986;
	add.s32 	%r1015, %r1014, %r987;
	add.s32 	%r1016, %r1015, %r988;
	add.s32 	%r1017, %r1016, %r989;
	add.s32 	%r1018, %r1017, %r990;
	add.s32 	%r1019, %r1018, %r991;
	add.s32 	%r1020, %r1019, %r992;
	add.s32 	%r1021, %r1020, %r963;
	mov.u32 	%r1022, 0;
	max.s32 	%r1416, %r1021, %r1022;
	mov.f32 	%f689, 0f00000000;
	setp.lt.s32	%p153, %r1416, 1;
	@%p153 bra 	LBB5_133;
	mov.f32 	%f687, 0f3F800000;
	mov.u32 	%r1417, %r1402;
LBB5_90:
	and.b32  	%r1023, %r1417, 1;
	shr.u32 	%r1417, %r1417, 1;
	mul.ftz.f32 	%f687, %f687, 0f3F000000;
	cvt.rn.f32.s32	%f572, %r1023;
	fma.rn.ftz.f32 	%f689, %f687, %f572, %f689;
	add.s32 	%r1416, %r1416, -1;
	setp.ne.s32	%p154, %r1416, 0;
	@%p154 bra 	LBB5_90;
	bra.uni 	LBB5_91;
LBB5_78:
	setp.lt.s32	%p116, %r1412, 8;
	@%p116 bra 	LBB5_80;
	bra.uni 	LBB5_79;
LBB5_80:
	mov.f32 	%f686, 0f00000000;
	setp.lt.s32	%p117, %r23, 1;
	@%p117 bra 	LBB5_131;
	and.b32  	%r945, %r1412, 63;
	// inline asm
	mov.b64 {_,%r937}, %rd9;
	// inline asm
	mov.u32 	%r943, 1;
	// inline asm
	call (%rd296), _rt_buffer_get_id_64, (%r937, %r943, %r943, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	ld.u64 	%rd301, [%rd296+512];
	mul.wide.u32 	%rd307, %r945, 4;
	// inline asm
	mov.b64 {_,%r941}, %rd301;
	// inline asm
	// inline asm
	call (%rd302), _rt_buffer_get_id_64, (%r941, %r943, %r943, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd308, %rd307, %rd302;
	ld.u32 	%r56, [%rd308+512];
	cvt.rn.f32.s32	%f562, %r56;
	rcp.approx.ftz.f32 	%f157, %f562;
	mov.f32 	%f683, %f157;
	mov.u32 	%r1415, %r23;
LBB5_82:
	setp.ne.s32	%p118, %r56, 0;
	selp.b32	%r946, %r56, %r1415, %p118;
	rem.s32 	%r947, %r1415, %r946;
	cvt.rn.f32.s32	%f563, %r947;
	fma.rn.ftz.f32 	%f686, %f683, %f563, %f686;
	mul.ftz.f32 	%f683, %f157, %f683;
	selp.b32	%r948, %r56, -2147483648, %p118;
	div.s32 	%r1415, %r1415, %r948;
	setp.gt.s32	%p119, %r1415, 0;
	@%p119 bra 	LBB5_82;
	bra.uni 	LBB5_83;
LBB5_79:
	shl.b32 	%r816, %r23, 4;
	add.s32 	%r817, %r816, -1556008596;
	add.s32 	%r818, %r23, -1640531527;
	xor.b32  	%r819, %r817, %r818;
	shr.u32 	%r820, %r23, 5;
	add.s32 	%r821, %r820, -939442524;
	xor.b32  	%r822, %r819, %r821;
	add.s32 	%r823, %r822, %r1412;
	shl.b32 	%r824, %r823, 4;
	add.s32 	%r825, %r824, -1383041155;
	add.s32 	%r826, %r823, -1640531527;
	xor.b32  	%r827, %r825, %r826;
	shr.u32 	%r828, %r823, 5;
	add.s32 	%r829, %r828, 2123724318;
	xor.b32  	%r830, %r827, %r829;
	add.s32 	%r831, %r830, %r23;
	shl.b32 	%r832, %r831, 4;
	add.s32 	%r833, %r832, -1556008596;
	add.s32 	%r834, %r831, 1013904242;
	xor.b32  	%r835, %r833, %r834;
	shr.u32 	%r836, %r831, 5;
	add.s32 	%r837, %r836, -939442524;
	xor.b32  	%r838, %r835, %r837;
	add.s32 	%r839, %r838, %r823;
	shl.b32 	%r840, %r839, 4;
	add.s32 	%r841, %r840, -1383041155;
	add.s32 	%r842, %r839, 1013904242;
	xor.b32  	%r843, %r841, %r842;
	shr.u32 	%r844, %r839, 5;
	add.s32 	%r845, %r844, 2123724318;
	xor.b32  	%r846, %r843, %r845;
	add.s32 	%r847, %r846, %r831;
	shl.b32 	%r848, %r847, 4;
	add.s32 	%r849, %r848, -1556008596;
	add.s32 	%r850, %r847, -626627285;
	xor.b32  	%r851, %r849, %r850;
	shr.u32 	%r852, %r847, 5;
	add.s32 	%r853, %r852, -939442524;
	xor.b32  	%r854, %r851, %r853;
	add.s32 	%r855, %r854, %r839;
	shl.b32 	%r856, %r855, 4;
	add.s32 	%r857, %r856, -1383041155;
	add.s32 	%r858, %r855, -626627285;
	xor.b32  	%r859, %r857, %r858;
	shr.u32 	%r860, %r855, 5;
	add.s32 	%r861, %r860, 2123724318;
	xor.b32  	%r862, %r859, %r861;
	add.s32 	%r863, %r862, %r847;
	shl.b32 	%r864, %r863, 4;
	add.s32 	%r865, %r864, -1556008596;
	add.s32 	%r866, %r863, 2027808484;
	xor.b32  	%r867, %r865, %r866;
	shr.u32 	%r868, %r863, 5;
	add.s32 	%r869, %r868, -939442524;
	xor.b32  	%r870, %r867, %r869;
	add.s32 	%r871, %r870, %r855;
	shl.b32 	%r872, %r871, 4;
	add.s32 	%r873, %r872, -1383041155;
	add.s32 	%r874, %r871, 2027808484;
	xor.b32  	%r875, %r873, %r874;
	shr.u32 	%r876, %r871, 5;
	add.s32 	%r877, %r876, 2123724318;
	xor.b32  	%r878, %r875, %r877;
	add.s32 	%r879, %r878, %r863;
	shl.b32 	%r880, %r879, 4;
	add.s32 	%r881, %r880, -1556008596;
	add.s32 	%r882, %r879, 387276957;
	xor.b32  	%r883, %r881, %r882;
	shr.u32 	%r884, %r879, 5;
	add.s32 	%r885, %r884, -939442524;
	xor.b32  	%r886, %r883, %r885;
	add.s32 	%r887, %r886, %r871;
	shl.b32 	%r888, %r887, 4;
	add.s32 	%r889, %r888, -1383041155;
	add.s32 	%r890, %r887, 387276957;
	xor.b32  	%r891, %r889, %r890;
	shr.u32 	%r892, %r887, 5;
	add.s32 	%r893, %r892, 2123724318;
	xor.b32  	%r894, %r891, %r893;
	add.s32 	%r895, %r894, %r879;
	shl.b32 	%r896, %r895, 4;
	add.s32 	%r897, %r896, -1556008596;
	add.s32 	%r898, %r895, -1253254570;
	xor.b32  	%r899, %r897, %r898;
	shr.u32 	%r900, %r895, 5;
	add.s32 	%r901, %r900, -939442524;
	xor.b32  	%r902, %r899, %r901;
	add.s32 	%r903, %r902, %r887;
	shl.b32 	%r904, %r903, 4;
	add.s32 	%r905, %r904, -1383041155;
	add.s32 	%r906, %r903, -1253254570;
	xor.b32  	%r907, %r905, %r906;
	shr.u32 	%r908, %r903, 5;
	add.s32 	%r909, %r908, 2123724318;
	xor.b32  	%r910, %r907, %r909;
	add.s32 	%r911, %r910, %r895;
	shl.b32 	%r912, %r911, 4;
	add.s32 	%r913, %r912, -1556008596;
	add.s32 	%r914, %r911, 1401181199;
	xor.b32  	%r915, %r913, %r914;
	shr.u32 	%r916, %r911, 5;
	add.s32 	%r917, %r916, -939442524;
	xor.b32  	%r918, %r915, %r917;
	add.s32 	%r919, %r918, %r903;
	shl.b32 	%r920, %r919, 4;
	add.s32 	%r921, %r920, -1383041155;
	add.s32 	%r922, %r919, 1401181199;
	xor.b32  	%r923, %r921, %r922;
	shr.u32 	%r924, %r919, 5;
	add.s32 	%r925, %r924, 2123724318;
	xor.b32  	%r926, %r923, %r925;
	add.s32 	%r927, %r926, %r911;
	shl.b32 	%r928, %r927, 4;
	add.s32 	%r929, %r928, 591475052;
	add.s32 	%r930, %r927, 1908133320;
	xor.b32  	%r931, %r929, %r930;
	shr.u32 	%r932, %r927, 5;
	add.s32 	%r933, %r932, 1208041124;
	xor.b32  	%r934, %r931, %r933;
	add.s32 	%r935, %r934, %r919;
	and.b32  	%r936, %r935, 2147483647;
	cvt.rn.f32.s32	%f560, %r936;
	mul.ftz.f32 	%f686, %f560, 0f30000000;
	bra.uni 	LBB5_87;
LBB5_133:
LBB5_91:
	add.ftz.f32 	%f573, %f166, %f689;
	add.ftz.f32 	%f574, %f658, %f573;
	cvt.rmi.ftz.f32.f32	%f575, %f574;
	sub.ftz.f32 	%f576, %f574, %f575;
	mov.f32 	%f577, 0f3727C5AC;
	max.ftz.f32 	%f578, %f576, %f577;
	mov.f32 	%f579, 0f3F7FFF58;
	min.ftz.f32 	%f686, %f578, %f579;
	bra.uni 	LBB5_92;
LBB5_131:
LBB5_83:
	setp.eq.s32	%p120, %r1412, 1;
	@%p120 bra 	LBB5_86;
	bra.uni 	LBB5_84;
LBB5_86:
	mul.wide.s32 	%rd309, %r22, -2032597691;
	shr.u64 	%rd310, %rd309, 32;
	cvt.u32.u64	%r949, %rd310;
	add.s32 	%r950, %r949, %r22;
	shr.u32 	%r951, %r950, 31;
	shr.s32 	%r952, %r950, 7;
	add.s32 	%r953, %r952, %r951;
	mul.lo.s32 	%r954, %r953, 243;
	sub.s32 	%r955, %r22, %r954;
	cvt.rn.f32.s32	%f564, %r955;
	neg.ftz.f32 	%f565, %f564;
	fma.rn.ftz.f32 	%f686, %f686, 0f43730000, %f565;
	bra.uni 	LBB5_87;
LBB5_84:
	setp.ne.s32	%p121, %r1412, 0;
	@%p121 bra 	LBB5_132;
	shr.s32 	%r956, %r21, 31;
	shr.u32 	%r957, %r956, 24;
	add.s32 	%r958, %r21, %r957;
	and.b32  	%r959, %r958, -256;
	sub.s32 	%r960, %r21, %r959;
	cvt.rn.f32.s32	%f566, %r960;
	neg.ftz.f32 	%f567, %f566;
	fma.rn.ftz.f32 	%f686, %f686, 0f43800000, %f567;
	bra.uni 	LBB5_87;
LBB5_132:
LBB5_87:
	add.s32 	%r1412, %r1412, 1;
LBB5_92:
	mul.ftz.f32 	%f74, %f394, %f399;
	mul.ftz.f32 	%f75, %f395, %f399;
	add.ftz.f32 	%f67, %f385, %f382;
	add.ftz.f32 	%f66, %f384, %f381;
	add.ftz.f32 	%f65, %f383, %f380;
	div.approx.ftz.f32 	%f92, %f708, %f426;
	sub.ftz.f32 	%f587, %f686, %f155;
	setp.gt.ftz.f32	%p155, %f587, 0f00000000;
	@%p155 bra 	LBB5_93;
	mul.ftz.f32 	%f328, %f322, %f322;
	mul.ftz.f32 	%f325, %f322, %f321;
	neg.ftz.f32 	%f329, %f328;
	neg.ftz.f32 	%f324, %f323;
	neg.ftz.f32 	%f326, %f325;
	fma.rn.ftz.f32 	%f330, %f321, %f323, %f329;
	fma.rn.ftz.f32 	%f327, %f323, %f324, %f326;
	mul.ftz.f32 	%f331, %f323, %f322;
	mul.ftz.f32 	%f333, %f330, %f330;
	fma.rn.ftz.f32 	%f332, %f321, %f321, %f331;
	fma.rn.ftz.f32 	%f334, %f327, %f327, %f333;
	fma.rn.ftz.f32 	%f335, %f332, %f332, %f334;
	rsqrt.approx.ftz.f32 	%f336, %f335;
	mul.ftz.f32 	%f337, %f336, %f330;
	mul.ftz.f32 	%f339, %f336, %f327;
	mul.ftz.f32 	%f402, %f337, %f367;
	mul.ftz.f32 	%f338, %f336, %f332;
	mul.ftz.f32 	%f401, %f337, %f366;
	fma.rn.ftz.f32 	%f403, %f339, %f363, %f402;
	mul.ftz.f32 	%f400, %f337, %f368;
	fma.rn.ftz.f32 	%f404, %f339, %f362, %f401;
	fma.rn.ftz.f32 	%f408, %f338, %f377, %f403;
	fma.rn.ftz.f32 	%f405, %f339, %f364, %f400;
	fma.rn.ftz.f32 	%f407, %f338, %f376, %f404;
	mul.ftz.f32 	%f409, %f408, %f408;
	fma.rn.ftz.f32 	%f406, %f338, %f378, %f405;
	fma.rn.ftz.f32 	%f410, %f407, %f407, %f409;
	fma.rn.ftz.f32 	%f411, %f406, %f406, %f410;
	rsqrt.approx.ftz.f32 	%f412, %f411;
	mul.ftz.f32 	%f88, %f406, %f412;
	mul.ftz.f32 	%f87, %f408, %f412;
	mul.ftz.f32 	%f416, %f74, %f88;
	mul.ftz.f32 	%f86, %f407, %f412;
	mul.ftz.f32 	%f413, %f79, %f87;
	neg.ftz.f32 	%f417, %f416;
	neg.ftz.f32 	%f414, %f413;
	fma.rn.ftz.f32 	%f418, %f79, %f86, %f417;
	mul.ftz.f32 	%f419, %f75, %f86;
	fma.rn.ftz.f32 	%f415, %f75, %f88, %f414;
	neg.ftz.f32 	%f420, %f419;
	mul.ftz.f32 	%f422, %f418, %f418;
	fma.rn.ftz.f32 	%f421, %f74, %f87, %f420;
	fma.rn.ftz.f32 	%f423, %f415, %f415, %f422;
	fma.rn.ftz.f32 	%f424, %f421, %f421, %f423;
	rsqrt.approx.ftz.f32 	%f425, %f424;
	mul.ftz.f32 	%f91, %f425, %f421;
	mul.ftz.f32 	%f90, %f425, %f418;
	mul.ftz.f32 	%f89, %f425, %f415;
	@%p45 bra 	LBB5_114;
	bra.uni 	LBB5_95;
LBB5_114:
	cvt.rn.f32.s32	%f606, %r1403;
	rcp.approx.ftz.f32 	%f607, %f606;
	mul.ftz.f32 	%f199, %f607, 0f3F000000;
	cvt.rn.f32.s32	%f608, %r1402;
	fma.rn.ftz.f32 	%f200, %f608, %f607, %f199;
	setp.gt.s32	%p169, %r1403, 1;
	selp.u32	%r1314, 1, 0, %p169;
	setp.gt.s32	%p170, %r1403, 2;
	selp.b32	%r1315, 2, 1, %p169;
	selp.b32	%r1316, %r1315, %r1314, %p170;
	setp.gt.s32	%p171, %r1403, 4;
	selp.u32	%r1317, 1, 0, %p171;
	setp.gt.s32	%p172, %r1403, 8;
	selp.u32	%r1318, 1, 0, %p172;
	setp.gt.s32	%p173, %r1403, 16;
	selp.u32	%r1319, 1, 0, %p173;
	setp.gt.s32	%p174, %r1403, 32;
	selp.u32	%r1320, 1, 0, %p174;
	setp.gt.s32	%p175, %r1403, 64;
	selp.u32	%r1321, 1, 0, %p175;
	setp.gt.s32	%p176, %r1403, 128;
	selp.u32	%r1322, 1, 0, %p176;
	setp.gt.s32	%p177, %r1403, 256;
	selp.u32	%r1323, 1, 0, %p177;
	setp.gt.s32	%p178, %r1403, 512;
	selp.u32	%r1324, 1, 0, %p178;
	setp.gt.s32	%p179, %r1403, 1024;
	selp.u32	%r1325, 1, 0, %p179;
	setp.gt.s32	%p180, %r1403, 2048;
	selp.u32	%r1326, 1, 0, %p180;
	setp.gt.s32	%p181, %r1403, 4096;
	selp.u32	%r1327, 1, 0, %p181;
	setp.gt.s32	%p182, %r1403, 8192;
	selp.u32	%r1328, 1, 0, %p182;
	setp.gt.s32	%p183, %r1403, 16384;
	selp.u32	%r1329, 1, 0, %p183;
	setp.gt.s32	%p184, %r1403, 32768;
	selp.u32	%r1330, 1, 0, %p184;
	setp.gt.s32	%p185, %r1403, 65536;
	selp.u32	%r1331, 1, 0, %p185;
	setp.gt.s32	%p186, %r1403, 131072;
	selp.u32	%r1332, 1, 0, %p186;
	setp.gt.s32	%p187, %r1403, 262144;
	selp.u32	%r1333, 1, 0, %p187;
	setp.gt.s32	%p188, %r1403, 524288;
	selp.u32	%r1334, 1, 0, %p188;
	setp.gt.s32	%p189, %r1403, 1048576;
	selp.u32	%r1335, 1, 0, %p189;
	setp.gt.s32	%p190, %r1403, 2097152;
	selp.u32	%r1336, 1, 0, %p190;
	setp.gt.s32	%p191, %r1403, 4194304;
	selp.u32	%r1337, 1, 0, %p191;
	setp.gt.s32	%p192, %r1403, 8388608;
	selp.u32	%r1338, 1, 0, %p192;
	setp.gt.s32	%p193, %r1403, 16777216;
	selp.u32	%r1339, 1, 0, %p193;
	setp.gt.s32	%p194, %r1403, 33554432;
	selp.u32	%r1340, 1, 0, %p194;
	setp.gt.s32	%p195, %r1403, 67108864;
	selp.u32	%r1341, 1, 0, %p195;
	setp.gt.s32	%p196, %r1403, 134217728;
	selp.u32	%r1342, 1, 0, %p196;
	setp.gt.s32	%p197, %r1403, 268435456;
	selp.u32	%r1343, 1, 0, %p197;
	setp.gt.s32	%p198, %r1403, 536870912;
	selp.u32	%r1344, 1, 0, %p198;
	setp.gt.s32	%p199, %r1403, 1073741824;
	selp.u32	%r1345, 1, 0, %p199;
	add.s32 	%r1346, %r1318, %r1317;
	add.s32 	%r1347, %r1346, %r1319;
	add.s32 	%r1348, %r1347, %r1320;
	add.s32 	%r1349, %r1348, %r1321;
	add.s32 	%r1350, %r1349, %r1322;
	add.s32 	%r1351, %r1350, %r1323;
	add.s32 	%r1352, %r1351, %r1324;
	add.s32 	%r1353, %r1352, %r1325;
	add.s32 	%r1354, %r1353, %r1326;
	add.s32 	%r1355, %r1354, %r1327;
	add.s32 	%r1356, %r1355, %r1328;
	add.s32 	%r1357, %r1356, %r1329;
	add.s32 	%r1358, %r1357, %r1330;
	add.s32 	%r1359, %r1358, %r1331;
	add.s32 	%r1360, %r1359, %r1332;
	add.s32 	%r1361, %r1360, %r1333;
	add.s32 	%r1362, %r1361, %r1334;
	add.s32 	%r1363, %r1362, %r1335;
	add.s32 	%r1364, %r1363, %r1336;
	add.s32 	%r1365, %r1364, %r1337;
	add.s32 	%r1366, %r1365, %r1338;
	add.s32 	%r1367, %r1366, %r1339;
	add.s32 	%r1368, %r1367, %r1340;
	add.s32 	%r1369, %r1368, %r1341;
	add.s32 	%r1370, %r1369, %r1342;
	add.s32 	%r1371, %r1370, %r1343;
	add.s32 	%r1372, %r1371, %r1344;
	add.s32 	%r1373, %r1372, %r1345;
	add.s32 	%r1374, %r1373, %r1316;
	mov.u32 	%r1375, 0;
	max.s32 	%r1422, %r1374, %r1375;
	mov.f32 	%f703, 0f00000000;
	setp.lt.s32	%p200, %r1422, 1;
	@%p200 bra 	LBB5_139;
	mov.f32 	%f701, 0f3F800000;
	mov.u32 	%r1423, %r1402;
LBB5_116:
	and.b32  	%r1376, %r1423, 1;
	shr.u32 	%r1423, %r1423, 1;
	mul.ftz.f32 	%f701, %f701, 0f3F000000;
	cvt.rn.f32.s32	%f609, %r1376;
	fma.rn.ftz.f32 	%f703, %f701, %f609, %f703;
	add.s32 	%r1422, %r1422, -1;
	setp.ne.s32	%p201, %r1422, 0;
	@%p201 bra 	LBB5_116;
	bra.uni 	LBB5_117;
LBB5_93:
	mov.f32 	%f706, 0f00000000;
	mov.f32 	%f707, %f706;
	mov.f32 	%f709, %f706;
	mov.f32 	%f710, %f706;
	mov.f32 	%f711, %f706;
	mov.f32 	%f712, %f706;
	bra.uni 	LBB5_119;
LBB5_95:
	setp.lt.s32	%p157, %r1412, 8;
	@%p157 bra 	LBB5_97;
	bra.uni 	LBB5_96;
LBB5_97:
	mov.f32 	%f699, 0f00000000;
	setp.lt.s32	%p158, %r23, 1;
	@%p158 bra 	LBB5_134;
	and.b32  	%r1153, %r1412, 63;
	// inline asm
	mov.b64 {_,%r1145}, %rd9;
	// inline asm
	mov.u32 	%r1151, 1;
	// inline asm
	call (%rd312), _rt_buffer_get_id_64, (%r1145, %r1151, %r1151, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	ld.u64 	%rd317, [%rd312+512];
	mul.wide.u32 	%rd323, %r1153, 4;
	// inline asm
	mov.b64 {_,%r1149}, %rd317;
	// inline asm
	// inline asm
	call (%rd318), _rt_buffer_get_id_64, (%r1149, %r1151, %r1151, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd324, %rd323, %rd318;
	ld.u32 	%r66, [%rd324+512];
	cvt.rn.f32.s32	%f590, %r66;
	rcp.approx.ftz.f32 	%f175, %f590;
	mov.f32 	%f691, %f175;
	mov.u32 	%r1419, %r23;
LBB5_99:
	setp.ne.s32	%p159, %r66, 0;
	selp.b32	%r1154, %r66, %r1419, %p159;
	rem.s32 	%r1155, %r1419, %r1154;
	cvt.rn.f32.s32	%f591, %r1155;
	fma.rn.ftz.f32 	%f699, %f691, %f591, %f699;
	mul.ftz.f32 	%f691, %f175, %f691;
	selp.b32	%r1156, %r66, -2147483648, %p159;
	div.s32 	%r1419, %r1419, %r1156;
	setp.gt.s32	%p160, %r1419, 0;
	@%p160 bra 	LBB5_99;
	bra.uni 	LBB5_100;
LBB5_96:
	shl.b32 	%r1024, %r23, 4;
	add.s32 	%r1025, %r1024, -1556008596;
	add.s32 	%r1026, %r23, -1640531527;
	xor.b32  	%r1027, %r1025, %r1026;
	shr.u32 	%r1028, %r23, 5;
	add.s32 	%r1029, %r1028, -939442524;
	xor.b32  	%r1030, %r1027, %r1029;
	add.s32 	%r1031, %r1030, %r1412;
	shl.b32 	%r1032, %r1031, 4;
	add.s32 	%r1033, %r1032, -1383041155;
	add.s32 	%r1034, %r1031, -1640531527;
	xor.b32  	%r1035, %r1033, %r1034;
	shr.u32 	%r1036, %r1031, 5;
	add.s32 	%r1037, %r1036, 2123724318;
	xor.b32  	%r1038, %r1035, %r1037;
	add.s32 	%r1039, %r1038, %r23;
	shl.b32 	%r1040, %r1039, 4;
	add.s32 	%r1041, %r1040, -1556008596;
	add.s32 	%r1042, %r1039, 1013904242;
	xor.b32  	%r1043, %r1041, %r1042;
	shr.u32 	%r1044, %r1039, 5;
	add.s32 	%r1045, %r1044, -939442524;
	xor.b32  	%r1046, %r1043, %r1045;
	add.s32 	%r1047, %r1046, %r1031;
	shl.b32 	%r1048, %r1047, 4;
	add.s32 	%r1049, %r1048, -1383041155;
	add.s32 	%r1050, %r1047, 1013904242;
	xor.b32  	%r1051, %r1049, %r1050;
	shr.u32 	%r1052, %r1047, 5;
	add.s32 	%r1053, %r1052, 2123724318;
	xor.b32  	%r1054, %r1051, %r1053;
	add.s32 	%r1055, %r1054, %r1039;
	shl.b32 	%r1056, %r1055, 4;
	add.s32 	%r1057, %r1056, -1556008596;
	add.s32 	%r1058, %r1055, -626627285;
	xor.b32  	%r1059, %r1057, %r1058;
	shr.u32 	%r1060, %r1055, 5;
	add.s32 	%r1061, %r1060, -939442524;
	xor.b32  	%r1062, %r1059, %r1061;
	add.s32 	%r1063, %r1062, %r1047;
	shl.b32 	%r1064, %r1063, 4;
	add.s32 	%r1065, %r1064, -1383041155;
	add.s32 	%r1066, %r1063, -626627285;
	xor.b32  	%r1067, %r1065, %r1066;
	shr.u32 	%r1068, %r1063, 5;
	add.s32 	%r1069, %r1068, 2123724318;
	xor.b32  	%r1070, %r1067, %r1069;
	add.s32 	%r1071, %r1070, %r1055;
	shl.b32 	%r1072, %r1071, 4;
	add.s32 	%r1073, %r1072, -1556008596;
	add.s32 	%r1074, %r1071, 2027808484;
	xor.b32  	%r1075, %r1073, %r1074;
	shr.u32 	%r1076, %r1071, 5;
	add.s32 	%r1077, %r1076, -939442524;
	xor.b32  	%r1078, %r1075, %r1077;
	add.s32 	%r1079, %r1078, %r1063;
	shl.b32 	%r1080, %r1079, 4;
	add.s32 	%r1081, %r1080, -1383041155;
	add.s32 	%r1082, %r1079, 2027808484;
	xor.b32  	%r1083, %r1081, %r1082;
	shr.u32 	%r1084, %r1079, 5;
	add.s32 	%r1085, %r1084, 2123724318;
	xor.b32  	%r1086, %r1083, %r1085;
	add.s32 	%r1087, %r1086, %r1071;
	shl.b32 	%r1088, %r1087, 4;
	add.s32 	%r1089, %r1088, -1556008596;
	add.s32 	%r1090, %r1087, 387276957;
	xor.b32  	%r1091, %r1089, %r1090;
	shr.u32 	%r1092, %r1087, 5;
	add.s32 	%r1093, %r1092, -939442524;
	xor.b32  	%r1094, %r1091, %r1093;
	add.s32 	%r1095, %r1094, %r1079;
	shl.b32 	%r1096, %r1095, 4;
	add.s32 	%r1097, %r1096, -1383041155;
	add.s32 	%r1098, %r1095, 387276957;
	xor.b32  	%r1099, %r1097, %r1098;
	shr.u32 	%r1100, %r1095, 5;
	add.s32 	%r1101, %r1100, 2123724318;
	xor.b32  	%r1102, %r1099, %r1101;
	add.s32 	%r1103, %r1102, %r1087;
	shl.b32 	%r1104, %r1103, 4;
	add.s32 	%r1105, %r1104, -1556008596;
	add.s32 	%r1106, %r1103, -1253254570;
	xor.b32  	%r1107, %r1105, %r1106;
	shr.u32 	%r1108, %r1103, 5;
	add.s32 	%r1109, %r1108, -939442524;
	xor.b32  	%r1110, %r1107, %r1109;
	add.s32 	%r1111, %r1110, %r1095;
	shl.b32 	%r1112, %r1111, 4;
	add.s32 	%r1113, %r1112, -1383041155;
	add.s32 	%r1114, %r1111, -1253254570;
	xor.b32  	%r1115, %r1113, %r1114;
	shr.u32 	%r1116, %r1111, 5;
	add.s32 	%r1117, %r1116, 2123724318;
	xor.b32  	%r1118, %r1115, %r1117;
	add.s32 	%r1119, %r1118, %r1103;
	shl.b32 	%r1120, %r1119, 4;
	add.s32 	%r1121, %r1120, -1556008596;
	add.s32 	%r1122, %r1119, 1401181199;
	xor.b32  	%r1123, %r1121, %r1122;
	shr.u32 	%r1124, %r1119, 5;
	add.s32 	%r1125, %r1124, -939442524;
	xor.b32  	%r1126, %r1123, %r1125;
	add.s32 	%r1127, %r1126, %r1111;
	shl.b32 	%r1128, %r1127, 4;
	add.s32 	%r1129, %r1128, -1383041155;
	add.s32 	%r1130, %r1127, 1401181199;
	xor.b32  	%r1131, %r1129, %r1130;
	shr.u32 	%r1132, %r1127, 5;
	add.s32 	%r1133, %r1132, 2123724318;
	xor.b32  	%r1134, %r1131, %r1133;
	add.s32 	%r1135, %r1134, %r1119;
	shl.b32 	%r1136, %r1135, 4;
	add.s32 	%r1137, %r1136, 591475052;
	add.s32 	%r1138, %r1135, 1908133320;
	xor.b32  	%r1139, %r1137, %r1138;
	shr.u32 	%r1140, %r1135, 5;
	add.s32 	%r1141, %r1140, 1208041124;
	xor.b32  	%r1142, %r1139, %r1141;
	add.s32 	%r1143, %r1142, %r1127;
	and.b32  	%r1144, %r1143, 2147483647;
	cvt.rn.f32.s32	%f588, %r1144;
	mul.ftz.f32 	%f699, %f588, 0f30000000;
	bra.uni 	LBB5_104;
LBB5_139:
LBB5_117:
	add.ftz.f32 	%f610, %f199, %f703;
	add.ftz.f32 	%f611, %f659, %f610;
	add.ftz.f32 	%f612, %f658, %f200;
	cvt.rmi.ftz.f32.f32	%f613, %f612;
	cvt.rmi.ftz.f32.f32	%f614, %f611;
	sub.ftz.f32 	%f615, %f611, %f614;
	sub.ftz.f32 	%f616, %f612, %f613;
	mov.f32 	%f617, 0f3727C5AC;
	max.ftz.f32 	%f618, %f616, %f617;
	mov.f32 	%f619, 0f3F7FFF58;
	min.ftz.f32 	%f699, %f618, %f619;
	max.ftz.f32 	%f620, %f615, %f617;
	min.ftz.f32 	%f700, %f620, %f619;
	bra.uni 	LBB5_118;
LBB5_134:
LBB5_100:
	setp.eq.s32	%p161, %r1412, 1;
	@%p161 bra 	LBB5_103;
	bra.uni 	LBB5_101;
LBB5_103:
	mul.wide.s32 	%rd325, %r22, -2032597691;
	shr.u64 	%rd326, %rd325, 32;
	cvt.u32.u64	%r1157, %rd326;
	add.s32 	%r1158, %r1157, %r22;
	shr.u32 	%r1159, %r1158, 31;
	shr.s32 	%r1160, %r1158, 7;
	add.s32 	%r1161, %r1160, %r1159;
	mul.lo.s32 	%r1162, %r1161, 243;
	sub.s32 	%r1163, %r22, %r1162;
	cvt.rn.f32.s32	%f592, %r1163;
	neg.ftz.f32 	%f593, %f592;
	fma.rn.ftz.f32 	%f699, %f699, 0f43730000, %f593;
	add.s32 	%r1420, %r1412, 1;
	bra.uni 	LBB5_106;
LBB5_101:
	setp.ne.s32	%p162, %r1412, 0;
	@%p162 bra 	LBB5_135;
	shr.s32 	%r1164, %r21, 31;
	shr.u32 	%r1165, %r1164, 24;
	add.s32 	%r1166, %r21, %r1165;
	and.b32  	%r1167, %r1166, -256;
	sub.s32 	%r1168, %r21, %r1167;
	cvt.rn.f32.s32	%f594, %r1168;
	neg.ftz.f32 	%f595, %f594;
	fma.rn.ftz.f32 	%f699, %f699, 0f43800000, %f595;
	add.s32 	%r1420, %r1412, 1;
	bra.uni 	LBB5_106;
LBB5_135:
LBB5_104:
	add.s32 	%r1420, %r1412, 1;
	setp.lt.s32	%p163, %r1420, 8;
	@%p163 bra 	LBB5_136;
	bra.uni 	LBB5_105;
LBB5_136:
LBB5_106:
	mov.f32 	%f700, 0f00000000;
	setp.lt.s32	%p164, %r23, 1;
	@%p164 bra 	LBB5_137;
	and.b32  	%r1298, %r1420, 63;
	// inline asm
	mov.b64 {_,%r1290}, %rd9;
	// inline asm
	mov.u32 	%r1296, 1;
	// inline asm
	call (%rd328), _rt_buffer_get_id_64, (%r1290, %r1296, %r1296, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	ld.u64 	%rd333, [%rd328+512];
	mul.wide.u32 	%rd339, %r1298, 4;
	// inline asm
	mov.b64 {_,%r1294}, %rd333;
	// inline asm
	// inline asm
	call (%rd334), _rt_buffer_get_id_64, (%r1294, %r1296, %r1296, %rd337, %rd337, %rd337, %rd337);
	// inline asm
	add.s64 	%rd340, %rd339, %rd334;
	ld.u32 	%r72, [%rd340+512];
	cvt.rn.f32.s32	%f598, %r72;
	rcp.approx.ftz.f32 	%f187, %f598;
	mov.f32 	%f696, %f187;
	mov.u32 	%r1421, %r23;
LBB5_108:
	setp.ne.s32	%p165, %r72, 0;
	selp.b32	%r1299, %r72, %r1421, %p165;
	rem.s32 	%r1300, %r1421, %r1299;
	cvt.rn.f32.s32	%f599, %r1300;
	fma.rn.ftz.f32 	%f700, %f696, %f599, %f700;
	mul.ftz.f32 	%f696, %f187, %f696;
	selp.b32	%r1301, %r72, -2147483648, %p165;
	div.s32 	%r1421, %r1421, %r1301;
	setp.gt.s32	%p166, %r1421, 0;
	@%p166 bra 	LBB5_108;
	bra.uni 	LBB5_109;
LBB5_105:
	shl.b32 	%r1169, %r23, 4;
	add.s32 	%r1170, %r1169, -1556008596;
	add.s32 	%r1171, %r23, -1640531527;
	xor.b32  	%r1172, %r1170, %r1171;
	shr.u32 	%r1173, %r23, 5;
	add.s32 	%r1174, %r1173, -939442524;
	xor.b32  	%r1175, %r1172, %r1174;
	add.s32 	%r1176, %r1175, %r1420;
	shl.b32 	%r1177, %r1176, 4;
	add.s32 	%r1178, %r1177, -1383041155;
	add.s32 	%r1179, %r1176, -1640531527;
	xor.b32  	%r1180, %r1178, %r1179;
	shr.u32 	%r1181, %r1176, 5;
	add.s32 	%r1182, %r1181, 2123724318;
	xor.b32  	%r1183, %r1180, %r1182;
	add.s32 	%r1184, %r1183, %r23;
	shl.b32 	%r1185, %r1184, 4;
	add.s32 	%r1186, %r1185, -1556008596;
	add.s32 	%r1187, %r1184, 1013904242;
	xor.b32  	%r1188, %r1186, %r1187;
	shr.u32 	%r1189, %r1184, 5;
	add.s32 	%r1190, %r1189, -939442524;
	xor.b32  	%r1191, %r1188, %r1190;
	add.s32 	%r1192, %r1191, %r1176;
	shl.b32 	%r1193, %r1192, 4;
	add.s32 	%r1194, %r1193, -1383041155;
	add.s32 	%r1195, %r1192, 1013904242;
	xor.b32  	%r1196, %r1194, %r1195;
	shr.u32 	%r1197, %r1192, 5;
	add.s32 	%r1198, %r1197, 2123724318;
	xor.b32  	%r1199, %r1196, %r1198;
	add.s32 	%r1200, %r1199, %r1184;
	shl.b32 	%r1201, %r1200, 4;
	add.s32 	%r1202, %r1201, -1556008596;
	add.s32 	%r1203, %r1200, -626627285;
	xor.b32  	%r1204, %r1202, %r1203;
	shr.u32 	%r1205, %r1200, 5;
	add.s32 	%r1206, %r1205, -939442524;
	xor.b32  	%r1207, %r1204, %r1206;
	add.s32 	%r1208, %r1207, %r1192;
	shl.b32 	%r1209, %r1208, 4;
	add.s32 	%r1210, %r1209, -1383041155;
	add.s32 	%r1211, %r1208, -626627285;
	xor.b32  	%r1212, %r1210, %r1211;
	shr.u32 	%r1213, %r1208, 5;
	add.s32 	%r1214, %r1213, 2123724318;
	xor.b32  	%r1215, %r1212, %r1214;
	add.s32 	%r1216, %r1215, %r1200;
	shl.b32 	%r1217, %r1216, 4;
	add.s32 	%r1218, %r1217, -1556008596;
	add.s32 	%r1219, %r1216, 2027808484;
	xor.b32  	%r1220, %r1218, %r1219;
	shr.u32 	%r1221, %r1216, 5;
	add.s32 	%r1222, %r1221, -939442524;
	xor.b32  	%r1223, %r1220, %r1222;
	add.s32 	%r1224, %r1223, %r1208;
	shl.b32 	%r1225, %r1224, 4;
	add.s32 	%r1226, %r1225, -1383041155;
	add.s32 	%r1227, %r1224, 2027808484;
	xor.b32  	%r1228, %r1226, %r1227;
	shr.u32 	%r1229, %r1224, 5;
	add.s32 	%r1230, %r1229, 2123724318;
	xor.b32  	%r1231, %r1228, %r1230;
	add.s32 	%r1232, %r1231, %r1216;
	shl.b32 	%r1233, %r1232, 4;
	add.s32 	%r1234, %r1233, -1556008596;
	add.s32 	%r1235, %r1232, 387276957;
	xor.b32  	%r1236, %r1234, %r1235;
	shr.u32 	%r1237, %r1232, 5;
	add.s32 	%r1238, %r1237, -939442524;
	xor.b32  	%r1239, %r1236, %r1238;
	add.s32 	%r1240, %r1239, %r1224;
	shl.b32 	%r1241, %r1240, 4;
	add.s32 	%r1242, %r1241, -1383041155;
	add.s32 	%r1243, %r1240, 387276957;
	xor.b32  	%r1244, %r1242, %r1243;
	shr.u32 	%r1245, %r1240, 5;
	add.s32 	%r1246, %r1245, 2123724318;
	xor.b32  	%r1247, %r1244, %r1246;
	add.s32 	%r1248, %r1247, %r1232;
	shl.b32 	%r1249, %r1248, 4;
	add.s32 	%r1250, %r1249, -1556008596;
	add.s32 	%r1251, %r1248, -1253254570;
	xor.b32  	%r1252, %r1250, %r1251;
	shr.u32 	%r1253, %r1248, 5;
	add.s32 	%r1254, %r1253, -939442524;
	xor.b32  	%r1255, %r1252, %r1254;
	add.s32 	%r1256, %r1255, %r1240;
	shl.b32 	%r1257, %r1256, 4;
	add.s32 	%r1258, %r1257, -1383041155;
	add.s32 	%r1259, %r1256, -1253254570;
	xor.b32  	%r1260, %r1258, %r1259;
	shr.u32 	%r1261, %r1256, 5;
	add.s32 	%r1262, %r1261, 2123724318;
	xor.b32  	%r1263, %r1260, %r1262;
	add.s32 	%r1264, %r1263, %r1248;
	shl.b32 	%r1265, %r1264, 4;
	add.s32 	%r1266, %r1265, -1556008596;
	add.s32 	%r1267, %r1264, 1401181199;
	xor.b32  	%r1268, %r1266, %r1267;
	shr.u32 	%r1269, %r1264, 5;
	add.s32 	%r1270, %r1269, -939442524;
	xor.b32  	%r1271, %r1268, %r1270;
	add.s32 	%r1272, %r1271, %r1256;
	shl.b32 	%r1273, %r1272, 4;
	add.s32 	%r1274, %r1273, -1383041155;
	add.s32 	%r1275, %r1272, 1401181199;
	xor.b32  	%r1276, %r1274, %r1275;
	shr.u32 	%r1277, %r1272, 5;
	add.s32 	%r1278, %r1277, 2123724318;
	xor.b32  	%r1279, %r1276, %r1278;
	add.s32 	%r1280, %r1279, %r1264;
	shl.b32 	%r1281, %r1280, 4;
	add.s32 	%r1282, %r1281, 591475052;
	add.s32 	%r1283, %r1280, 1908133320;
	xor.b32  	%r1284, %r1282, %r1283;
	shr.u32 	%r1285, %r1280, 5;
	add.s32 	%r1286, %r1285, 1208041124;
	xor.b32  	%r1287, %r1284, %r1286;
	add.s32 	%r1288, %r1287, %r1272;
	and.b32  	%r1289, %r1288, 2147483647;
	cvt.rn.f32.s32	%f596, %r1289;
	mul.ftz.f32 	%f700, %f596, 0f30000000;
	bra.uni 	LBB5_113;
LBB5_137:
LBB5_109:
	setp.ne.s32	%p167, %r1420, 0;
	@%p167 bra 	LBB5_111;
	bra.uni 	LBB5_110;
LBB5_111:
	setp.ne.s32	%p168, %r1412, 0;
	@%p168 bra 	LBB5_138;
	bra.uni 	LBB5_112;
LBB5_138:
	bra.uni 	LBB5_113;
LBB5_110:
	shr.s32 	%r1309, %r21, 31;
	shr.u32 	%r1310, %r1309, 24;
	add.s32 	%r1311, %r21, %r1310;
	and.b32  	%r1312, %r1311, -256;
	sub.s32 	%r1313, %r21, %r1312;
	cvt.rn.f32.s32	%f602, %r1313;
	neg.ftz.f32 	%f603, %f602;
	fma.rn.ftz.f32 	%f700, %f700, 0f43800000, %f603;
	bra.uni 	LBB5_113;
LBB5_112:
	mul.wide.s32 	%rd341, %r22, -2032597691;
	shr.u64 	%rd342, %rd341, 32;
	cvt.u32.u64	%r1302, %rd342;
	add.s32 	%r1303, %r1302, %r22;
	shr.u32 	%r1304, %r1303, 31;
	shr.s32 	%r1305, %r1303, 7;
	add.s32 	%r1306, %r1305, %r1304;
	mul.lo.s32 	%r1307, %r1306, 243;
	sub.s32 	%r1308, %r22, %r1307;
	cvt.rn.f32.s32	%f600, %r1308;
	neg.ftz.f32 	%f601, %f600;
	fma.rn.ftz.f32 	%f700, %f700, 0f43730000, %f601;
LBB5_113:
	add.s32 	%r1412, %r1412, 2;
LBB5_118:
	mov.f32 	%f621, 0f3F800000;
	sub.ftz.f32 	%f622, %f621, %f700;
	sqrt.approx.ftz.f32 	%f623, %f622;
	sqrt.approx.ftz.f32 	%f624, %f700;
	mul.ftz.f32 	%f625, %f699, 0f40C90FDB;
	cos.approx.ftz.f32 	%f626, %f625;
	sin.approx.ftz.f32 	%f627, %f625;
	mul.ftz.f32 	%f628, %f623, %f626;
	mul.ftz.f32 	%f629, %f623, %f627;
	mul.ftz.f32 	%f630, %f624, 0f3EA2F983;
	mul.ftz.f32 	%f710, %f677, %f682;
	mul.ftz.f32 	%f711, %f678, %f682;
	mul.ftz.f32 	%f712, %f679, %f682;
	mul.ftz.f32 	%f631, %f91, %f629;
	mul.ftz.f32 	%f632, %f90, %f629;
	mul.ftz.f32 	%f633, %f89, %f629;
	fma.rn.ftz.f32 	%f634, %f86, %f628, %f633;
	fma.rn.ftz.f32 	%f635, %f87, %f628, %f632;
	fma.rn.ftz.f32 	%f636, %f88, %f628, %f631;
	fma.rn.ftz.f32 	%f708, %f79, %f624, %f636;
	fma.rn.ftz.f32 	%f707, %f75, %f624, %f635;
	fma.rn.ftz.f32 	%f706, %f74, %f624, %f634;
	mul.ftz.f32 	%f709, %f155, %f630;
LBB5_119:
	mov.b32 	 %r1377, %f706;
	cvt.u64.u32	%rd343, %r1377;
	mov.b32 	 %r1378, %f707;
	cvt.u64.u32	%rd344, %r1378;
	shl.b64 	%rd345, %rd344, 32;
	or.b64  	%rd346, %rd343, %rd345;
	mul.ftz.f32 	%f637, %f75, %f707;
	fma.rn.ftz.f32 	%f638, %f74, %f706, %f637;
	fma.rn.ftz.f32 	%f639, %f79, %f708, %f638;
	setp.lt.ftz.f32	%p202, %f639, 0f00000000;
	neg.ftz.f32 	%f640, %f639;
	selp.f32	%f641, %f640, %f639, %p202;
	mul.ftz.f32 	%f642, %f710, %f641;
	mov.b32 	 %r1379, %f642;
	cvt.u64.u32	%rd347, %r1379;
	mul.ftz.f32 	%f643, %f711, %f641;
	mov.b32 	 %r1380, %f643;
	cvt.u64.u32	%rd348, %r1380;
	shl.b64 	%rd349, %rd348, 32;
	or.b64  	%rd350, %rd347, %rd349;
	mul.ftz.f32 	%f644, %f712, %f641;
	mul.ftz.f32 	%f645, %f92, %f709;
	mov.b32 	 %r1381, %f434;
	cvt.u64.u32	%rd351, %r1381;
	shl.b64 	%rd353, %rd351, 32;
	or.b64  	%rd354, %rd351, %rd353;
	mov.b32 	 %r1385, %f65;
	cvt.u64.u32	%rd359, %r1385;
	mov.b32 	 %r1386, %f66;
	cvt.u64.u32	%rd360, %r1386;
	shl.b64 	%rd361, %rd360, 32;
	or.b64  	%rd362, %rd359, %rd361;
	mov.b32 	 %r1391, %f74;
	cvt.u64.u32	%rd371, %r1391;
	mov.b32 	 %r1392, %f75;
	cvt.u64.u32	%rd372, %r1392;
	shl.b64 	%rd373, %rd372, 32;
	or.b64  	%rd374, %rd371, %rd373;
	st.v2.f32 	[%rd8+216], {%f658, %f659};
	st.v2.u32 	[%rd8+200], {%r21, %r22};
	st.v2.f32 	[%rd8+184], {%f25, %f26};
	mov.u32 	%r1393, 0;
	st.u32 	[%rd8+104], %r1393;
	st.u64 	[%rd8+96], %rd337;
	st.u32 	[%rd8+88], %r1393;
	st.u64 	[%rd8+80], %rd337;
	st.u32 	[%rd8+212], %r1403;
	st.u32 	[%rd8+208], %r1402;
	st.u32 	[%rd8+196], %r23;
	st.u32 	[%rd8+192], %r1412;
	st.u32 	[%rd8+176], %r24;
	st.u32 	[%rd8+168], %r1393;
	st.u32 	[%rd8+164], %r1393;
	st.f32 	[%rd8+160], %f645;
	st.f32 	[%rd8+120], %f79;
	st.f32 	[%rd8+72], %f708;
	st.f32 	[%rd8+24], %f67;
	st.f32 	[%rd8+136], %f434;
	st.u64 	[%rd8+112], %rd374;
	st.u64 	[%rd8+64], %rd346;
	st.f32 	[%rd8+40], %f434;
	st.u64 	[%rd8+16], %rd362;
	st.f32 	[%rd8+8], %f644;
	st.f32 	[%rd8+152], %f434;
	st.u64 	[%rd8+128], %rd354;
	st.f32 	[%rd8+56], %f434;
	st.u64 	[%rd8+32], %rd354;
	st.u64 	[%rd8+144], %rd354;
	st.u64 	[%rd8+48], %rd354;
	st.u64 	[%rd8], %rd350;
	ret;
}

	// .globl	stlr_power
.visible .func stlr_power(
	.param .b64 stlr_power_param_0,
	.param .b64 stlr_power_param_1,
	.param .b64 stlr_power_param_2,
	.param .b64 stlr_power_param_3
)
{
	.local .align 8 .b8 	__local_depot6[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<14>;
	.reg .f32 	%f<125>;
	.reg .s32 	%r<75>;
	.reg .s64 	%rd<104>;

	mov.u64 	%rd103, __local_depot6;
	cvta.local.u64 	%SP, %rd103;
	ld.param.u64 	%rd4, [stlr_power_param_1];
	ld.param.u64 	%rd2, [stlr_power_param_0];
	ld.param.u64 	%rd16, [stlr_power_param_2];
	ld.u32 	%r15, [%rd16];
	// inline asm
	mov.b64 {_,%r5}, %rd4;
	// inline asm
	mov.u32 	%r74, 1;
	mov.u64 	%rd14, 0;
	// inline asm
	call (%rd5), _rt_buffer_get_id_64, (%r5, %r74, %r74, %rd14, %rd14, %rd14, %rd14);
	// inline asm
	ld.u64 	%rd10, [%rd5+560];
	mul.wide.s32 	%rd17, %r15, 672;
	// inline asm
	mov.b64 {_,%r9}, %rd10;
	// inline asm
	// inline asm
	call (%rd11), _rt_buffer_get_id_64, (%r9, %r74, %r74, %rd14, %rd14, %rd14, %rd14);
	// inline asm
	add.s64 	%rd18, %rd11, %rd17;
	ld.f32 	%f124, [%rd18+920];
	ld.f32 	%f123, [%rd18+916];
	ld.f32 	%f122, [%rd18+912];
	ld.u32 	%r1, [%rd18+752];
	ld.f32 	%f4, [%rd18+756];
	ld.v2.f32 	{%f5, %f6}, [%rd18+768];
	ld.v2.f32 	{%f7, %f8}, [%rd18+760];
	ld.u32 	%r14, [%rd18+776];
	setp.eq.s32	%p1, %r14, 3;
	@%p1 bra 	LBB6_6;
	setp.eq.s32	%p2, %r14, 2;
	@%p2 bra 	LBB6_5;
	bra.uni 	LBB6_2;
LBB6_5:
	mov.u32 	%r74, 2;
	bra.uni 	LBB6_7;
LBB6_6:
	mov.u32 	%r74, 3;
	bra.uni 	LBB6_7;
LBB6_2:
	setp.ne.s32	%p3, %r14, 1;
	@%p3 bra 	LBB6_3;
	bra.uni 	LBB6_7;
LBB6_3:
	mov.u32 	%r74, 0;
LBB6_7:
	cvt.s64.s32	%rd1, %r15;
	and.b32  	%r19, %r1, 1;
	setp.eq.b32	%p4, %r19, 1;
	@!%p4 bra 	LBB6_18;
	bra.uni 	LBB6_8;
LBB6_8:
	mov.f32 	%f46, 0f3F000000;
	div.approx.ftz.f32 	%f47, %f46, %f5;
	div.approx.ftz.f32 	%f48, %f46, %f6;
	cos.approx.ftz.f32 	%f49, %f4;
	sin.approx.ftz.f32 	%f50, %f4;
	mul.ftz.f32 	%f51, %f48, %f50;
	neg.ftz.f32 	%f52, %f51;
	fma.rn.ftz.f32 	%f53, %f47, %f49, %f52;
	neg.ftz.f32 	%f54, %f8;
	fma.rn.ftz.f32 	%f55, %f47, %f50, %f54;
	fma.rn.ftz.f32 	%f42, %f48, %f49, %f55;
	sub.ftz.f32 	%f41, %f53, %f7;
	// inline asm
	mov.b64 {_,%r20}, %rd4;
	// inline asm
	mov.u32 	%r45, 1;
	// inline asm
	call (%rd20), _rt_buffer_get_id_64, (%r20, %r45, %r45, %rd14, %rd14, %rd14, %rd14);
	// inline asm
	ld.u64 	%rd25, [%rd20+560];
	mul.lo.s64 	%rd55, %rd1, 672;
	add.s64 	%rd56, %rd55, 176;
	// inline asm
	mov.b64 {_,%r24}, %rd25;
	// inline asm
	// inline asm
	call (%rd26), _rt_buffer_get_id_64, (%r24, %r45, %r45, %rd14, %rd14, %rd14, %rd14);
	// inline asm
	add.s64 	%rd57, %rd26, %rd56;
	ld.f32 	%f9, [%rd57+544];
	ld.f32 	%f10, [%rd57+548];
	ld.f32 	%f11, [%rd57+552];
	ld.u32 	%r47, [%rd57+512];
	ld.u64 	%rd58, [%rd57+520];
	ld.u64 	%rd31, [%rd20+520];
	mul.wide.s32 	%rd59, %r47, 32;
	// inline asm
	mov.b64 {_,%r28}, %rd31;
	// inline asm
	// inline asm
	call (%rd32), _rt_buffer_get_id_64, (%r28, %r45, %r45, %rd14, %rd14, %rd14, %rd14);
	// inline asm
	add.s64 	%rd60, %rd59, %rd32;
	ld.v2.u32 	{%r3, %r4}, [%rd60+528];
	st.u64 	[%SP+0], %rd58;
	cvt.u32.u64	%r32, %rd58;
	mov.u32 	%r33, 2;
	mov.u32 	%r34, 0;
	mov.f32 	%f44, 0f00000000;
	// inline asm
	call (%f37,%f38,%f39,%f40), _rt_texture_get_level_id, (%r32, %r33, %f41, %f42, %f44, %r34, %f44);
	// inline asm
	// inline asm
	mov.b64 {_,%r35}, %rd4;
	// inline asm
	// inline asm
	call (%rd38), _rt_buffer_get_id_64, (%r35, %r45, %r45, %rd14, %rd14, %rd14, %rd14);
	// inline asm
	ld.u64 	%rd43, [%rd38+520];
	// inline asm
	mov.b64 {_,%r39}, %rd43;
	// inline asm
	// inline asm
	call (%rd44), _rt_buffer_get_id_64, (%r39, %r45, %r45, %rd14, %rd14, %rd14, %rd14);
	// inline asm
	add.s64 	%rd61, %rd44, %rd59;
	ld.u32 	%r48, [%rd61+512];
	setp.lt.s32	%p5, %r48, 4;
	ld.u64 	%rd49, [%rd38+560];
	// inline asm
	mov.b64 {_,%r43}, %rd49;
	// inline asm
	// inline asm
	call (%rd50), _rt_buffer_get_id_64, (%r43, %r45, %r45, %rd14, %rd14, %rd14, %rd14);
	// inline asm
	add.s64 	%rd62, %rd56, %rd50;
	ld.u32 	%r49, [%rd62+528];
	setp.ne.s32	%p6, %r49, 1;
	mov.f32 	%f120, %f44;
	@%p6 bra 	LBB6_10;
	cvt.rn.f32.s32	%f56, %r3;
	div.approx.ftz.f32 	%f58, %f46, %f56;
	mov.f32 	%f59, 0f3F800000;
	sub.ftz.f32 	%f60, %f59, %f58;
	max.ftz.f32 	%f61, %f41, %f58;
	min.ftz.f32 	%f62, %f61, %f60;
	sub.ftz.f32 	%f63, %f41, %f62;
	abs.ftz.f32 	%f64, %f63;
	mul.ftz.f32 	%f65, %f56, %f64;
	mov.f32 	%f66, 0f00000000;
	max.ftz.f32 	%f67, %f65, %f66;
	min.ftz.f32 	%f120, %f67, %f59;
LBB6_10:
	selp.f32	%f16, %f37, %f39, %p5;
	selp.f32	%f15, %f37, %f38, %p5;
	// inline asm
	mov.b64 {_,%r50}, %rd4;
	// inline asm
	// inline asm
	call (%rd64), _rt_buffer_get_id_64, (%r50, %r45, %r45, %rd14, %rd14, %rd14, %rd14);
	// inline asm
	ld.u64 	%rd69, [%rd64+560];
	// inline asm
	mov.b64 {_,%r54}, %rd69;
	// inline asm
	// inline asm
	call (%rd70), _rt_buffer_get_id_64, (%r54, %r45, %r45, %rd14, %rd14, %rd14, %rd14);
	// inline asm
	add.s64 	%rd76, %rd55, %rd70;
	ld.u32 	%r58, [%rd76+708];
	setp.ne.s32	%p7, %r58, 1;
	mov.f32 	%f121, %f44;
	@%p7 bra 	LBB6_12;
	cvt.rn.f32.s32	%f69, %r4;
	div.approx.ftz.f32 	%f71, %f46, %f69;
	mov.f32 	%f72, 0f3F800000;
	sub.ftz.f32 	%f73, %f72, %f71;
	max.ftz.f32 	%f74, %f42, %f71;
	min.ftz.f32 	%f75, %f74, %f73;
	sub.ftz.f32 	%f76, %f42, %f75;
	abs.ftz.f32 	%f77, %f76;
	mul.ftz.f32 	%f78, %f69, %f77;
	mov.f32 	%f79, 0f00000000;
	max.ftz.f32 	%f80, %f78, %f79;
	min.ftz.f32 	%f121, %f80, %f72;
LBB6_12:
	sub.ftz.f32 	%f81, %f9, %f37;
	fma.rn.ftz.f32 	%f82, %f120, %f81, %f37;
	sub.ftz.f32 	%f83, %f10, %f15;
	fma.rn.ftz.f32 	%f84, %f120, %f83, %f15;
	sub.ftz.f32 	%f85, %f11, %f16;
	fma.rn.ftz.f32 	%f86, %f120, %f85, %f16;
	sub.ftz.f32 	%f87, %f9, %f82;
	fma.rn.ftz.f32 	%f88, %f121, %f87, %f82;
	sub.ftz.f32 	%f89, %f10, %f84;
	fma.rn.ftz.f32 	%f90, %f121, %f89, %f84;
	sub.ftz.f32 	%f91, %f11, %f86;
	fma.rn.ftz.f32 	%f92, %f121, %f91, %f86;
	mul.ftz.f32 	%f93, %f92, 0f3D93D07D;
	mov.f32 	%f94, 0f3F371437;
	fma.rn.ftz.f32 	%f95, %f94, %f90, %f93;
	mov.f32 	%f96, 0f3E59C6ED;
	fma.rn.ftz.f32 	%f97, %f96, %f88, %f95;
	sub.ftz.f32 	%f98, %f88, %f97;
	sub.ftz.f32 	%f99, %f90, %f97;
	sub.ftz.f32 	%f100, %f92, %f97;
	add.ftz.f32 	%f101, %f97, %f100;
	add.ftz.f32 	%f102, %f97, %f99;
	add.ftz.f32 	%f103, %f97, %f98;
	max.ftz.f32 	%f22, %f44, %f103;
	max.ftz.f32 	%f23, %f44, %f102;
	max.ftz.f32 	%f24, %f44, %f101;
	setp.eq.s32	%p8, %r74, 3;
	@%p8 bra 	LBB6_17;
	setp.eq.s32	%p9, %r74, 2;
	@%p9 bra 	LBB6_16;
	bra.uni 	LBB6_14;
LBB6_16:
	add.ftz.f32 	%f122, %f122, %f22;
	add.ftz.f32 	%f123, %f123, %f23;
	add.ftz.f32 	%f124, %f124, %f24;
	bra.uni 	LBB6_18;
LBB6_17:
	mul.ftz.f32 	%f122, %f122, %f22;
	mul.ftz.f32 	%f123, %f123, %f23;
	mul.ftz.f32 	%f124, %f124, %f24;
	bra.uni 	LBB6_18;
LBB6_14:
	setp.ne.s32	%p10, %r74, 1;
	@%p10 bra 	LBB6_19;
	mul.ftz.f32 	%f122, %f122, %f22;
	mul.ftz.f32 	%f123, %f123, %f23;
	mul.ftz.f32 	%f124, %f124, %f24;
	bra.uni 	LBB6_18;
LBB6_19:
	mov.f32 	%f122, %f22;
	mov.f32 	%f123, %f23;
	mov.f32 	%f124, %f24;
LBB6_18:
	mul.ftz.f32 	%f105, %f124, 0f3D93D07D;
	mov.f32 	%f106, 0f3F371437;
	fma.rn.ftz.f32 	%f107, %f106, %f123, %f105;
	mov.f32 	%f108, 0f3E59C6ED;
	fma.rn.ftz.f32 	%f109, %f108, %f122, %f107;
	// inline asm
	mov.b64 {_,%r60}, %rd4;
	// inline asm
	mov.u32 	%r69, 1;
	// inline asm
	call (%rd78), _rt_buffer_get_id_64, (%r60, %r69, %r69, %rd14, %rd14, %rd14, %rd14);
	// inline asm
	ld.u64 	%rd83, [%rd78+560];
	mul.lo.s64 	%rd95, %rd1, 672;
	// inline asm
	mov.b64 {_,%r64}, %rd83;
	// inline asm
	// inline asm
	call (%rd84), _rt_buffer_get_id_64, (%r64, %r69, %r69, %rd14, %rd14, %rd14, %rd14);
	// inline asm
	add.s64 	%rd96, %rd95, %rd84;
	ld.u32 	%r71, [%rd96+656];
	ld.u64 	%rd89, [%rd78+544];
	mul.wide.s32 	%rd97, %r71, 64;
	// inline asm
	mov.b64 {_,%r68}, %rd89;
	// inline asm
	// inline asm
	call (%rd90), _rt_buffer_get_id_64, (%r68, %r69, %r69, %rd14, %rd14, %rd14, %rd14);
	// inline asm
	add.s64 	%rd98, %rd97, %rd90;
	ld.f32 	%f110, [%rd98+512];
	setp.gt.ftz.f32	%p11, %f109, 0f00000000;
	setp.lt.ftz.f32	%p12, %f109, 0f7F800000;
	and.pred  	%p13, %p11, %p12;
	mul.ftz.f32 	%f111, %f122, 0f40490FDB;
	mul.ftz.f32 	%f112, %f123, 0f40490FDB;
	mul.ftz.f32 	%f113, %f124, 0f40490FDB;
	mul.ftz.f32 	%f114, %f113, %f110;
	mul.ftz.f32 	%f115, %f112, %f110;
	mul.ftz.f32 	%f116, %f111, %f110;
	selp.f32	%f117, %f116, 0f00000000, %p13;
	mov.b32 	 %r72, %f117;
	cvt.u64.u32	%rd99, %r72;
	selp.f32	%f118, %f115, 0f00000000, %p13;
	mov.b32 	 %r73, %f118;
	cvt.u64.u32	%rd100, %r73;
	shl.b64 	%rd101, %rd100, 32;
	or.b64  	%rd102, %rd99, %rd101;
	selp.f32	%f119, %f114, 0f00000000, %p13;
	st.f32 	[%rd2+8], %f119;
	st.u64 	[%rd2], %rd102;
	ret;
}


