//
// Generated by LWPU LWVM Compiler
// Compiler built on Tue Apr 29 16:11:09 2014 (1398805869)
// Driver 
//

.version 3.0
.target sm_30, texmode_independent
.address_size 32

.const .align 1 .b8 $str[31] = {99, 97, 110, 110, 111, 116, 32, 114, 101, 97, 100, 32, 95, 95, 119, 114, 105, 116, 101, 95, 111, 110, 108, 121, 32, 105, 109, 97, 103, 101, 0};
.visible .global .samplerref imgSamplerInt2 = { addr_mode_0 = wrap, addr_mode_1 = wrap, addr_mode_2 = wrap, filter_mode = nearest, force_unnormalized_coords = 1 };
.visible .shared .align 4 .b8 shr_1_loc[1536];

.entry p_s(
	.param .u32 .ptr .global .align 16 p_s_param_0,
	.param .u32 .ptr .global .align 16 p_s_param_1,
	.param .u32 .ptr .global .align 16 p_s_param_2,
	.param .texref p_s_param_3,
	.param .u32 .ptr .global .align 8 p_s_param_4,
	.param .u32 .ptr .global .align 16 p_s_param_5,
	.param .u32 .ptr .global .align 4 p_s_param_6
)
.reqntid 16, 8, 1
{
	.reg .f32 	%f<171>;
	.reg .pred 	%p<32>;
	.reg .s32 	%r<107>;


	ld.param.u32 	%r25, [p_s_param_5];
	ld.param.u32 	%r26, [p_s_param_6];
	// inline asm
	mov.u32 	%r17, %elwreg4;
	// inline asm
	// inline asm
	mov.u32 	%r18, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r19, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r20, %tid.y;
	// inline asm
	add.s32 	%r27, %r20, %r17;
	mad.lo.s32 	%r28, %r19, %r18, %r27;
	shl.b32 	%r29, %r28, 8;
	// inline asm
	mov.u32 	%r21, %elwreg3;
	// inline asm
	// inline asm
	mov.u32 	%r22, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r23, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r24, %tid.x;
	// inline asm
	add.s32 	%r30, %r29, %r21;
	mad.lo.s32 	%r31, %r23, %r22, %r30;
	add.s32 	%r5, %r31, %r24;
	shl.b32 	%r32, %r5, 5;
	add.s32 	%r6, %r25, %r32;
	ld.global.v4.f32 	{%f166, %f167, %f168, %f169}, [%r6];
	setp.ltu.ftz.f32 	%p5, %f169, 0f80000000;
	shl.b32 	%r33, %r5, 2;
	add.s32 	%r7, %r26, %r33;
	@%p5 bra 	BB0_27;

	ld.global.v4.f32 	{%f162, %f163, %f164, %f165}, [%r6+16];
	mov.f32 	%f17, 0f00000000;
	mov.u32 	%r91, 0;
	// inline asm
	tex.2d.v4.s32.f32 {%r34, %r35, %r36, %r37}, [p_s_param_3, imgSamplerInt2, {%f17, %f17}];
	// inline asm
	mov.u32 	%r39, 2;
	mov.u32 	%r98, %r37;
	mov.u32 	%r105, %r39;

BB0_2:
	mov.u32 	%r84, %r105;
	setp.lt.s32 	%p6, %r98, 0;
	mov.u32 	%r92, %r84;
	shl.b32 	%r40, %r91, 16;
	add.s32 	%r41, %r40, %r5;
	shl.b32 	%r42, %r41, 3;
	ld.param.u32 	%r90, [p_s_param_4];
	add.s32 	%r11, %r90, %r42;
	@%p6 bra 	BB0_9;

BB0_3:
	setp.gt.s32 	%p7, %r92, %r98;
	@%p7 bra 	BB0_7;

	shl.b32 	%r43, %r92, 4;
	ld.param.u32 	%r88, [p_s_param_1];
	add.s32 	%r44, %r88, %r43;
	ld.global.v4.f32 	{%f142, %f143, %f144, %f145}, [%r44];
	mul.rn.f32 	%f19, %f167, %f144;
	mul.rn.f32 	%f21, %f168, %f143;
	sub.ftz.f32 	%f22, %f19, %f21;
	mul.rn.f32 	%f24, %f168, %f142;
	mul.rn.f32 	%f25, %f166, %f144;
	sub.ftz.f32 	%f26, %f24, %f25;
	mul.rn.f32 	%f27, %f166, %f143;
	mul.rn.f32 	%f28, %f167, %f142;
	sub.ftz.f32 	%f29, %f27, %f28;
	ld.param.u32 	%r87, [p_s_param_0];
	add.s32 	%r45, %r87, %r43;
	ld.global.v4.f32 	{%f146, %f147, %f148, %f149}, [%r45];
	mul.ftz.f32 	%f32, %f26, %f147;
	fma.rn.ftz.f32 	%f33, %f22, %f146, %f32;
	fma.rn.ftz.f32 	%f35, %f29, %f148, %f33;
	rcp.approx.ftz.f32 	%f36, %f35;
	mov.f32 	%f37, 0f3F800000;
	ld.param.u32 	%r89, [p_s_param_2];
	add.s32 	%r46, %r89, %r43;
	ld.global.v4.f32 	{%f150, %f151, %f152, %f153}, [%r46];
	sub.ftz.f32 	%f154, %f162, %f150;
	sub.ftz.f32 	%f155, %f163, %f151;
	sub.ftz.f32 	%f156, %f164, %f152;
	mul.ftz.f32 	%f40, %f155, %f26;
	fma.rn.ftz.f32 	%f41, %f154, %f22, %f40;
	fma.rn.ftz.f32 	%f43, %f156, %f29, %f41;
	mul.ftz.f32 	%f44, %f43, %f36;
	mul.rn.f32 	%f45, %f155, %f148;
	mul.rn.f32 	%f46, %f156, %f147;
	sub.ftz.f32 	%f47, %f45, %f46;
	mul.rn.f32 	%f48, %f156, %f146;
	mul.rn.f32 	%f49, %f154, %f148;
	sub.ftz.f32 	%f50, %f48, %f49;
	mul.rn.f32 	%f51, %f154, %f147;
	mul.rn.f32 	%f52, %f155, %f146;
	sub.ftz.f32 	%f53, %f51, %f52;
	mul.ftz.f32 	%f54, %f167, %f50;
	fma.rn.ftz.f32 	%f55, %f166, %f47, %f54;
	fma.rn.ftz.f32 	%f56, %f168, %f53, %f55;
	mul.ftz.f32 	%f57, %f56, %f36;
	neg.f32 	%f58, %f43;
	fma.rn.ftz.f32 	%f59, %f58, %f36, %f37;
	neg.f32 	%f60, %f56;
	fma.rn.ftz.f32 	%f61, %f60, %f36, %f59;
	mul.ftz.f32 	%f62, %f143, %f50;
	fma.rn.ftz.f32 	%f63, %f142, %f47, %f62;
	fma.rn.ftz.f32 	%f64, %f144, %f53, %f63;
	mul.ftz.f32 	%f65, %f64, %f36;
	setp.ge.ftz.f32 	%p8, %f57, 0f80000000;
	setp.ge.ftz.f32 	%p9, %f44, 0f80000000;
	and.pred  	%p10, %p9, %p8;
	setp.ge.ftz.f32 	%p11, %f61, 0f80000000;
	and.pred  	%p12, %p10, %p11;
	setp.gt.ftz.f32 	%p13, %f65, 0f00000000;
	and.pred  	%p14, %p13, %p12;
	setp.le.ftz.f32 	%p15, %f65, %f165;
	and.pred  	%p16, %p14, %p15;
	@%p16 bra 	BB0_6;

	add.s32 	%r92, %r92, 1;
	bra.uni 	BB0_3;

BB0_6:
	mov.f32 	%f170, %f37;
	bra.uni 	BB0_19;

BB0_7:
	setp.eq.s32 	%p17, %r91, 0;
	@%p17 bra 	BB0_18;

BB0_8:
	add.s32 	%r91, %r91, -1;
	ld.global.v2.u32 	{%r98, %r105}, [%r11+-524288];
	bra.uni 	BB0_2;

BB0_9:
	and.b32  	%r55, %r84, 2047;
	cvt.rn.f32.s32 	%f67, %r55;
	shr.s32 	%r56, %r84, 11;
	cvt.rn.f32.s32 	%f70, %r56;
	mov.u32 	%r57, 0;
	// inline asm
	tex.2d.v4.s32.f32 {%r47, %r48, %r49, %r50}, [p_s_param_3, imgSamplerInt2, {%f67, %f70}];
	// inline asm
	mov.b32 	 %f77, %r47;
	mov.b32 	 %f78, %r48;
	mov.b32 	 %f79, %r49;
	add.s32 	%r58, %r55, 1;
	cvt.rn.f32.s32 	%f69, %r58;
	// inline asm
	tex.2d.v4.s32.f32 {%r51, %r52, %r53, %r54}, [p_s_param_3, imgSamplerInt2, {%f69, %f70}];
	// inline asm
	mov.b32 	 %f80, %r51;
	mov.b32 	 %f81, %r52;
	mov.b32 	 %f82, %r53;
	mov.u32 	%r77, %r50;
	mov.u32 	%r78, %r54;
	// inline asm
	div.approx.f32 	%f71, 1.0, %f166;
	// inline asm
	sub.ftz.f32 	%f83, %f77, %f162;
	mul.ftz.f32 	%f84, %f83, %f71;
	sub.ftz.f32 	%f85, %f80, %f162;
	mul.ftz.f32 	%f86, %f85, %f71;
	min.f32 	%f87, %f84, %f86;
	max.f32 	%f88, %f84, %f86;
	// inline asm
	div.approx.f32 	%f73, 1.0, %f167;
	// inline asm
	sub.ftz.f32 	%f89, %f78, %f163;
	mul.ftz.f32 	%f90, %f89, %f73;
	sub.ftz.f32 	%f91, %f81, %f163;
	mul.ftz.f32 	%f92, %f91, %f73;
	min.f32 	%f93, %f90, %f92;
	max.f32 	%f94, %f93, %f87;
	max.f32 	%f95, %f90, %f92;
	min.f32 	%f96, %f95, %f88;
	// inline asm
	div.approx.f32 	%f75, 1.0, %f168;
	// inline asm
	sub.ftz.f32 	%f97, %f79, %f164;
	mul.ftz.f32 	%f98, %f97, %f75;
	sub.ftz.f32 	%f99, %f82, %f164;
	mul.ftz.f32 	%f100, %f99, %f75;
	min.f32 	%f101, %f98, %f100;
	max.f32 	%f8, %f101, %f94;
	max.f32 	%f102, %f98, %f100;
	min.f32 	%f9, %f102, %f96;
	mov.f32 	%f103, 0f00000000;
	max.f32 	%f10, %f103, %f8;
	setp.ltu.ftz.f32 	%p18, %f9, 0f80000000;
	setp.ltu.ftz.f32 	%p19, %f9, %f8;
	or.pred  	%p20, %p18, %p19;
	@%p20 bra 	BB0_11;

	setp.le.ftz.f32 	%p30, %f8, %f165;
	bra.uni 	BB0_12;

BB0_11:
	mov.pred 	%p30, 0;

BB0_12:
	add.s32 	%r67, %r84, 2;
	and.b32  	%r68, %r67, 2047;
	shr.s32 	%r69, %r67, 11;
	cvt.rn.f32.s32 	%f104, %r68;
	cvt.rn.f32.s32 	%f107, %r69;
	// inline asm
	tex.2d.v4.s32.f32 {%r59, %r60, %r61, %r62}, [p_s_param_3, imgSamplerInt2, {%f104, %f107}];
	// inline asm
	mov.b32 	 %f114, %r59;
	mov.b32 	 %f115, %r60;
	mov.b32 	 %f116, %r61;
	add.s32 	%r71, %r68, 1;
	cvt.rn.f32.s32 	%f106, %r71;
	// inline asm
	tex.2d.v4.s32.f32 {%r63, %r64, %r65, %r66}, [p_s_param_3, imgSamplerInt2, {%f106, %f107}];
	// inline asm
	mov.b32 	 %f117, %r63;
	mov.b32 	 %f118, %r64;
	mov.b32 	 %f119, %r65;
	mov.u32 	%r75, %r62;
	mov.u32 	%r76, %r66;
	// inline asm
	div.approx.f32 	%f108, 1.0, %f166;
	// inline asm
	sub.ftz.f32 	%f120, %f114, %f162;
	mul.ftz.f32 	%f121, %f120, %f108;
	sub.ftz.f32 	%f122, %f117, %f162;
	mul.ftz.f32 	%f123, %f122, %f108;
	min.f32 	%f124, %f121, %f123;
	max.f32 	%f125, %f121, %f123;
	// inline asm
	div.approx.f32 	%f110, 1.0, %f167;
	// inline asm
	sub.ftz.f32 	%f126, %f115, %f163;
	mul.ftz.f32 	%f127, %f126, %f110;
	sub.ftz.f32 	%f128, %f118, %f163;
	mul.ftz.f32 	%f129, %f128, %f110;
	min.f32 	%f130, %f127, %f129;
	max.f32 	%f131, %f130, %f124;
	max.f32 	%f132, %f127, %f129;
	min.f32 	%f133, %f132, %f125;
	// inline asm
	div.approx.f32 	%f112, 1.0, %f168;
	// inline asm
	sub.ftz.f32 	%f134, %f116, %f164;
	mul.ftz.f32 	%f135, %f134, %f112;
	sub.ftz.f32 	%f136, %f119, %f164;
	mul.ftz.f32 	%f137, %f136, %f112;
	min.f32 	%f138, %f135, %f137;
	max.f32 	%f11, %f138, %f131;
	max.f32 	%f139, %f135, %f137;
	min.f32 	%f12, %f139, %f133;
	max.f32 	%f13, %f103, %f11;
	setp.ltu.ftz.f32 	%p22, %f12, 0f80000000;
	setp.ltu.ftz.f32 	%p23, %f12, %f11;
	or.pred  	%p24, %p22, %p23;
	@%p24 bra 	BB0_14;

	setp.le.ftz.f32 	%p31, %f11, %f165;
	bra.uni 	BB0_15;

BB0_14:
	mov.pred 	%p31, 0;

BB0_15:
	@%p30 bra 	BB0_20;

	mov.u32 	%r98, %r75;
	mov.u32 	%r105, %r76;
	@%p31 bra 	BB0_2;

	setp.eq.s32 	%p26, %r91, 0;
	@%p26 bra 	BB0_18;
	bra.uni 	BB0_8;

BB0_18:
	mov.f32 	%f170, %f17;

BB0_19:
	st.global.f32 	[%r7], %f170;
	ret;

BB0_20:
	mov.u32 	%r98, %r77;
	mov.u32 	%r105, %r78;
	@!%p31 bra 	BB0_2;

	setp.lt.ftz.f32 	%p27, %f10, %f13;
	@%p27 bra 	BB0_25;

	setp.lt.ftz.f32 	%p28, %f13, %f10;
	@%p28 bra 	BB0_24;

	setp.lt.ftz.f32 	%p29, %f9, %f12;
	@%p29 bra 	BB0_25;

BB0_24:
	st.global.v2.u32 	[%r11], {%r50, %r54};
	mov.u32 	%r99, %r75;
	mov.u32 	%r106, %r76;
	bra.uni 	BB0_26;

BB0_25:
	st.global.v2.u32 	[%r11], {%r62, %r66};
	mov.u32 	%r99, %r77;
	mov.u32 	%r106, %r78;

BB0_26:
	mov.u32 	%r105, %r106;
	mov.u32 	%r98, %r99;
	add.s32 	%r91, %r91, 1;
	bra.uni 	BB0_2;

BB0_27:
	mov.u32 	%r72, -1082130432;
	st.global.u32 	[%r7], %r72;
	ret;
}

.entry p_c(
	.param .u32 .ptr .global .align 16 p_c_param_0,
	.param .u32 .ptr .global .align 16 p_c_param_1,
	.param .u32 .ptr .global .align 16 p_c_param_2,
	.param .texref p_c_param_3,
	.param .u32 .ptr .global .align 8 p_c_param_4,
	.param .u32 .ptr .global .align 16 p_c_param_5,
	.param .u32 .ptr .global .align 16 p_c_param_6
)
.reqntid 16, 8, 1
{
	.reg .f32 	%f<201>;
	.reg .pred 	%p<33>;
	.reg .s32 	%r<106>;


	ld.param.u32 	%r25, [p_c_param_5];
	ld.param.u32 	%r26, [p_c_param_6];
	// inline asm
	mov.u32 	%r17, %elwreg4;
	// inline asm
	// inline asm
	mov.u32 	%r18, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r19, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r20, %tid.y;
	// inline asm
	add.s32 	%r27, %r20, %r17;
	mad.lo.s32 	%r28, %r19, %r18, %r27;
	shl.b32 	%r29, %r28, 8;
	// inline asm
	mov.u32 	%r21, %elwreg3;
	// inline asm
	// inline asm
	mov.u32 	%r22, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r23, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r24, %tid.x;
	// inline asm
	add.s32 	%r30, %r29, %r21;
	mad.lo.s32 	%r31, %r23, %r22, %r30;
	add.s32 	%r5, %r31, %r24;
	shl.b32 	%r32, %r5, 4;
	add.s32 	%r6, %r26, %r32;
	mov.f32 	%f21, 0fBF800000;
	st.global.v4.f32 	[%r6], {%f21, %f21, %f21, %f21};
	shl.b32 	%r33, %r5, 5;
	add.s32 	%r7, %r25, %r33;
	ld.global.v4.f32 	{%f179, %f180, %f181, %f182}, [%r7];
	setp.ltu.ftz.f32 	%p5, %f182, 0f80000000;
	@%p5 bra 	BB1_30;

	ld.global.v4.f32 	{%f171, %f172, %f173, %f174}, [%r7+16];
	mov.f32 	%f200, %f174;
	mov.u32 	%r39, 2;
	mov.f32 	%f26, 0fFFFFFFFF;
	st.global.v4.f32 	[%r6], {%f21, %f21, %f174, %f26};
	mov.f32 	%f24, 0f00000000;
	mov.u32 	%r90, 0;
	// inline asm
	tex.2d.v4.s32.f32 {%r34, %r35, %r36, %r37}, [p_c_param_3, imgSamplerInt2, {%f24, %f24}];
	// inline asm
	mov.u32 	%r97, %r37;
	mov.u32 	%r104, %r39;

BB1_2:
	mov.u32 	%r83, %r104;
	mov.f32 	%f190, %f200;
	mov.f32 	%f8, %f190;
	setp.lt.s32 	%p6, %r97, 0;
	mov.u32 	%r91, %r83;
	shl.b32 	%r40, %r90, 16;
	add.s32 	%r41, %r40, %r5;
	shl.b32 	%r42, %r41, 3;
	ld.param.u32 	%r89, [p_c_param_4];
	add.s32 	%r11, %r89, %r42;
	@%p6 bra 	BB1_13;

	setp.gt.s32 	%p7, %r83, %r97;
	@%p7 bra 	BB1_10;

	mov.f32 	%f197, %f8;

BB1_5:
	mov.f32 	%f187, %f197;
	mov.f32 	%f9, %f187;
	shl.b32 	%r43, %r91, 4;
	ld.param.u32 	%r87, [p_c_param_1];
	add.s32 	%r44, %r87, %r43;
	ld.global.v4.f32 	{%f151, %f152, %f153, %f154}, [%r44];
	mul.rn.f32 	%f28, %f180, %f153;
	mul.rn.f32 	%f30, %f181, %f152;
	sub.ftz.f32 	%f31, %f28, %f30;
	mul.rn.f32 	%f33, %f181, %f151;
	mul.rn.f32 	%f34, %f179, %f153;
	sub.ftz.f32 	%f35, %f33, %f34;
	mul.rn.f32 	%f36, %f179, %f152;
	mul.rn.f32 	%f37, %f180, %f151;
	sub.ftz.f32 	%f38, %f36, %f37;
	ld.param.u32 	%r86, [p_c_param_0];
	add.s32 	%r45, %r86, %r43;
	ld.global.v4.f32 	{%f155, %f156, %f157, %f158}, [%r45];
	mul.ftz.f32 	%f41, %f35, %f156;
	fma.rn.ftz.f32 	%f42, %f31, %f155, %f41;
	fma.rn.ftz.f32 	%f44, %f38, %f157, %f42;
	rcp.approx.ftz.f32 	%f45, %f44;
	mov.f32 	%f46, 0f3F800000;
	ld.param.u32 	%r88, [p_c_param_2];
	add.s32 	%r46, %r88, %r43;
	ld.global.v4.f32 	{%f159, %f160, %f161, %f162}, [%r46];
	sub.ftz.f32 	%f163, %f171, %f159;
	sub.ftz.f32 	%f164, %f172, %f160;
	sub.ftz.f32 	%f165, %f173, %f161;
	mul.ftz.f32 	%f49, %f164, %f35;
	fma.rn.ftz.f32 	%f50, %f163, %f31, %f49;
	fma.rn.ftz.f32 	%f52, %f165, %f38, %f50;
	mul.ftz.f32 	%f10, %f52, %f45;
	mul.rn.f32 	%f53, %f164, %f157;
	mul.rn.f32 	%f54, %f165, %f156;
	sub.ftz.f32 	%f55, %f53, %f54;
	mul.rn.f32 	%f56, %f165, %f155;
	mul.rn.f32 	%f57, %f163, %f157;
	sub.ftz.f32 	%f58, %f56, %f57;
	mul.rn.f32 	%f59, %f163, %f156;
	mul.rn.f32 	%f60, %f164, %f155;
	sub.ftz.f32 	%f61, %f59, %f60;
	mul.ftz.f32 	%f62, %f180, %f58;
	fma.rn.ftz.f32 	%f63, %f179, %f55, %f62;
	fma.rn.ftz.f32 	%f64, %f181, %f61, %f63;
	mul.ftz.f32 	%f11, %f64, %f45;
	neg.f32 	%f65, %f52;
	fma.rn.ftz.f32 	%f66, %f65, %f45, %f46;
	neg.f32 	%f67, %f64;
	fma.rn.ftz.f32 	%f68, %f67, %f45, %f66;
	mul.ftz.f32 	%f69, %f152, %f58;
	fma.rn.ftz.f32 	%f70, %f151, %f55, %f69;
	fma.rn.ftz.f32 	%f71, %f153, %f61, %f70;
	mul.ftz.f32 	%f12, %f71, %f45;
	setp.ge.ftz.f32 	%p8, %f11, 0f80000000;
	setp.ge.ftz.f32 	%p9, %f10, 0f80000000;
	and.pred  	%p10, %p9, %p8;
	setp.ge.ftz.f32 	%p11, %f68, 0f80000000;
	and.pred  	%p12, %p10, %p11;
	setp.gt.ftz.f32 	%p13, %f12, 0f00000000;
	and.pred  	%p14, %p13, %p12;
	setp.lt.ftz.f32 	%p15, %f12, %f9;
	and.pred  	%p16, %p14, %p15;
	@%p16 bra 	BB1_7;

	mov.f32 	%f198, %f9;
	bra.uni 	BB1_8;

BB1_7:
	mov.b32 	 %f72, %r91;
	st.global.v4.f32 	[%r6], {%f10, %f11, %f12, %f72};
	mov.f32 	%f198, %f12;

BB1_8:
	mov.f32 	%f13, %f198;
	add.s32 	%r91, %r91, 1;
	setp.le.s32 	%p17, %r91, %r97;
	mov.f32 	%f197, %f13;
	@%p17 bra 	BB1_5;

	mov.f32 	%f199, %f13;
	bra.uni 	BB1_11;

BB1_10:
	mov.f32 	%f199, %f8;

BB1_11:
	mov.f32 	%f200, %f199;
	setp.eq.s32 	%p18, %r90, 0;
	@%p18 bra 	BB1_30;

	add.s32 	%r90, %r90, -1;
	ld.global.v2.u32 	{%r97, %r104}, [%r11+-524288];
	bra.uni 	BB1_2;

BB1_13:
	and.b32  	%r55, %r83, 2047;
	cvt.rn.f32.s32 	%f73, %r55;
	shr.s32 	%r56, %r83, 11;
	cvt.rn.f32.s32 	%f76, %r56;
	mov.u32 	%r57, 0;
	// inline asm
	tex.2d.v4.s32.f32 {%r47, %r48, %r49, %r50}, [p_c_param_3, imgSamplerInt2, {%f73, %f76}];
	// inline asm
	mov.b32 	 %f83, %r47;
	mov.b32 	 %f84, %r48;
	mov.b32 	 %f85, %r49;
	add.s32 	%r58, %r55, 1;
	cvt.rn.f32.s32 	%f75, %r58;
	// inline asm
	tex.2d.v4.s32.f32 {%r51, %r52, %r53, %r54}, [p_c_param_3, imgSamplerInt2, {%f75, %f76}];
	// inline asm
	mov.b32 	 %f86, %r51;
	mov.b32 	 %f87, %r52;
	mov.b32 	 %f88, %r53;
	mov.u32 	%r76, %r50;
	mov.u32 	%r77, %r54;
	// inline asm
	div.approx.f32 	%f77, 1.0, %f179;
	// inline asm
	sub.ftz.f32 	%f89, %f83, %f171;
	mul.ftz.f32 	%f90, %f89, %f77;
	sub.ftz.f32 	%f91, %f86, %f171;
	mul.ftz.f32 	%f92, %f91, %f77;
	min.f32 	%f93, %f90, %f92;
	max.f32 	%f94, %f90, %f92;
	// inline asm
	div.approx.f32 	%f79, 1.0, %f180;
	// inline asm
	sub.ftz.f32 	%f95, %f84, %f172;
	mul.ftz.f32 	%f96, %f95, %f79;
	sub.ftz.f32 	%f97, %f87, %f172;
	mul.ftz.f32 	%f98, %f97, %f79;
	min.f32 	%f99, %f96, %f98;
	max.f32 	%f100, %f99, %f93;
	max.f32 	%f101, %f96, %f98;
	min.f32 	%f102, %f101, %f94;
	// inline asm
	div.approx.f32 	%f81, 1.0, %f181;
	// inline asm
	sub.ftz.f32 	%f103, %f85, %f173;
	mul.ftz.f32 	%f104, %f103, %f81;
	sub.ftz.f32 	%f105, %f88, %f173;
	mul.ftz.f32 	%f106, %f105, %f81;
	min.f32 	%f107, %f104, %f106;
	max.f32 	%f15, %f107, %f100;
	max.f32 	%f108, %f104, %f106;
	min.f32 	%f16, %f108, %f102;
	max.f32 	%f17, %f24, %f15;
	setp.ltu.ftz.f32 	%p19, %f16, 0f80000000;
	setp.ltu.ftz.f32 	%p20, %f16, %f15;
	or.pred  	%p21, %p19, %p20;
	@%p21 bra 	BB1_15;

	setp.le.ftz.f32 	%p31, %f15, %f8;
	bra.uni 	BB1_16;

BB1_15:
	mov.pred 	%p31, 0;

BB1_16:
	add.s32 	%r67, %r83, 2;
	and.b32  	%r68, %r67, 2047;
	shr.s32 	%r69, %r67, 11;
	cvt.rn.f32.s32 	%f110, %r68;
	cvt.rn.f32.s32 	%f113, %r69;
	// inline asm
	tex.2d.v4.s32.f32 {%r59, %r60, %r61, %r62}, [p_c_param_3, imgSamplerInt2, {%f110, %f113}];
	// inline asm
	mov.b32 	 %f120, %r59;
	mov.b32 	 %f121, %r60;
	mov.b32 	 %f122, %r61;
	add.s32 	%r71, %r68, 1;
	cvt.rn.f32.s32 	%f112, %r71;
	// inline asm
	tex.2d.v4.s32.f32 {%r63, %r64, %r65, %r66}, [p_c_param_3, imgSamplerInt2, {%f112, %f113}];
	// inline asm
	mov.b32 	 %f123, %r63;
	mov.b32 	 %f124, %r64;
	mov.b32 	 %f125, %r65;
	mov.u32 	%r74, %r62;
	mov.u32 	%r75, %r66;
	// inline asm
	div.approx.f32 	%f114, 1.0, %f179;
	// inline asm
	sub.ftz.f32 	%f126, %f120, %f171;
	mul.ftz.f32 	%f127, %f126, %f114;
	sub.ftz.f32 	%f128, %f123, %f171;
	mul.ftz.f32 	%f129, %f128, %f114;
	min.f32 	%f130, %f127, %f129;
	max.f32 	%f131, %f127, %f129;
	// inline asm
	div.approx.f32 	%f116, 1.0, %f180;
	// inline asm
	sub.ftz.f32 	%f132, %f121, %f172;
	mul.ftz.f32 	%f133, %f132, %f116;
	sub.ftz.f32 	%f134, %f124, %f172;
	mul.ftz.f32 	%f135, %f134, %f116;
	min.f32 	%f136, %f133, %f135;
	max.f32 	%f137, %f136, %f130;
	max.f32 	%f138, %f133, %f135;
	min.f32 	%f139, %f138, %f131;
	// inline asm
	div.approx.f32 	%f118, 1.0, %f181;
	// inline asm
	sub.ftz.f32 	%f140, %f122, %f173;
	mul.ftz.f32 	%f141, %f140, %f118;
	sub.ftz.f32 	%f142, %f125, %f173;
	mul.ftz.f32 	%f143, %f142, %f118;
	min.f32 	%f144, %f141, %f143;
	max.f32 	%f18, %f144, %f137;
	max.f32 	%f145, %f141, %f143;
	min.f32 	%f19, %f145, %f139;
	max.f32 	%f20, %f24, %f18;
	setp.ltu.ftz.f32 	%p23, %f19, 0f80000000;
	setp.ltu.ftz.f32 	%p24, %f19, %f18;
	or.pred  	%p25, %p23, %p24;
	@%p25 bra 	BB1_18;

	setp.le.ftz.f32 	%p32, %f18, %f8;
	bra.uni 	BB1_19;

BB1_18:
	mov.pred 	%p32, 0;

BB1_19:
	@%p31 bra 	BB1_23;

	mov.f32 	%f200, %f8;
	mov.u32 	%r97, %r74;
	mov.u32 	%r104, %r75;
	@%p32 bra 	BB1_2;

	setp.eq.s32 	%p27, %r90, 0;
	@%p27 bra 	BB1_30;

	add.s32 	%r90, %r90, -1;
	ld.global.v2.u32 	{%r97, %r104}, [%r11+-524288];
	mov.f32 	%f192, %f8;
	mov.f32 	%f200, %f192;
	bra.uni 	BB1_2;

BB1_23:
	mov.f32 	%f193, %f8;
	mov.f32 	%f200, %f193;
	mov.u32 	%r97, %r76;
	mov.u32 	%r104, %r77;
	@!%p32 bra 	BB1_2;

	setp.lt.ftz.f32 	%p28, %f17, %f20;
	@%p28 bra 	BB1_28;

	setp.lt.ftz.f32 	%p29, %f20, %f17;
	@%p29 bra 	BB1_27;

	setp.lt.ftz.f32 	%p30, %f16, %f19;
	@%p30 bra 	BB1_28;

BB1_27:
	st.global.v2.u32 	[%r11], {%r50, %r54};
	mov.u32 	%r98, %r74;
	mov.u32 	%r105, %r75;
	bra.uni 	BB1_29;

BB1_28:
	st.global.v2.u32 	[%r11], {%r62, %r66};
	mov.u32 	%r98, %r76;
	mov.u32 	%r105, %r77;

BB1_29:
	mov.u32 	%r104, %r105;
	mov.u32 	%r97, %r98;
	add.s32 	%r90, %r90, 1;
	mov.f32 	%f191, %f8;
	mov.f32 	%f200, %f191;
	bra.uni 	BB1_2;

BB1_30:
	ret;
}

.entry f_n_A(
	.param .u32 .ptr .const .align 16 f_n_A_param_0,
	.param .align 16 .b8 f_n_A_param_1[16],
	.param .u32 .ptr .global .align 16 f_n_A_param_2
)
.reqntid 16, 8, 1
{
	.reg .f32 	%f<185>;
	.reg .pred 	%p<17>;
	.reg .s32 	%r<21>;


	ld.param.u32 	%r1, [f_n_A_param_0];
	ld.param.u32 	%r11, [f_n_A_param_2];
	ld.param.v4.f32 	{%f177, %f178, %f179, %f180}, [f_n_A_param_1];
	// inline asm
	mov.u32 	%r3, %elwreg3;
	// inline asm
	// inline asm
	mov.u32 	%r4, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r5, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r6, %tid.x;
	// inline asm
	add.s32 	%r12, %r6, %r3;
	mad.lo.s32 	%r13, %r5, %r4, %r12;
	// inline asm
	mov.u32 	%r7, %elwreg4;
	// inline asm
	// inline asm
	mov.u32 	%r8, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r9, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r10, %tid.y;
	// inline asm
	add.s32 	%r14, %r10, %r7;
	mad.lo.s32 	%r15, %r9, %r8, %r14;
	shl.b32 	%r16, %r15, 8;
	add.s32 	%r17, %r16, %r13;
	cvt.rn.f32.s32 	%f9, %r13;
	add.ftz.f32 	%f11, %f9, %f177;
	cvt.rn.f32.s32 	%f12, %r15;
	add.ftz.f32 	%f14, %f12, %f178;
	ld.const.v4.f32 	{%f117, %f118, %f119, %f120}, [%r1+64];
	setp.gtu.ftz.f32 	%p1, %f11, %f117;
	setp.gtu.ftz.f32 	%p2, %f14, %f118;
	or.pred  	%p3, %p1, %p2;
	shl.b32 	%r18, %r17, 5;
	add.s32 	%r2, %r11, %r18;
	@%p3 bra 	BB2_11;

	mov.f32 	%f27, 0fBF800000;
	fma.rn.ftz.f32 	%f125, %f11, %f119, %f27;
	fma.rn.ftz.f32 	%f126, %f14, %f120, %f27;
	ld.param.u32 	%r19, [f_n_A_param_0];
	ld.const.v4.f32 	{%f141, %f142, %f143, %f144}, [%r19+16];
	ld.const.v4.f32 	{%f157, %f158, %f159, %f160}, [%r19+32];
	mul.ftz.f32 	%f161, %f157, %f126;
	mul.ftz.f32 	%f162, %f158, %f126;
	mul.ftz.f32 	%f163, %f159, %f126;
	mul.ftz.f32 	%f164, %f160, %f126;
	fma.rn.ftz.f32 	%f165, %f141, %f125, %f161;
	fma.rn.ftz.f32 	%f166, %f142, %f125, %f162;
	fma.rn.ftz.f32 	%f167, %f143, %f125, %f163;
	fma.rn.ftz.f32 	%f168, %f144, %f125, %f164;
	ld.const.v4.f32 	{%f169, %f170, %f171, %f172}, [%r19+48];
	add.ftz.f32 	%f173, %f165, %f169;
	add.ftz.f32 	%f174, %f166, %f170;
	add.ftz.f32 	%f175, %f167, %f171;
	add.ftz.f32 	%f176, %f168, %f172;
	ld.const.v4.f32 	{%f85, %f86, %f87, %f88}, [%r19+80];
	sub.ftz.f32 	%f97, %f173, %f85;
	sub.ftz.f32 	%f98, %f174, %f86;
	sub.ftz.f32 	%f99, %f175, %f87;
	sub.ftz.f32 	%f100, %f176, %f88;
	// inline asm
	abs.f32 	%f17, %f97;
	// inline asm
	// inline asm
	abs.f32 	%f19, %f98;
	// inline asm
	// inline asm
	abs.f32 	%f21, %f99;
	// inline asm
	// inline asm
	abs.f32 	%f23, %f100;
	// inline asm
	setp.nan.ftz.f32 	%p4, %f17, %f19;
	setp.nan.ftz.f32 	%p5, %f21, %f21;
	or.pred  	%p6, %p4, %p5;
	setp.nan.ftz.f32 	%p7, %f23, %f23;
	or.pred  	%p8, %p6, %p7;
	@%p8 bra 	BB2_9;

	setp.lt.ftz.f32 	%p9, %f17, %f19;
	selp.f32 	%f36, %f19, %f17, %p9;
	setp.lt.ftz.f32 	%p10, %f36, %f21;
	selp.f32 	%f37, %f21, %f36, %p10;
	setp.lt.ftz.f32 	%p11, %f37, %f23;
	selp.f32 	%f5, %f23, %f37, %p11;
	setp.eq.ftz.f32 	%p12, %f5, 0f00000000;
	@%p12 bra 	BB2_8;

	mov.f32 	%f6, 0f7F800000;
	setp.eq.ftz.f32 	%p13, %f5, 0f7F800000;
	@%p13 bra 	BB2_7;

	div.approx.ftz.f32 	%f40, %f17, %f5;
	mul.rn.f32 	%f41, %f40, %f40;
	div.approx.ftz.f32 	%f42, %f19, %f5;
	mul.rn.f32 	%f43, %f42, %f42;
	add.ftz.f32 	%f44, %f41, %f43;
	div.approx.ftz.f32 	%f45, %f21, %f5;
	mul.rn.f32 	%f46, %f45, %f45;
	add.ftz.f32 	%f47, %f44, %f46;
	div.approx.ftz.f32 	%f48, %f23, %f5;
	mul.rn.f32 	%f49, %f48, %f48;
	add.ftz.f32 	%f39, %f47, %f49;
	// inline asm
	sqrt.rn.f32 	%f38, %f39;
	// inline asm
	mul.rn.f32 	%f8, %f38, %f5;
	setp.eq.ftz.f32 	%p14, %f8, %f6;
	setp.eq.ftz.f32 	%p15, %f8, 0fFF800000;
	or.pred  	%p16, %p14, %p15;
	@%p16 bra 	BB2_6;

	div.approx.ftz.f32 	%f181, %f97, %f8;
	div.approx.ftz.f32 	%f182, %f98, %f8;
	div.approx.ftz.f32 	%f183, %f99, %f8;
	div.approx.ftz.f32 	%f184, %f100, %f8;
	bra.uni 	BB2_10;

BB2_6:
	div.approx.ftz.f32 	%f105, %f97, %f38;
	div.approx.ftz.f32 	%f106, %f98, %f38;
	div.approx.ftz.f32 	%f107, %f99, %f38;
	div.approx.ftz.f32 	%f108, %f100, %f38;
	div.approx.ftz.f32 	%f181, %f105, %f5;
	div.approx.ftz.f32 	%f182, %f106, %f5;
	div.approx.ftz.f32 	%f183, %f107, %f5;
	div.approx.ftz.f32 	%f184, %f108, %f5;
	bra.uni 	BB2_10;

BB2_7:
	div.approx.ftz.f32 	%f181, %f97, %f6;
	div.approx.ftz.f32 	%f182, %f98, %f6;
	div.approx.ftz.f32 	%f183, %f99, %f6;
	div.approx.ftz.f32 	%f184, %f100, %f6;
	bra.uni 	BB2_10;

BB2_8:
	mov.f32 	%f50, 0f00000000;
	mov.f32 	%f181, %f50;
	mov.f32 	%f182, %f50;
	mov.f32 	%f183, %f50;
	mov.f32 	%f184, %f50;
	bra.uni 	BB2_10;

BB2_9:
	mov.f32 	%f51, 0f7FFFFFFF;
	mov.f32 	%f181, %f51;
	mov.f32 	%f182, %f51;
	mov.f32 	%f183, %f51;
	mov.f32 	%f184, %f51;

BB2_10:
	st.global.v4.f32 	[%r2], {%f181, %f182, %f183, %f184};
	ld.param.u32 	%r20, [f_n_A_param_0];
	ld.const.f32 	%f55, [%r20];
	st.global.v4.f32 	[%r2+16], {%f85, %f86, %f87, %f55};
	ret;

BB2_11:
	mov.f32 	%f56, 0fBF800000;
	st.global.v4.f32 	[%r2], {%f56, %f56, %f56, %f56};
	ret;
}

.entry f_n_B(
	.param .u32 .ptr .const .align 16 f_n_B_param_0,
	.param .u32 .ptr .global .align 16 f_n_B_param_1,
	.param .u32 .ptr .global .align 16 f_n_B_param_2,
	.param .u32 .ptr .global .align 16 f_n_B_param_3,
	.param .u32 .ptr .global .align 16 f_n_B_param_4,
	.param .u32 .ptr .global .align 16 f_n_B_param_5
)
.reqntid 16, 8, 1
{
	.reg .f32 	%f<139>;
	.reg .pred 	%p<15>;
	.reg .s32 	%r<33>;


	ld.param.u32 	%r14, [f_n_B_param_4];
	ld.param.u32 	%r15, [f_n_B_param_5];
	// inline asm
	mov.u32 	%r6, %elwreg4;
	// inline asm
	// inline asm
	mov.u32 	%r7, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r8, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r9, %tid.y;
	// inline asm
	add.s32 	%r16, %r9, %r6;
	mad.lo.s32 	%r17, %r8, %r7, %r16;
	shl.b32 	%r18, %r17, 8;
	// inline asm
	mov.u32 	%r10, %elwreg3;
	// inline asm
	// inline asm
	mov.u32 	%r11, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r12, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r13, %tid.x;
	// inline asm
	add.s32 	%r19, %r18, %r10;
	mad.lo.s32 	%r20, %r12, %r11, %r19;
	add.s32 	%r21, %r20, %r13;
	shl.b32 	%r22, %r21, 4;
	add.s32 	%r23, %r15, %r22;
	ld.global.v4.f32 	{%f120, %f121, %f122, %f123}, [%r23];
	setp.ltu.ftz.f32 	%p1, %f120, 0f80000000;
	add.s32 	%r5, %r14, %r22;
	@%p1 bra 	BB3_11;

	mov.f32 	%f18, 0f3F800000;
	sub.ftz.f32 	%f19, %f18, %f120;
	sub.ftz.f32 	%f21, %f19, %f121;
	mov.b32 	 %r24, %f123;
	shl.b32 	%r25, %r24, 4;
	ld.param.u32 	%r30, [f_n_B_param_1];
	add.s32 	%r26, %r30, %r25;
	ld.param.u32 	%r31, [f_n_B_param_2];
	add.s32 	%r27, %r31, %r25;
	ld.param.u32 	%r32, [f_n_B_param_3];
	add.s32 	%r28, %r32, %r25;
	ld.global.v4.f32 	{%f124, %f125, %f126, %f127}, [%r26];
	mul.ftz.f32 	%f25, %f125, %f120;
	fma.rn.ftz.f32 	%f26, %f124, %f21, %f25;
	fma.rn.ftz.f32 	%f11, %f126, %f121, %f26;
	ld.global.v4.f32 	{%f128, %f129, %f130, %f131}, [%r27];
	mul.ftz.f32 	%f30, %f129, %f120;
	fma.rn.ftz.f32 	%f31, %f128, %f21, %f30;
	fma.rn.ftz.f32 	%f13, %f130, %f121, %f31;
	ld.global.v4.f32 	{%f132, %f133, %f134, %f135}, [%r28];
	mul.ftz.f32 	%f35, %f133, %f120;
	fma.rn.ftz.f32 	%f36, %f132, %f21, %f35;
	fma.rn.ftz.f32 	%f15, %f134, %f121, %f36;
	mov.f32 	%f17, 0f00000000;
	// inline asm
	abs.f32 	%f10, %f11;
	// inline asm
	// inline asm
	abs.f32 	%f12, %f13;
	// inline asm
	// inline asm
	abs.f32 	%f14, %f15;
	// inline asm
	// inline asm
	abs.f32 	%f16, %f17;
	// inline asm
	setp.nan.ftz.f32 	%p2, %f10, %f12;
	setp.nan.ftz.f32 	%p3, %f14, %f14;
	or.pred  	%p4, %p2, %p3;
	setp.nan.ftz.f32 	%p5, %f16, %f16;
	or.pred  	%p6, %p4, %p5;
	@%p6 bra 	BB3_9;

	setp.lt.ftz.f32 	%p7, %f10, %f12;
	selp.f32 	%f38, %f12, %f10, %p7;
	setp.lt.ftz.f32 	%p8, %f38, %f14;
	selp.f32 	%f39, %f14, %f38, %p8;
	setp.lt.ftz.f32 	%p9, %f39, %f16;
	selp.f32 	%f6, %f16, %f39, %p9;
	setp.eq.ftz.f32 	%p10, %f6, 0f00000000;
	@%p10 bra 	BB3_8;

	mov.f32 	%f7, 0f7F800000;
	setp.eq.ftz.f32 	%p11, %f6, 0f7F800000;
	@%p11 bra 	BB3_7;

	div.approx.ftz.f32 	%f42, %f10, %f6;
	mul.rn.f32 	%f43, %f42, %f42;
	div.approx.ftz.f32 	%f44, %f12, %f6;
	mul.rn.f32 	%f45, %f44, %f44;
	add.ftz.f32 	%f46, %f43, %f45;
	div.approx.ftz.f32 	%f47, %f14, %f6;
	mul.rn.f32 	%f48, %f47, %f47;
	add.ftz.f32 	%f49, %f46, %f48;
	div.approx.ftz.f32 	%f50, %f16, %f6;
	mul.rn.f32 	%f51, %f50, %f50;
	add.ftz.f32 	%f41, %f49, %f51;
	// inline asm
	sqrt.rn.f32 	%f40, %f41;
	// inline asm
	mul.rn.f32 	%f9, %f40, %f6;
	setp.eq.ftz.f32 	%p12, %f9, %f7;
	setp.eq.ftz.f32 	%p13, %f9, 0fFF800000;
	or.pred  	%p14, %p12, %p13;
	@%p14 bra 	BB3_6;

	div.approx.ftz.f32 	%f136, %f11, %f9;
	div.approx.ftz.f32 	%f137, %f13, %f9;
	div.approx.ftz.f32 	%f138, %f15, %f9;
	bra.uni 	BB3_10;

BB3_6:
	div.approx.ftz.f32 	%f108, %f11, %f40;
	div.approx.ftz.f32 	%f109, %f13, %f40;
	div.approx.ftz.f32 	%f110, %f15, %f40;
	div.approx.ftz.f32 	%f136, %f108, %f6;
	div.approx.ftz.f32 	%f137, %f109, %f6;
	div.approx.ftz.f32 	%f138, %f110, %f6;
	bra.uni 	BB3_10;

BB3_7:
	div.approx.ftz.f32 	%f136, %f11, %f7;
	div.approx.ftz.f32 	%f137, %f13, %f7;
	div.approx.ftz.f32 	%f138, %f15, %f7;
	bra.uni 	BB3_10;

BB3_8:
	mov.f32 	%f136, %f17;
	mov.f32 	%f137, %f17;
	mov.f32 	%f138, %f17;
	mov.f32 	%f79, %f17;
	bra.uni 	BB3_10;

BB3_9:
	mov.f32 	%f53, 0f7FFFFFFF;
	mov.f32 	%f136, %f53;
	mov.f32 	%f137, %f53;
	mov.f32 	%f138, %f53;
	mov.f32 	%f83, %f53;

BB3_10:
	mov.f32 	%f54, 0f3F000000;
	fma.rn.ftz.f32 	%f88, %f136, %f54, %f54;
	fma.rn.ftz.f32 	%f89, %f137, %f54, %f54;
	fma.rn.ftz.f32 	%f90, %f138, %f54, %f54;
	st.global.v4.f32 	[%r5], {%f88, %f89, %f90, %f18};
	ret;

BB3_11:
	ld.param.u32 	%r29, [f_n_B_param_0];
	ld.const.v4.f32 	{%f56, %f57, %f58, %f59}, [%r29+144];
	st.global.v4.f32 	[%r5], {%f56, %f57, %f58, %f59};
	ret;
}

.entry f_m_A(
	.param .u32 .ptr .const .align 16 f_m_A_param_0,
	.param .align 16 .b8 f_m_A_param_1[16],
	.param .u32 .ptr .global .align 16 f_m_A_param_2
)
.reqntid 16, 8, 1
{
	.reg .f32 	%f<185>;
	.reg .pred 	%p<17>;
	.reg .s32 	%r<21>;


	ld.param.u32 	%r1, [f_m_A_param_0];
	ld.param.u32 	%r11, [f_m_A_param_2];
	ld.param.v4.f32 	{%f177, %f178, %f179, %f180}, [f_m_A_param_1];
	// inline asm
	mov.u32 	%r3, %elwreg3;
	// inline asm
	// inline asm
	mov.u32 	%r4, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r5, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r6, %tid.x;
	// inline asm
	add.s32 	%r12, %r6, %r3;
	mad.lo.s32 	%r13, %r5, %r4, %r12;
	// inline asm
	mov.u32 	%r7, %elwreg4;
	// inline asm
	// inline asm
	mov.u32 	%r8, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r9, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r10, %tid.y;
	// inline asm
	add.s32 	%r14, %r10, %r7;
	mad.lo.s32 	%r15, %r9, %r8, %r14;
	shl.b32 	%r16, %r15, 8;
	add.s32 	%r17, %r16, %r13;
	cvt.rn.f32.s32 	%f9, %r13;
	add.ftz.f32 	%f11, %f9, %f177;
	cvt.rn.f32.s32 	%f12, %r15;
	add.ftz.f32 	%f14, %f12, %f178;
	ld.const.v4.f32 	{%f117, %f118, %f119, %f120}, [%r1+64];
	setp.gtu.ftz.f32 	%p1, %f11, %f117;
	setp.gtu.ftz.f32 	%p2, %f14, %f118;
	or.pred  	%p3, %p1, %p2;
	shl.b32 	%r18, %r17, 5;
	add.s32 	%r2, %r11, %r18;
	@%p3 bra 	BB4_11;

	mov.f32 	%f27, 0fBF800000;
	fma.rn.ftz.f32 	%f125, %f11, %f119, %f27;
	fma.rn.ftz.f32 	%f126, %f14, %f120, %f27;
	ld.param.u32 	%r19, [f_m_A_param_0];
	ld.const.v4.f32 	{%f141, %f142, %f143, %f144}, [%r19+16];
	ld.const.v4.f32 	{%f157, %f158, %f159, %f160}, [%r19+32];
	mul.ftz.f32 	%f161, %f157, %f126;
	mul.ftz.f32 	%f162, %f158, %f126;
	mul.ftz.f32 	%f163, %f159, %f126;
	mul.ftz.f32 	%f164, %f160, %f126;
	fma.rn.ftz.f32 	%f165, %f141, %f125, %f161;
	fma.rn.ftz.f32 	%f166, %f142, %f125, %f162;
	fma.rn.ftz.f32 	%f167, %f143, %f125, %f163;
	fma.rn.ftz.f32 	%f168, %f144, %f125, %f164;
	ld.const.v4.f32 	{%f169, %f170, %f171, %f172}, [%r19+48];
	add.ftz.f32 	%f173, %f165, %f169;
	add.ftz.f32 	%f174, %f166, %f170;
	add.ftz.f32 	%f175, %f167, %f171;
	add.ftz.f32 	%f176, %f168, %f172;
	ld.const.v4.f32 	{%f85, %f86, %f87, %f88}, [%r19+80];
	sub.ftz.f32 	%f97, %f173, %f85;
	sub.ftz.f32 	%f98, %f174, %f86;
	sub.ftz.f32 	%f99, %f175, %f87;
	sub.ftz.f32 	%f100, %f176, %f88;
	// inline asm
	abs.f32 	%f17, %f97;
	// inline asm
	// inline asm
	abs.f32 	%f19, %f98;
	// inline asm
	// inline asm
	abs.f32 	%f21, %f99;
	// inline asm
	// inline asm
	abs.f32 	%f23, %f100;
	// inline asm
	setp.nan.ftz.f32 	%p4, %f17, %f19;
	setp.nan.ftz.f32 	%p5, %f21, %f21;
	or.pred  	%p6, %p4, %p5;
	setp.nan.ftz.f32 	%p7, %f23, %f23;
	or.pred  	%p8, %p6, %p7;
	@%p8 bra 	BB4_9;

	setp.lt.ftz.f32 	%p9, %f17, %f19;
	selp.f32 	%f36, %f19, %f17, %p9;
	setp.lt.ftz.f32 	%p10, %f36, %f21;
	selp.f32 	%f37, %f21, %f36, %p10;
	setp.lt.ftz.f32 	%p11, %f37, %f23;
	selp.f32 	%f5, %f23, %f37, %p11;
	setp.eq.ftz.f32 	%p12, %f5, 0f00000000;
	@%p12 bra 	BB4_8;

	mov.f32 	%f6, 0f7F800000;
	setp.eq.ftz.f32 	%p13, %f5, 0f7F800000;
	@%p13 bra 	BB4_7;

	div.approx.ftz.f32 	%f40, %f17, %f5;
	mul.rn.f32 	%f41, %f40, %f40;
	div.approx.ftz.f32 	%f42, %f19, %f5;
	mul.rn.f32 	%f43, %f42, %f42;
	add.ftz.f32 	%f44, %f41, %f43;
	div.approx.ftz.f32 	%f45, %f21, %f5;
	mul.rn.f32 	%f46, %f45, %f45;
	add.ftz.f32 	%f47, %f44, %f46;
	div.approx.ftz.f32 	%f48, %f23, %f5;
	mul.rn.f32 	%f49, %f48, %f48;
	add.ftz.f32 	%f39, %f47, %f49;
	// inline asm
	sqrt.rn.f32 	%f38, %f39;
	// inline asm
	mul.rn.f32 	%f8, %f38, %f5;
	setp.eq.ftz.f32 	%p14, %f8, %f6;
	setp.eq.ftz.f32 	%p15, %f8, 0fFF800000;
	or.pred  	%p16, %p14, %p15;
	@%p16 bra 	BB4_6;

	div.approx.ftz.f32 	%f181, %f97, %f8;
	div.approx.ftz.f32 	%f182, %f98, %f8;
	div.approx.ftz.f32 	%f183, %f99, %f8;
	div.approx.ftz.f32 	%f184, %f100, %f8;
	bra.uni 	BB4_10;

BB4_6:
	div.approx.ftz.f32 	%f105, %f97, %f38;
	div.approx.ftz.f32 	%f106, %f98, %f38;
	div.approx.ftz.f32 	%f107, %f99, %f38;
	div.approx.ftz.f32 	%f108, %f100, %f38;
	div.approx.ftz.f32 	%f181, %f105, %f5;
	div.approx.ftz.f32 	%f182, %f106, %f5;
	div.approx.ftz.f32 	%f183, %f107, %f5;
	div.approx.ftz.f32 	%f184, %f108, %f5;
	bra.uni 	BB4_10;

BB4_7:
	div.approx.ftz.f32 	%f181, %f97, %f6;
	div.approx.ftz.f32 	%f182, %f98, %f6;
	div.approx.ftz.f32 	%f183, %f99, %f6;
	div.approx.ftz.f32 	%f184, %f100, %f6;
	bra.uni 	BB4_10;

BB4_8:
	mov.f32 	%f50, 0f00000000;
	mov.f32 	%f181, %f50;
	mov.f32 	%f182, %f50;
	mov.f32 	%f183, %f50;
	mov.f32 	%f184, %f50;
	bra.uni 	BB4_10;

BB4_9:
	mov.f32 	%f51, 0f7FFFFFFF;
	mov.f32 	%f181, %f51;
	mov.f32 	%f182, %f51;
	mov.f32 	%f183, %f51;
	mov.f32 	%f184, %f51;

BB4_10:
	st.global.v4.f32 	[%r2], {%f181, %f182, %f183, %f184};
	ld.param.u32 	%r20, [f_m_A_param_0];
	ld.const.f32 	%f55, [%r20];
	st.global.v4.f32 	[%r2+16], {%f85, %f86, %f87, %f55};
	ret;

BB4_11:
	mov.f32 	%f56, 0fBF800000;
	st.global.v4.f32 	[%r2], {%f56, %f56, %f56, %f56};
	ret;
}

.entry f_m_B(
	.param .u32 .ptr .const .align 16 f_m_B_param_0,
	.param .u32 .ptr .global .align 16 f_m_B_param_1,
	.param .u32 .ptr .global .align 16 f_m_B_param_2,
	.param .u32 .ptr .global .align 16 f_m_B_param_3,
	.param .u32 .ptr .global .align 16 f_m_B_param_4,
	.param .u32 .ptr .global .align 16 f_m_B_param_5,
	.param .u32 .ptr .global .align 16 f_m_B_param_6,
	.param .u32 .ptr .global .align 4 f_m_B_param_7,
	.param .u32 .ptr .global .align 16 f_m_B_param_8,
	.param .u32 .ptr .global .align 16 f_m_B_param_9
)
.reqntid 16, 8, 1
{
	.reg .f32 	%f<228>;
	.reg .pred 	%p<9>;
	.reg .s32 	%r<92>;


	ld.param.u32 	%r18, [f_m_B_param_8];
	ld.param.u32 	%r19, [f_m_B_param_9];
	// inline asm
	mov.u32 	%r10, %elwreg4;
	// inline asm
	// inline asm
	mov.u32 	%r11, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r12, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r13, %tid.y;
	// inline asm
	add.s32 	%r20, %r13, %r10;
	mad.lo.s32 	%r21, %r12, %r11, %r20;
	shl.b32 	%r22, %r21, 8;
	// inline asm
	mov.u32 	%r14, %elwreg3;
	// inline asm
	// inline asm
	mov.u32 	%r15, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r16, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r17, %tid.x;
	// inline asm
	add.s32 	%r23, %r22, %r14;
	mad.lo.s32 	%r24, %r16, %r15, %r23;
	add.s32 	%r25, %r24, %r17;
	shl.b32 	%r26, %r25, 4;
	add.s32 	%r27, %r19, %r26;
	ld.global.v4.f32 	{%f128, %f129, %f130, %f131}, [%r27];
	setp.ltu.ftz.f32 	%p1, %f128, 0f80000000;
	add.s32 	%r6, %r18, %r26;
	@%p1 bra 	BB5_5;

	mov.b32 	 %r7, %f131;
	shl.b32 	%r28, %r7, 4;
	ld.param.u32 	%r88, [f_m_B_param_1];
	add.s32 	%r29, %r88, %r28;
	ld.global.u32 	%r30, [%r29+12];
	ld.param.u32 	%r87, [f_m_B_param_0];
	mad.lo.s32 	%r8, %r30, 96, %r87;
	ld.const.u32 	%r9, [%r8+196];
	setp.gt.s32 	%p2, %r9, 0;
	@%p2 bra 	BB5_3;

	mov.f32 	%f3, 0f3F800000;
	mov.f32 	%f224, %f3;
	mov.f32 	%f225, %f3;
	mov.f32 	%f226, %f3;
	mov.f32 	%f227, %f3;
	bra.uni 	BB5_4;

BB5_3:
	mov.f32 	%f16, 0f3F800000;
	sub.ftz.f32 	%f17, %f16, %f128;
	sub.ftz.f32 	%f19, %f17, %f129;
	ld.param.u32 	%r89, [f_m_B_param_5];
	add.s32 	%r32, %r89, %r28;
	ld.global.v4.f32 	{%f132, %f133, %f134, %f135}, [%r32];
	mul.ftz.f32 	%f22, %f133, %f128;
	fma.rn.ftz.f32 	%f23, %f132, %f19, %f22;
	fma.rn.ftz.f32 	%f5, %f134, %f129, %f23;
	ld.param.u32 	%r90, [f_m_B_param_6];
	add.s32 	%r33, %r90, %r28;
	ld.global.v4.f32 	{%f136, %f137, %f138, %f139}, [%r33];
	mul.ftz.f32 	%f27, %f137, %f128;
	fma.rn.ftz.f32 	%f28, %f136, %f19, %f27;
	fma.rn.ftz.f32 	%f9, %f138, %f129, %f28;
	ld.const.f32 	%f30, [%r8+200];
	ld.const.f32 	%f31, [%r8+208];
	ld.const.f32 	%f32, [%r8+204];
	ld.const.f32 	%f33, [%r8+212];
	// inline asm
	cvt.rzi.f32.f32 	%f4, %f5;
	// inline asm
	setp.eq.ftz.f32 	%p3, %f5, 0f7F800000;
	setp.eq.ftz.f32 	%p4, %f5, 0fFF800000;
	or.pred  	%p5, %p3, %p4;
	sub.ftz.f32 	%f34, %f5, %f4;
	mov.b32 	 %r34, %f5;
	and.b32  	%r35, %r34, -2147483648;
	mov.b32 	 %r36, %f34;
	and.b32  	%r37, %r36, 2147483647;
	selp.b32 	%r38, 0, %r37, %p5;
	or.b32  	%r39, %r38, %r35;
	mov.b32 	 %f7, %r39;
	// inline asm
	abs.f32 	%f6, %f7;
	// inline asm
	// inline asm
	cvt.rzi.f32.f32 	%f8, %f9;
	// inline asm
	setp.eq.ftz.f32 	%p6, %f9, 0f7F800000;
	setp.eq.ftz.f32 	%p7, %f9, 0fFF800000;
	or.pred  	%p8, %p6, %p7;
	sub.ftz.f32 	%f35, %f9, %f8;
	mov.b32 	 %r40, %f9;
	and.b32  	%r41, %r40, -2147483648;
	mov.b32 	 %r42, %f35;
	and.b32  	%r43, %r42, 2147483647;
	selp.b32 	%r44, 0, %r43, %p8;
	or.b32  	%r45, %r44, %r41;
	mov.b32 	 %f11, %r45;
	// inline asm
	abs.f32 	%f10, %f11;
	// inline asm
	sub.ftz.f32 	%f36, %f16, %f6;
	sub.ftz.f32 	%f37, %f16, %f10;
	add.ftz.f32 	%f38, %f30, 0fBF800000;
	mul.ftz.f32 	%f13, %f6, %f38;
	add.ftz.f32 	%f39, %f32, 0fBF800000;
	mul.ftz.f32 	%f15, %f10, %f39;
	// inline asm
	cvt.rzi.f32.f32 	%f12, %f13;
	// inline asm
	// inline asm
	cvt.rzi.f32.f32 	%f14, %f15;
	// inline asm
	mov.f32 	%f40, 0f00000000;
	max.f32 	%f41, %f40, %f12;
	min.f32 	%f42, %f41, %f31;
	max.f32 	%f43, %f40, %f14;
	min.f32 	%f44, %f43, %f33;
	fma.rn.ftz.f32 	%f45, %f44, %f30, %f42;
	cvt.rzi.ftz.s32.f32 	%r46, %f45;
	add.s32 	%r47, %r46, %r9;
	shl.b32 	%r48, %r47, 2;
	ld.param.u32 	%r91, [f_m_B_param_7];
	add.s32 	%r49, %r91, %r48;
	ld.global.u32 	%r50, [%r49];
	shr.u32 	%r51, %r50, 16;
	and.b32  	%r52, %r51, 255;
	cvt.rn.f32.u32 	%f46, %r52;
	mul.ftz.f32 	%f47, %f46, 0f3B808081;
	shr.u32 	%r53, %r50, 8;
	and.b32  	%r54, %r53, 255;
	cvt.rn.f32.u32 	%f48, %r54;
	mul.ftz.f32 	%f49, %f48, 0f3B808081;
	and.b32  	%r55, %r50, 255;
	cvt.rn.f32.u32 	%f50, %r55;
	mul.ftz.f32 	%f51, %f50, 0f3B808081;
	add.ftz.f32 	%f55, %f12, 0f3F800000;
	max.f32 	%f56, %f40, %f55;
	min.f32 	%f57, %f56, %f31;
	fma.rn.ftz.f32 	%f58, %f44, %f30, %f57;
	cvt.rzi.ftz.s32.f32 	%r56, %f58;
	add.s32 	%r57, %r56, %r9;
	shl.b32 	%r58, %r57, 2;
	add.s32 	%r59, %r91, %r58;
	ld.global.u32 	%r60, [%r59];
	shr.u32 	%r61, %r60, 16;
	and.b32  	%r62, %r61, 255;
	cvt.rn.f32.u32 	%f59, %r62;
	mul.ftz.f32 	%f60, %f59, 0f3B808081;
	shr.u32 	%r63, %r60, 8;
	and.b32  	%r64, %r63, 255;
	cvt.rn.f32.u32 	%f61, %r64;
	mul.ftz.f32 	%f62, %f61, 0f3B808081;
	and.b32  	%r65, %r60, 255;
	cvt.rn.f32.u32 	%f63, %r65;
	mul.ftz.f32 	%f64, %f63, 0f3B808081;
	mul.ftz.f32 	%f172, %f60, %f6;
	mul.ftz.f32 	%f173, %f62, %f6;
	mul.ftz.f32 	%f174, %f64, %f6;
	mul.ftz.f32 	%f175, %f16, %f6;
	fma.rn.ftz.f32 	%f176, %f47, %f36, %f172;
	fma.rn.ftz.f32 	%f177, %f49, %f36, %f173;
	fma.rn.ftz.f32 	%f178, %f51, %f36, %f174;
	fma.rn.ftz.f32 	%f179, %f16, %f36, %f175;
	add.ftz.f32 	%f71, %f14, 0f3F800000;
	max.f32 	%f72, %f40, %f71;
	min.f32 	%f73, %f72, %f33;
	fma.rn.ftz.f32 	%f74, %f73, %f30, %f42;
	cvt.rzi.ftz.s32.f32 	%r66, %f74;
	add.s32 	%r67, %r66, %r9;
	shl.b32 	%r68, %r67, 2;
	add.s32 	%r69, %r91, %r68;
	ld.global.u32 	%r70, [%r69];
	shr.u32 	%r71, %r70, 16;
	and.b32  	%r72, %r71, 255;
	cvt.rn.f32.u32 	%f75, %r72;
	mul.ftz.f32 	%f76, %f75, 0f3B808081;
	shr.u32 	%r73, %r70, 8;
	and.b32  	%r74, %r73, 255;
	cvt.rn.f32.u32 	%f77, %r74;
	mul.ftz.f32 	%f78, %f77, 0f3B808081;
	and.b32  	%r75, %r70, 255;
	cvt.rn.f32.u32 	%f79, %r75;
	mul.ftz.f32 	%f80, %f79, 0f3B808081;
	fma.rn.ftz.f32 	%f81, %f73, %f30, %f57;
	cvt.rzi.ftz.s32.f32 	%r76, %f81;
	add.s32 	%r77, %r76, %r9;
	shl.b32 	%r78, %r77, 2;
	add.s32 	%r79, %r91, %r78;
	ld.global.u32 	%r80, [%r79];
	shr.u32 	%r81, %r80, 16;
	and.b32  	%r82, %r81, 255;
	cvt.rn.f32.u32 	%f82, %r82;
	mul.ftz.f32 	%f83, %f82, 0f3B808081;
	shr.u32 	%r83, %r80, 8;
	and.b32  	%r84, %r83, 255;
	cvt.rn.f32.u32 	%f84, %r84;
	mul.ftz.f32 	%f85, %f84, 0f3B808081;
	and.b32  	%r85, %r80, 255;
	cvt.rn.f32.u32 	%f86, %r85;
	mul.ftz.f32 	%f87, %f86, 0f3B808081;
	mul.ftz.f32 	%f200, %f83, %f6;
	mul.ftz.f32 	%f201, %f85, %f6;
	mul.ftz.f32 	%f202, %f87, %f6;
	fma.rn.ftz.f32 	%f204, %f76, %f36, %f200;
	fma.rn.ftz.f32 	%f205, %f78, %f36, %f201;
	fma.rn.ftz.f32 	%f206, %f80, %f36, %f202;
	mul.ftz.f32 	%f220, %f204, %f10;
	mul.ftz.f32 	%f221, %f205, %f10;
	mul.ftz.f32 	%f222, %f206, %f10;
	mul.ftz.f32 	%f223, %f179, %f10;
	fma.rn.ftz.f32 	%f224, %f176, %f37, %f220;
	fma.rn.ftz.f32 	%f225, %f177, %f37, %f221;
	fma.rn.ftz.f32 	%f226, %f178, %f37, %f222;
	fma.rn.ftz.f32 	%f227, %f179, %f37, %f223;

BB5_4:
	ld.const.f32 	%f92, [%r8+160];
	mul.ftz.f32 	%f93, %f224, %f92;
	ld.const.f32 	%f95, [%r8+164];
	mul.ftz.f32 	%f96, %f225, %f95;
	ld.const.f32 	%f98, [%r8+168];
	mul.ftz.f32 	%f99, %f226, %f98;
	st.global.v4.f32 	[%r6], {%f93, %f96, %f99, %f227};
	ret;

BB5_5:
	ld.param.u32 	%r86, [f_m_B_param_0];
	ld.const.v4.f32 	{%f100, %f101, %f102, %f103}, [%r86+144];
	st.global.v4.f32 	[%r6], {%f100, %f101, %f102, %f103};
	ret;
}

.entry f_d_A(
	.param .u32 .ptr .const .align 16 f_d_A_param_0,
	.param .align 16 .b8 f_d_A_param_1[16],
	.param .u32 .ptr .global .align 16 f_d_A_param_2
)
.reqntid 16, 8, 1
{
	.reg .f32 	%f<185>;
	.reg .pred 	%p<17>;
	.reg .s32 	%r<21>;


	ld.param.u32 	%r1, [f_d_A_param_0];
	ld.param.u32 	%r11, [f_d_A_param_2];
	ld.param.v4.f32 	{%f177, %f178, %f179, %f180}, [f_d_A_param_1];
	// inline asm
	mov.u32 	%r3, %elwreg3;
	// inline asm
	// inline asm
	mov.u32 	%r4, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r5, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r6, %tid.x;
	// inline asm
	add.s32 	%r12, %r6, %r3;
	mad.lo.s32 	%r13, %r5, %r4, %r12;
	// inline asm
	mov.u32 	%r7, %elwreg4;
	// inline asm
	// inline asm
	mov.u32 	%r8, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r9, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r10, %tid.y;
	// inline asm
	add.s32 	%r14, %r10, %r7;
	mad.lo.s32 	%r15, %r9, %r8, %r14;
	shl.b32 	%r16, %r15, 8;
	add.s32 	%r17, %r16, %r13;
	cvt.rn.f32.s32 	%f9, %r13;
	add.ftz.f32 	%f11, %f9, %f177;
	cvt.rn.f32.s32 	%f12, %r15;
	add.ftz.f32 	%f14, %f12, %f178;
	ld.const.v4.f32 	{%f117, %f118, %f119, %f120}, [%r1+64];
	setp.gtu.ftz.f32 	%p1, %f11, %f117;
	setp.gtu.ftz.f32 	%p2, %f14, %f118;
	or.pred  	%p3, %p1, %p2;
	shl.b32 	%r18, %r17, 5;
	add.s32 	%r2, %r11, %r18;
	@%p3 bra 	BB6_11;

	mov.f32 	%f27, 0fBF800000;
	fma.rn.ftz.f32 	%f125, %f11, %f119, %f27;
	fma.rn.ftz.f32 	%f126, %f14, %f120, %f27;
	ld.param.u32 	%r19, [f_d_A_param_0];
	ld.const.v4.f32 	{%f141, %f142, %f143, %f144}, [%r19+16];
	ld.const.v4.f32 	{%f157, %f158, %f159, %f160}, [%r19+32];
	mul.ftz.f32 	%f161, %f157, %f126;
	mul.ftz.f32 	%f162, %f158, %f126;
	mul.ftz.f32 	%f163, %f159, %f126;
	mul.ftz.f32 	%f164, %f160, %f126;
	fma.rn.ftz.f32 	%f165, %f141, %f125, %f161;
	fma.rn.ftz.f32 	%f166, %f142, %f125, %f162;
	fma.rn.ftz.f32 	%f167, %f143, %f125, %f163;
	fma.rn.ftz.f32 	%f168, %f144, %f125, %f164;
	ld.const.v4.f32 	{%f169, %f170, %f171, %f172}, [%r19+48];
	add.ftz.f32 	%f173, %f165, %f169;
	add.ftz.f32 	%f174, %f166, %f170;
	add.ftz.f32 	%f175, %f167, %f171;
	add.ftz.f32 	%f176, %f168, %f172;
	ld.const.v4.f32 	{%f85, %f86, %f87, %f88}, [%r19+80];
	sub.ftz.f32 	%f97, %f173, %f85;
	sub.ftz.f32 	%f98, %f174, %f86;
	sub.ftz.f32 	%f99, %f175, %f87;
	sub.ftz.f32 	%f100, %f176, %f88;
	// inline asm
	abs.f32 	%f17, %f97;
	// inline asm
	// inline asm
	abs.f32 	%f19, %f98;
	// inline asm
	// inline asm
	abs.f32 	%f21, %f99;
	// inline asm
	// inline asm
	abs.f32 	%f23, %f100;
	// inline asm
	setp.nan.ftz.f32 	%p4, %f17, %f19;
	setp.nan.ftz.f32 	%p5, %f21, %f21;
	or.pred  	%p6, %p4, %p5;
	setp.nan.ftz.f32 	%p7, %f23, %f23;
	or.pred  	%p8, %p6, %p7;
	@%p8 bra 	BB6_9;

	setp.lt.ftz.f32 	%p9, %f17, %f19;
	selp.f32 	%f36, %f19, %f17, %p9;
	setp.lt.ftz.f32 	%p10, %f36, %f21;
	selp.f32 	%f37, %f21, %f36, %p10;
	setp.lt.ftz.f32 	%p11, %f37, %f23;
	selp.f32 	%f5, %f23, %f37, %p11;
	setp.eq.ftz.f32 	%p12, %f5, 0f00000000;
	@%p12 bra 	BB6_8;

	mov.f32 	%f6, 0f7F800000;
	setp.eq.ftz.f32 	%p13, %f5, 0f7F800000;
	@%p13 bra 	BB6_7;

	div.approx.ftz.f32 	%f40, %f17, %f5;
	mul.rn.f32 	%f41, %f40, %f40;
	div.approx.ftz.f32 	%f42, %f19, %f5;
	mul.rn.f32 	%f43, %f42, %f42;
	add.ftz.f32 	%f44, %f41, %f43;
	div.approx.ftz.f32 	%f45, %f21, %f5;
	mul.rn.f32 	%f46, %f45, %f45;
	add.ftz.f32 	%f47, %f44, %f46;
	div.approx.ftz.f32 	%f48, %f23, %f5;
	mul.rn.f32 	%f49, %f48, %f48;
	add.ftz.f32 	%f39, %f47, %f49;
	// inline asm
	sqrt.rn.f32 	%f38, %f39;
	// inline asm
	mul.rn.f32 	%f8, %f38, %f5;
	setp.eq.ftz.f32 	%p14, %f8, %f6;
	setp.eq.ftz.f32 	%p15, %f8, 0fFF800000;
	or.pred  	%p16, %p14, %p15;
	@%p16 bra 	BB6_6;

	div.approx.ftz.f32 	%f181, %f97, %f8;
	div.approx.ftz.f32 	%f182, %f98, %f8;
	div.approx.ftz.f32 	%f183, %f99, %f8;
	div.approx.ftz.f32 	%f184, %f100, %f8;
	bra.uni 	BB6_10;

BB6_6:
	div.approx.ftz.f32 	%f105, %f97, %f38;
	div.approx.ftz.f32 	%f106, %f98, %f38;
	div.approx.ftz.f32 	%f107, %f99, %f38;
	div.approx.ftz.f32 	%f108, %f100, %f38;
	div.approx.ftz.f32 	%f181, %f105, %f5;
	div.approx.ftz.f32 	%f182, %f106, %f5;
	div.approx.ftz.f32 	%f183, %f107, %f5;
	div.approx.ftz.f32 	%f184, %f108, %f5;
	bra.uni 	BB6_10;

BB6_7:
	div.approx.ftz.f32 	%f181, %f97, %f6;
	div.approx.ftz.f32 	%f182, %f98, %f6;
	div.approx.ftz.f32 	%f183, %f99, %f6;
	div.approx.ftz.f32 	%f184, %f100, %f6;
	bra.uni 	BB6_10;

BB6_8:
	mov.f32 	%f50, 0f00000000;
	mov.f32 	%f181, %f50;
	mov.f32 	%f182, %f50;
	mov.f32 	%f183, %f50;
	mov.f32 	%f184, %f50;
	bra.uni 	BB6_10;

BB6_9:
	mov.f32 	%f51, 0f7FFFFFFF;
	mov.f32 	%f181, %f51;
	mov.f32 	%f182, %f51;
	mov.f32 	%f183, %f51;
	mov.f32 	%f184, %f51;

BB6_10:
	st.global.v4.f32 	[%r2], {%f181, %f182, %f183, %f184};
	ld.param.u32 	%r20, [f_d_A_param_0];
	ld.const.f32 	%f55, [%r20];
	st.global.v4.f32 	[%r2+16], {%f85, %f86, %f87, %f55};
	ret;

BB6_11:
	mov.f32 	%f56, 0fBF800000;
	st.global.v4.f32 	[%r2], {%f56, %f56, %f56, %f56};
	ret;
}

.entry f_d_B(
	.param .u32 .ptr .const .align 16 f_d_B_param_0,
	.param .u32 .ptr .global .align 16 f_d_B_param_1,
	.param .u32 .ptr .global .align 16 f_d_B_param_2
)
.reqntid 16, 8, 1
{
	.reg .f32 	%f<21>;
	.reg .pred 	%p<2>;
	.reg .s32 	%r<23>;


	ld.param.u32 	%r11, [f_d_B_param_1];
	ld.param.u32 	%r12, [f_d_B_param_2];
	// inline asm
	mov.u32 	%r3, %elwreg4;
	// inline asm
	// inline asm
	mov.u32 	%r4, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r5, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r6, %tid.y;
	// inline asm
	add.s32 	%r13, %r6, %r3;
	mad.lo.s32 	%r14, %r5, %r4, %r13;
	shl.b32 	%r15, %r14, 8;
	// inline asm
	mov.u32 	%r7, %elwreg3;
	// inline asm
	// inline asm
	mov.u32 	%r8, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r9, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r10, %tid.x;
	// inline asm
	add.s32 	%r16, %r15, %r7;
	mad.lo.s32 	%r17, %r9, %r8, %r16;
	add.s32 	%r18, %r17, %r10;
	shl.b32 	%r19, %r18, 4;
	add.s32 	%r20, %r11, %r19;
	ld.global.v4.f32 	{%f13, %f14, %f15, %f16}, [%r20];
	setp.ltu.ftz.f32 	%p1, %f13, 0f80000000;
	add.s32 	%r1, %r12, %r19;
	@%p1 bra 	BB7_2;

	ld.param.u32 	%r22, [f_d_B_param_0];
	ld.const.f32 	%f3, [%r22+108];
	mul.ftz.f32 	%f4, %f15, %f3;
	mov.f32 	%f5, 0f3F800000;
	min.f32 	%f6, %f5, %f4;
	mov.f32 	%f7, 0f00000000;
	max.f32 	%f8, %f7, %f6;
	st.global.v4.f32 	[%r1], {%f8, %f8, %f8, %f5};
	ret;

BB7_2:
	ld.param.u32 	%r21, [f_d_B_param_0];
	ld.const.v4.f32 	{%f9, %f10, %f11, %f12}, [%r21+144];
	st.global.v4.f32 	[%r1], {%f9, %f10, %f11, %f12};
	ret;
}

.entry f_a_i_A(
	.param .u32 .ptr .const .align 16 f_a_i_A_param_0,
	.param .align 16 .b8 f_a_i_A_param_1[16],
	.param .u32 .ptr .global .align 4 f_a_i_A_param_2,
	.param .u32 .ptr .global .align 16 f_a_i_A_param_3,
	.param .u32 .ptr .global .align 8 f_a_i_A_param_4
)
.reqntid 16, 8, 1
{
	.reg .f32 	%f<199>;
	.reg .pred 	%p<18>;
	.reg .s32 	%r<37>;


	ld.param.v4.f32 	{%f189, %f190, %f191, %f192}, [f_a_i_A_param_1];
	// inline asm
	mov.u32 	%r10, %elwreg3;
	// inline asm
	// inline asm
	mov.u32 	%r11, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r12, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r13, %tid.x;
	// inline asm
	add.s32 	%r18, %r13, %r10;
	mad.lo.s32 	%r5, %r12, %r11, %r18;
	// inline asm
	mov.u32 	%r14, %elwreg4;
	// inline asm
	// inline asm
	mov.u32 	%r15, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r16, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r17, %tid.y;
	// inline asm
	add.s32 	%r19, %r17, %r14;
	mad.lo.s32 	%r6, %r16, %r15, %r19;
	shl.b32 	%r20, %r6, 8;
	add.s32 	%r7, %r20, %r5;
	setp.neu.ftz.f32 	%p1, %f191, 0f00000000;
	@%p1 bra 	BB8_2;

	shl.b32 	%r21, %r7, 3;
	ld.param.u32 	%r36, [f_a_i_A_param_4];
	add.s32 	%r22, %r36, %r21;
	mov.f32 	%f10, 0f00000000;
	st.global.v2.f32 	[%r22], {%f10, %f10};

BB8_2:
	cvt.rn.f32.s32 	%f11, %r5;
	add.ftz.f32 	%f13, %f11, %f189;
	cvt.rn.f32.s32 	%f15, %r6;
	add.ftz.f32 	%f16, %f15, %f190;
	shl.b32 	%r23, %r7, 2;
	ld.param.u32 	%r34, [f_a_i_A_param_2];
	add.s32 	%r24, %r34, %r23;
	ld.global.u32 	%r25, [%r24];
	mad.lo.s32 	%r26, %r25, 1664525, 1013904223;
	and.b32  	%r27, %r26, 16777215;
	cvt.rn.f32.u32 	%f17, %r27;
	div.rn.ftz.f32 	%f18, %f17, 0f4B800000;
	add.ftz.f32 	%f19, %f18, %f18;
	add.ftz.f32 	%f20, %f19, 0fBF800000;
	mad.lo.s32 	%r28, %r26, 1664525, 1013904223;
	and.b32  	%r29, %r28, 16777215;
	cvt.rn.f32.u32 	%f21, %r29;
	div.rn.ftz.f32 	%f22, %f21, 0f4B800000;
	add.ftz.f32 	%f23, %f22, %f22;
	add.ftz.f32 	%f24, %f23, 0fBF800000;
	st.global.u32 	[%r24], %r28;
	fma.rn.ftz.f32 	%f25, %f20, 0f3F000000, %f13;
	fma.rn.ftz.f32 	%f26, %f24, 0f3F000000, %f16;
	ld.param.u32 	%r33, [f_a_i_A_param_0];
	ld.const.v4.f32 	{%f129, %f130, %f131, %f132}, [%r33+64];
	setp.gtu.ftz.f32 	%p2, %f25, %f129;
	setp.gtu.ftz.f32 	%p3, %f26, %f130;
	or.pred  	%p4, %p2, %p3;
	shl.b32 	%r30, %r7, 5;
	ld.param.u32 	%r35, [f_a_i_A_param_3];
	add.s32 	%r9, %r35, %r30;
	@%p4 bra 	BB8_13;

	mov.f32 	%f39, 0fBF800000;
	fma.rn.ftz.f32 	%f137, %f25, %f131, %f39;
	fma.rn.ftz.f32 	%f138, %f26, %f132, %f39;
	ld.param.u32 	%r31, [f_a_i_A_param_0];
	ld.const.v4.f32 	{%f153, %f154, %f155, %f156}, [%r31+16];
	ld.const.v4.f32 	{%f169, %f170, %f171, %f172}, [%r31+32];
	mul.ftz.f32 	%f173, %f169, %f138;
	mul.ftz.f32 	%f174, %f170, %f138;
	mul.ftz.f32 	%f175, %f171, %f138;
	mul.ftz.f32 	%f176, %f172, %f138;
	fma.rn.ftz.f32 	%f177, %f153, %f137, %f173;
	fma.rn.ftz.f32 	%f178, %f154, %f137, %f174;
	fma.rn.ftz.f32 	%f179, %f155, %f137, %f175;
	fma.rn.ftz.f32 	%f180, %f156, %f137, %f176;
	ld.const.v4.f32 	{%f181, %f182, %f183, %f184}, [%r31+48];
	add.ftz.f32 	%f185, %f177, %f181;
	add.ftz.f32 	%f186, %f178, %f182;
	add.ftz.f32 	%f187, %f179, %f183;
	add.ftz.f32 	%f188, %f180, %f184;
	ld.const.v4.f32 	{%f97, %f98, %f99, %f100}, [%r31+80];
	sub.ftz.f32 	%f109, %f185, %f97;
	sub.ftz.f32 	%f110, %f186, %f98;
	sub.ftz.f32 	%f111, %f187, %f99;
	sub.ftz.f32 	%f112, %f188, %f100;
	// inline asm
	abs.f32 	%f29, %f109;
	// inline asm
	// inline asm
	abs.f32 	%f31, %f110;
	// inline asm
	// inline asm
	abs.f32 	%f33, %f111;
	// inline asm
	// inline asm
	abs.f32 	%f35, %f112;
	// inline asm
	setp.nan.ftz.f32 	%p5, %f29, %f31;
	setp.nan.ftz.f32 	%p6, %f33, %f33;
	or.pred  	%p7, %p5, %p6;
	setp.nan.ftz.f32 	%p8, %f35, %f35;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB8_11;

	setp.lt.ftz.f32 	%p10, %f29, %f31;
	selp.f32 	%f48, %f31, %f29, %p10;
	setp.lt.ftz.f32 	%p11, %f48, %f33;
	selp.f32 	%f49, %f33, %f48, %p11;
	setp.lt.ftz.f32 	%p12, %f49, %f35;
	selp.f32 	%f5, %f35, %f49, %p12;
	setp.eq.ftz.f32 	%p13, %f5, 0f00000000;
	@%p13 bra 	BB8_10;

	mov.f32 	%f6, 0f7F800000;
	setp.eq.ftz.f32 	%p14, %f5, 0f7F800000;
	@%p14 bra 	BB8_9;

	div.approx.ftz.f32 	%f52, %f29, %f5;
	mul.rn.f32 	%f53, %f52, %f52;
	div.approx.ftz.f32 	%f54, %f31, %f5;
	mul.rn.f32 	%f55, %f54, %f54;
	add.ftz.f32 	%f56, %f53, %f55;
	div.approx.ftz.f32 	%f57, %f33, %f5;
	mul.rn.f32 	%f58, %f57, %f57;
	add.ftz.f32 	%f59, %f56, %f58;
	div.approx.ftz.f32 	%f60, %f35, %f5;
	mul.rn.f32 	%f61, %f60, %f60;
	add.ftz.f32 	%f51, %f59, %f61;
	// inline asm
	sqrt.rn.f32 	%f50, %f51;
	// inline asm
	mul.rn.f32 	%f8, %f50, %f5;
	setp.eq.ftz.f32 	%p15, %f8, %f6;
	setp.eq.ftz.f32 	%p16, %f8, 0fFF800000;
	or.pred  	%p17, %p15, %p16;
	@%p17 bra 	BB8_8;

	div.approx.ftz.f32 	%f195, %f109, %f8;
	div.approx.ftz.f32 	%f196, %f110, %f8;
	div.approx.ftz.f32 	%f197, %f111, %f8;
	div.approx.ftz.f32 	%f198, %f112, %f8;
	bra.uni 	BB8_12;

BB8_8:
	div.approx.ftz.f32 	%f117, %f109, %f50;
	div.approx.ftz.f32 	%f118, %f110, %f50;
	div.approx.ftz.f32 	%f119, %f111, %f50;
	div.approx.ftz.f32 	%f120, %f112, %f50;
	div.approx.ftz.f32 	%f195, %f117, %f5;
	div.approx.ftz.f32 	%f196, %f118, %f5;
	div.approx.ftz.f32 	%f197, %f119, %f5;
	div.approx.ftz.f32 	%f198, %f120, %f5;
	bra.uni 	BB8_12;

BB8_9:
	div.approx.ftz.f32 	%f195, %f109, %f6;
	div.approx.ftz.f32 	%f196, %f110, %f6;
	div.approx.ftz.f32 	%f197, %f111, %f6;
	div.approx.ftz.f32 	%f198, %f112, %f6;
	bra.uni 	BB8_12;

BB8_10:
	mov.f32 	%f62, 0f00000000;
	mov.f32 	%f195, %f62;
	mov.f32 	%f196, %f62;
	mov.f32 	%f197, %f62;
	mov.f32 	%f198, %f62;
	bra.uni 	BB8_12;

BB8_11:
	mov.f32 	%f63, 0f7FFFFFFF;
	mov.f32 	%f195, %f63;
	mov.f32 	%f196, %f63;
	mov.f32 	%f197, %f63;
	mov.f32 	%f198, %f63;

BB8_12:
	st.global.v4.f32 	[%r9], {%f195, %f196, %f197, %f198};
	ld.param.u32 	%r32, [f_a_i_A_param_0];
	ld.const.f32 	%f67, [%r32];
	st.global.v4.f32 	[%r9+16], {%f97, %f98, %f99, %f67};
	ret;

BB8_13:
	mov.f32 	%f68, 0fBF800000;
	st.global.v4.f32 	[%r9], {%f68, %f68, %f68, %f68};
	ret;
}

.entry f_a_i_B(
	.param .u32 .ptr .const .align 16 f_a_i_B_param_0,
	.param .u32 .ptr .global .align 16 f_a_i_B_param_1,
	.param .u32 .ptr .global .align 16 f_a_i_B_param_2,
	.param .u32 .ptr .global .align 16 f_a_i_B_param_3,
	.param .u32 .ptr .global .align 16 f_a_i_B_param_4,
	.param .u32 .ptr .global .align 16 f_a_i_B_param_5,
	.param .u32 .ptr .global .align 16 f_a_i_B_param_6,
	.param .u32 .ptr .global .align 8 f_a_i_B_param_7,
	.param .u32 .ptr .global .align 16 f_a_i_B_param_8
)
.reqntid 16, 8, 1
{
	.reg .f32 	%f<119>;
	.reg .pred 	%p<6>;
	.reg .s32 	%r<45>;


	ld.param.u32 	%r18, [f_a_i_B_param_6];
	ld.param.u32 	%r19, [f_a_i_B_param_8];
	// inline asm
	mov.u32 	%r10, %elwreg4;
	// inline asm
	// inline asm
	mov.u32 	%r11, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r12, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r13, %tid.y;
	// inline asm
	add.s32 	%r20, %r13, %r10;
	mad.lo.s32 	%r21, %r12, %r11, %r20;
	shl.b32 	%r22, %r21, 8;
	// inline asm
	mov.u32 	%r14, %elwreg3;
	// inline asm
	// inline asm
	mov.u32 	%r15, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r16, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r17, %tid.x;
	// inline asm
	add.s32 	%r23, %r22, %r14;
	mad.lo.s32 	%r24, %r16, %r15, %r23;
	add.s32 	%r8, %r24, %r17;
	shl.b32 	%r25, %r8, 4;
	add.s32 	%r26, %r19, %r25;
	ld.global.v4.f32 	{%f79, %f80, %f81, %f82}, [%r26];
	setp.ltu.ftz.f32 	%p1, %f79, 0f80000000;
	add.s32 	%r9, %r18, %r25;
	@%p1 bra 	BB9_7;

	shl.b32 	%r27, %r8, 3;
	ld.param.u32 	%r44, [f_a_i_B_param_7];
	add.s32 	%r28, %r44, %r27;
	ld.global.v2.f32 	{%f67, %f68}, [%r28];
	add.ftz.f32 	%f8, %f68, 0f3F800000;
	mov.f32 	%f9, 0f3F800000;
	st.global.v2.f32 	[%r28], {%f67, %f8};
	shl.b32 	%r29, %r8, 5;
	ld.param.u32 	%r42, [f_a_i_B_param_4];
	add.s32 	%r30, %r42, %r29;
	ld.global.v4.f32 	{%f91, %f92, %f93, %f94}, [%r30];
	ld.param.u32 	%r38, [f_a_i_B_param_0];
	ld.const.v4.f32 	{%f95, %f96, %f97, %f98}, [%r38+80];
	fma.rn.ftz.f32 	%f99, %f91, %f81, %f95;
	fma.rn.ftz.f32 	%f100, %f92, %f81, %f96;
	fma.rn.ftz.f32 	%f101, %f93, %f81, %f97;
	fma.rn.ftz.f32 	%f102, %f94, %f81, %f98;
	ld.param.u32 	%r43, [f_a_i_B_param_5];
	add.s32 	%r32, %r43, %r25;
	st.global.v4.f32 	[%r32], {%f99, %f100, %f101, %f102};
	sub.ftz.f32 	%f11, %f9, %f79;
	sub.ftz.f32 	%f12, %f11, %f80;
	mov.b32 	 %r33, %f82;
	shl.b32 	%r34, %r33, 4;
	ld.param.u32 	%r39, [f_a_i_B_param_1];
	add.s32 	%r35, %r39, %r34;
	ld.param.u32 	%r40, [f_a_i_B_param_2];
	add.s32 	%r36, %r40, %r34;
	ld.param.u32 	%r41, [f_a_i_B_param_3];
	add.s32 	%r37, %r41, %r34;
	ld.global.v4.f32 	{%f103, %f104, %f105, %f106}, [%r35];
	mul.ftz.f32 	%f16, %f104, %f79;
	fma.rn.ftz.f32 	%f17, %f103, %f12, %f16;
	fma.rn.ftz.f32 	%f6, %f105, %f80, %f17;
	ld.global.v4.f32 	{%f107, %f108, %f109, %f110}, [%r36];
	mul.ftz.f32 	%f21, %f108, %f79;
	fma.rn.ftz.f32 	%f22, %f107, %f12, %f21;
	fma.rn.ftz.f32 	%f3, %f109, %f80, %f22;
	ld.global.v4.f32 	{%f111, %f112, %f113, %f114}, [%r37];
	mul.ftz.f32 	%f26, %f112, %f79;
	fma.rn.ftz.f32 	%f27, %f111, %f12, %f26;
	fma.rn.ftz.f32 	%f4, %f113, %f80, %f27;
	mov.f32 	%f29, 0f00000000;
	mov.f32 	%f115, %f6;
	mov.f32 	%f116, %f3;
	mov.f32 	%f117, %f4;
	mov.f32 	%f118, %f29;
	// inline asm
	abs.f32 	%f5, %f6;
	// inline asm
	setp.neu.ftz.f32 	%p2, %f5, 0f00000000;
	@%p2 bra 	BB9_5;

	// inline asm
	abs.f32 	%f30, %f3;
	// inline asm
	setp.neu.ftz.f32 	%p3, %f30, 0f00000000;
	@%p3 bra 	BB9_5;

	// inline asm
	abs.f32 	%f32, %f4;
	// inline asm
	setp.neu.ftz.f32 	%p4, %f32, 0f00000000;
	@%p4 bra 	BB9_5;

	// inline asm
	abs.f32 	%f34, %f29;
	// inline asm
	setp.eq.ftz.f32 	%p5, %f34, 0f00000000;
	@%p5 bra 	BB9_6;

BB9_5:
	mul.rn.f32 	%f38, %f6, %f6;
	mul.rn.f32 	%f39, %f3, %f3;
	add.ftz.f32 	%f40, %f38, %f39;
	mul.rn.f32 	%f41, %f4, %f4;
	add.ftz.f32 	%f42, %f40, %f41;
	mul.rn.f32 	%f44, %f29, %f29;
	add.ftz.f32 	%f37, %f42, %f44;
	// inline asm
	rsqrt.approx.f32 	%f36, %f37;
	// inline asm
	mul.rn.f32 	%f45, %f6, %f36;
	mul.rn.f32 	%f46, %f3, %f36;
	mul.rn.f32 	%f47, %f4, %f36;
	mul.rn.f32 	%f48, %f29, %f36;
	mov.f32 	%f115, %f45;
	mov.f32 	%f116, %f46;
	mov.f32 	%f117, %f47;
	mov.f32 	%f118, %f48;

BB9_6:
	st.global.v4.f32 	[%r9], {%f115, %f116, %f117, %f118};
	ret;

BB9_7:
	mov.f32 	%f49, 0fBF800000;
	mov.f32 	%f50, 0f00000000;
	st.global.v4.f32 	[%r9], {%f50, %f50, %f50, %f49};
	ret;
}

.entry f_a_a_A(
	.param .u32 .ptr .const .align 16 f_a_a_A_param_0,
	.param .align 16 .b8 f_a_a_A_param_1[16],
	.param .u32 .ptr .global .align 4 f_a_a_A_param_2,
	.param .u32 .ptr .global .align 16 f_a_a_A_param_3,
	.param .u32 .ptr .global .align 16 f_a_a_A_param_4,
	.param .u32 .ptr .global .align 16 f_a_a_A_param_5
)
.reqntid 16, 8, 1
{
	.reg .f32 	%f<247>;
	.reg .pred 	%p<11>;
	.reg .s32 	%r<38>;


	ld.param.u32 	%r15, [f_a_a_A_param_4];
	ld.param.u32 	%r16, [f_a_a_A_param_5];
	ld.param.v4.f32 	{%f235, %f236, %f237, %f238}, [f_a_a_A_param_1];
	// inline asm
	mov.u32 	%r7, %elwreg4;
	// inline asm
	// inline asm
	mov.u32 	%r8, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r9, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r10, %tid.y;
	// inline asm
	add.s32 	%r17, %r10, %r7;
	mad.lo.s32 	%r18, %r9, %r8, %r17;
	shl.b32 	%r19, %r18, 8;
	// inline asm
	mov.u32 	%r11, %elwreg3;
	// inline asm
	// inline asm
	mov.u32 	%r12, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r13, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r14, %tid.x;
	// inline asm
	add.s32 	%r20, %r19, %r11;
	mad.lo.s32 	%r21, %r13, %r12, %r20;
	add.s32 	%r4, %r21, %r14;
	shl.b32 	%r22, %r4, 4;
	add.s32 	%r23, %r15, %r22;
	ld.global.v4.f32 	{%f175, %f176, %f177, %f178}, [%r23];
	setp.ltu.ftz.f32 	%p1, %f178, 0f00000000;
	shl.b32 	%r24, %r4, 5;
	add.s32 	%r5, %r16, %r24;
	@%p1 bra 	BB10_15;

	shl.b32 	%r25, %r4, 2;
	ld.param.u32 	%r36, [f_a_a_A_param_2];
	add.s32 	%r26, %r36, %r25;
	ld.global.u32 	%r27, [%r26];
	mad.lo.s32 	%r28, %r27, 1664525, 1013904223;
	and.b32  	%r29, %r28, 16777215;
	cvt.rn.f32.u32 	%f22, %r29;
	div.rn.ftz.f32 	%f23, %f22, 0f4B800000;
	add.ftz.f32 	%f25, %f235, %f23;
	mul.ftz.f32 	%f27, %f25, %f237;
	mad.lo.s32 	%r30, %r28, 1664525, 1013904223;
	and.b32  	%r31, %r30, 16777215;
	cvt.rn.f32.u32 	%f28, %r31;
	div.rn.ftz.f32 	%f29, %f28, 0f4B800000;
	add.ftz.f32 	%f31, %f236, %f29;
	mul.ftz.f32 	%f11, %f31, %f238;
	st.global.u32 	[%r26], %r30;
	ld.param.u32 	%r35, [f_a_a_A_param_0];
	ld.const.f32 	%f33, [%r35+112];
	ld.const.f32 	%f34, [%r35+116];
	// inline asm
	sqrt.approx.f32 	%f10, %f11;
	// inline asm
	mul.ftz.f32 	%f35, %f33, %f10;
	mul.ftz.f32 	%f17, %f27, 0f40C90FE4;
	mov.f32 	%f36, 0f3F800000;
	neg.f32 	%f37, %f11;
	fma.rn.ftz.f32 	%f38, %f37, %f34, %f36;
	mov.f32 	%f39, 0f00000000;
	max.f32 	%f13, %f39, %f38;
	// inline asm
	sqrt.approx.f32 	%f12, %f13;
	// inline asm
	// inline asm
	sin.approx.f32 	%f14, %f17;
	// inline asm
	mul.ftz.f32 	%f2, %f14, %f35;
	// inline asm
	cos.approx.f32 	%f16, %f17;
	// inline asm
	mul.ftz.f32 	%f3, %f16, %f35;
	// inline asm
	abs.f32 	%f18, %f175;
	// inline asm
	// inline asm
	abs.f32 	%f20, %f177;
	// inline asm
	setp.ltu.ftz.f32 	%p2, %f18, %f20;
	@%p2 bra 	BB10_3;

	neg.ftz.f32 	%f40, %f176;
	mov.f32 	%f243, %f40;
	mov.f32 	%f244, %f175;
	mov.f32 	%f245, %f39;
	bra.uni 	BB10_4;

BB10_3:
	neg.ftz.f32 	%f42, %f177;
	mov.f32 	%f243, %f39;
	mov.f32 	%f244, %f42;
	mov.f32 	%f245, %f176;

BB10_4:
	mov.f32 	%f246, %f39;
	// inline asm
	abs.f32 	%f44, %f243;
	// inline asm
	setp.neu.ftz.f32 	%p3, %f44, 0f00000000;
	@%p3 bra 	BB10_8;

	// inline asm
	abs.f32 	%f46, %f244;
	// inline asm
	setp.neu.ftz.f32 	%p4, %f46, 0f00000000;
	@%p4 bra 	BB10_8;

	// inline asm
	abs.f32 	%f48, %f245;
	// inline asm
	setp.neu.ftz.f32 	%p5, %f48, 0f00000000;
	@%p5 bra 	BB10_8;

	// inline asm
	abs.f32 	%f50, %f246;
	// inline asm
	setp.eq.ftz.f32 	%p6, %f50, 0f00000000;
	@%p6 bra 	BB10_9;

BB10_8:
	mul.rn.f32 	%f54, %f243, %f243;
	mul.rn.f32 	%f56, %f244, %f244;
	add.ftz.f32 	%f57, %f54, %f56;
	mul.rn.f32 	%f59, %f245, %f245;
	add.ftz.f32 	%f60, %f57, %f59;
	mul.rn.f32 	%f62, %f246, %f246;
	add.ftz.f32 	%f53, %f60, %f62;
	// inline asm
	rsqrt.approx.f32 	%f52, %f53;
	// inline asm
	mul.rn.f32 	%f63, %f243, %f52;
	mul.rn.f32 	%f64, %f244, %f52;
	mul.rn.f32 	%f65, %f245, %f52;
	mul.rn.f32 	%f66, %f246, %f52;
	mov.f32 	%f243, %f63;
	mov.f32 	%f244, %f64;
	mov.f32 	%f245, %f65;
	mov.f32 	%f246, %f66;

BB10_9:
	mul.rn.f32 	%f70, %f244, %f177;
	mul.rn.f32 	%f73, %f245, %f176;
	sub.ftz.f32 	%f74, %f70, %f73;
	mul.rn.f32 	%f76, %f245, %f175;
	mul.rn.f32 	%f78, %f243, %f177;
	sub.ftz.f32 	%f79, %f76, %f78;
	mul.rn.f32 	%f80, %f243, %f176;
	mul.rn.f32 	%f81, %f244, %f175;
	sub.ftz.f32 	%f82, %f80, %f81;
	mul.ftz.f32 	%f207, %f74, %f2;
	mul.ftz.f32 	%f208, %f79, %f2;
	mul.ftz.f32 	%f209, %f82, %f2;
	mul.ftz.f32 	%f210, %f39, %f2;
	fma.rn.ftz.f32 	%f211, %f175, %f12, %f207;
	fma.rn.ftz.f32 	%f212, %f176, %f12, %f208;
	fma.rn.ftz.f32 	%f213, %f177, %f12, %f209;
	fma.rn.ftz.f32 	%f214, %f178, %f12, %f210;
	fma.rn.ftz.f32 	%f239, %f243, %f3, %f211;
	fma.rn.ftz.f32 	%f240, %f244, %f3, %f212;
	fma.rn.ftz.f32 	%f241, %f245, %f3, %f213;
	fma.rn.ftz.f32 	%f242, %f246, %f3, %f214;
	// inline asm
	abs.f32 	%f67, %f239;
	// inline asm
	setp.neu.ftz.f32 	%p7, %f67, 0f00000000;
	@%p7 bra 	BB10_13;

	// inline asm
	abs.f32 	%f93, %f240;
	// inline asm
	setp.neu.ftz.f32 	%p8, %f93, 0f00000000;
	@%p8 bra 	BB10_13;

	// inline asm
	abs.f32 	%f95, %f241;
	// inline asm
	setp.neu.ftz.f32 	%p9, %f95, 0f00000000;
	@%p9 bra 	BB10_13;

	// inline asm
	abs.f32 	%f97, %f242;
	// inline asm
	setp.eq.ftz.f32 	%p10, %f97, 0f00000000;
	@%p10 bra 	BB10_14;

BB10_13:
	mul.rn.f32 	%f101, %f239, %f239;
	mul.rn.f32 	%f103, %f240, %f240;
	add.ftz.f32 	%f104, %f101, %f103;
	mul.rn.f32 	%f106, %f241, %f241;
	add.ftz.f32 	%f107, %f104, %f106;
	mul.rn.f32 	%f109, %f242, %f242;
	add.ftz.f32 	%f100, %f107, %f109;
	// inline asm
	rsqrt.approx.f32 	%f99, %f100;
	// inline asm
	mul.rn.f32 	%f110, %f239, %f99;
	mul.rn.f32 	%f111, %f240, %f99;
	mul.rn.f32 	%f112, %f241, %f99;
	mul.rn.f32 	%f113, %f242, %f99;
	mov.f32 	%f239, %f110;
	mov.f32 	%f240, %f111;
	mov.f32 	%f241, %f112;
	mov.f32 	%f242, %f113;

BB10_14:
	st.global.v4.f32 	[%r5], {%f239, %f240, %f241, %f242};
	ld.param.u32 	%r34, [f_a_a_A_param_0];
	ld.const.f32 	%f114, [%r34+96];
	ld.param.u32 	%r37, [f_a_a_A_param_3];
	add.s32 	%r33, %r37, %r22;
	ld.global.v4.f32 	{%f151, %f152, %f153, %f154}, [%r33];
	fma.rn.ftz.f32 	%f155, %f239, %f114, %f151;
	fma.rn.ftz.f32 	%f156, %f240, %f114, %f152;
	fma.rn.ftz.f32 	%f157, %f241, %f114, %f153;
	ld.const.f32 	%f121, [%r34+124];
	st.global.v4.f32 	[%r5+16], {%f155, %f156, %f157, %f121};
	ret;

BB10_15:
	mov.f32 	%f122, 0fBF800000;
	st.global.v4.f32 	[%r5], {%f122, %f122, %f122, %f122};
	ret;
}

.entry f_a_a_B(
	.param .u32 .ptr .global .align 8 f_a_a_B_param_0,
	.param .u32 .ptr .global .align 4 f_a_a_B_param_1
)
.reqntid 16, 8, 1
{
	.reg .f32 	%f<8>;
	.reg .pred 	%p<2>;
	.reg .s32 	%r<22>;


	ld.param.u32 	%r11, [f_a_a_B_param_1];
	// inline asm
	mov.u32 	%r3, %elwreg4;
	// inline asm
	// inline asm
	mov.u32 	%r4, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r5, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r6, %tid.y;
	// inline asm
	add.s32 	%r12, %r6, %r3;
	mad.lo.s32 	%r13, %r5, %r4, %r12;
	shl.b32 	%r14, %r13, 8;
	// inline asm
	mov.u32 	%r7, %elwreg3;
	// inline asm
	// inline asm
	mov.u32 	%r8, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r9, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r10, %tid.x;
	// inline asm
	add.s32 	%r15, %r14, %r7;
	mad.lo.s32 	%r16, %r9, %r8, %r15;
	add.s32 	%r2, %r16, %r10;
	shl.b32 	%r17, %r2, 2;
	add.s32 	%r18, %r11, %r17;
	ld.global.f32 	%f1, [%r18];
	setp.gt.ftz.f32 	%p1, %f1, 0f00000000;
	@%p1 bra 	BB11_2;

	ret;

BB11_2:
	shl.b32 	%r19, %r2, 3;
	ld.param.u32 	%r21, [f_a_a_B_param_0];
	add.s32 	%r20, %r21, %r19;
	ld.global.v2.f32 	{%f4, %f5}, [%r20];
	add.ftz.f32 	%f3, %f4, 0f3F800000;
	st.global.v2.f32 	[%r20], {%f3, %f5};
	ret;
}

.entry f_a_f(
	.param .u32 .ptr .const .align 16 f_a_f_param_0,
	.param .align 16 .b8 f_a_f_param_1[16],
	.param .u32 .ptr .global .align 8 f_a_f_param_2,
	.param .u32 .ptr .global .align 16 f_a_f_param_3
)
.reqntid 16, 8, 1
{
	.reg .f32 	%f<35>;
	.reg .pred 	%p<2>;
	.reg .s32 	%r<23>;


	ld.param.u32 	%r11, [f_a_f_param_2];
	ld.param.u32 	%r12, [f_a_f_param_3];
	ld.param.v4.f32 	{%f13, %f14, %f15, %f16}, [f_a_f_param_1];
	// inline asm
	mov.u32 	%r3, %elwreg4;
	// inline asm
	// inline asm
	mov.u32 	%r4, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r5, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r6, %tid.y;
	// inline asm
	add.s32 	%r13, %r6, %r3;
	mad.lo.s32 	%r14, %r5, %r4, %r13;
	shl.b32 	%r15, %r14, 8;
	// inline asm
	mov.u32 	%r7, %elwreg3;
	// inline asm
	// inline asm
	mov.u32 	%r8, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r9, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r10, %tid.x;
	// inline asm
	add.s32 	%r16, %r15, %r7;
	mad.lo.s32 	%r17, %r9, %r8, %r16;
	add.s32 	%r18, %r17, %r10;
	shl.b32 	%r19, %r18, 3;
	add.s32 	%r20, %r11, %r19;
	ld.global.v2.f32 	{%f25, %f26}, [%r20];
	setp.gt.ftz.f32 	%p1, %f26, 0f00000000;
	shl.b32 	%r21, %r18, 4;
	add.s32 	%r2, %r12, %r21;
	@%p1 bra 	BB12_2;

	ld.param.u32 	%r22, [f_a_f_param_0];
	ld.const.v4.f32 	{%f31, %f32, %f33, %f34}, [%r22+144];
	st.global.v4.f32 	[%r2], {%f31, %f32, %f33, %f34};
	ret;

BB12_2:
	mul.ftz.f32 	%f23, %f25, %f13;
	mul.ftz.f32 	%f24, %f26, %f13;
	mov.f32 	%f5, 0f3F800000;
	min.f32 	%f6, %f5, %f23;
	mov.f32 	%f7, 0f00000000;
	max.f32 	%f8, %f7, %f6;
	sub.ftz.f32 	%f9, %f5, %f8;
	min.f32 	%f11, %f5, %f24;
	max.f32 	%f12, %f7, %f11;
	st.global.v4.f32 	[%r2], {%f9, %f9, %f9, %f12};
	ret;
}

.entry f_p_i_A(
	.param .u32 .ptr .const .align 16 f_p_i_A_param_0,
	.param .align 16 .b8 f_p_i_A_param_1[16],
	.param .u32 .ptr .global .align 4 f_p_i_A_param_2,
	.param .u32 .ptr .global .align 16 f_p_i_A_param_3
)
.reqntid 16, 8, 1
{
	.reg .f32 	%f<195>;
	.reg .pred 	%p<17>;
	.reg .s32 	%r<29>;


	ld.param.u32 	%r1, [f_p_i_A_param_0];
	ld.param.u32 	%r11, [f_p_i_A_param_2];
	ld.param.u32 	%r12, [f_p_i_A_param_3];
	ld.param.v4.f32 	{%f187, %f188, %f189, %f190}, [f_p_i_A_param_1];
	// inline asm
	mov.u32 	%r3, %elwreg3;
	// inline asm
	// inline asm
	mov.u32 	%r4, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r5, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r6, %tid.x;
	// inline asm
	add.s32 	%r13, %r6, %r3;
	mad.lo.s32 	%r14, %r5, %r4, %r13;
	// inline asm
	mov.u32 	%r7, %elwreg4;
	// inline asm
	// inline asm
	mov.u32 	%r8, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r9, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r10, %tid.y;
	// inline asm
	add.s32 	%r15, %r10, %r7;
	mad.lo.s32 	%r16, %r9, %r8, %r15;
	shl.b32 	%r17, %r16, 8;
	add.s32 	%r18, %r17, %r14;
	cvt.rn.f32.s32 	%f9, %r14;
	add.ftz.f32 	%f11, %f9, %f187;
	cvt.rn.f32.s32 	%f12, %r16;
	add.ftz.f32 	%f14, %f12, %f188;
	shl.b32 	%r19, %r18, 2;
	add.s32 	%r20, %r11, %r19;
	ld.global.u32 	%r21, [%r20];
	mad.lo.s32 	%r22, %r21, 1664525, 1013904223;
	and.b32  	%r23, %r22, 16777215;
	cvt.rn.f32.u32 	%f15, %r23;
	div.rn.ftz.f32 	%f16, %f15, 0f4B800000;
	add.ftz.f32 	%f17, %f16, %f16;
	add.ftz.f32 	%f18, %f17, 0fBF800000;
	mad.lo.s32 	%r24, %r22, 1664525, 1013904223;
	and.b32  	%r25, %r24, 16777215;
	cvt.rn.f32.u32 	%f19, %r25;
	div.rn.ftz.f32 	%f20, %f19, 0f4B800000;
	add.ftz.f32 	%f21, %f20, %f20;
	add.ftz.f32 	%f22, %f21, 0fBF800000;
	st.global.u32 	[%r20], %r24;
	fma.rn.ftz.f32 	%f23, %f18, 0f3F000000, %f11;
	fma.rn.ftz.f32 	%f24, %f22, 0f3F000000, %f14;
	ld.const.v4.f32 	{%f127, %f128, %f129, %f130}, [%r1+64];
	setp.gtu.ftz.f32 	%p1, %f23, %f127;
	setp.gtu.ftz.f32 	%p2, %f24, %f128;
	or.pred  	%p3, %p1, %p2;
	shl.b32 	%r26, %r18, 5;
	add.s32 	%r2, %r12, %r26;
	@%p3 bra 	BB13_11;

	mov.f32 	%f37, 0fBF800000;
	fma.rn.ftz.f32 	%f135, %f23, %f129, %f37;
	fma.rn.ftz.f32 	%f136, %f24, %f130, %f37;
	ld.param.u32 	%r27, [f_p_i_A_param_0];
	ld.const.v4.f32 	{%f151, %f152, %f153, %f154}, [%r27+16];
	ld.const.v4.f32 	{%f167, %f168, %f169, %f170}, [%r27+32];
	mul.ftz.f32 	%f171, %f167, %f136;
	mul.ftz.f32 	%f172, %f168, %f136;
	mul.ftz.f32 	%f173, %f169, %f136;
	mul.ftz.f32 	%f174, %f170, %f136;
	fma.rn.ftz.f32 	%f175, %f151, %f135, %f171;
	fma.rn.ftz.f32 	%f176, %f152, %f135, %f172;
	fma.rn.ftz.f32 	%f177, %f153, %f135, %f173;
	fma.rn.ftz.f32 	%f178, %f154, %f135, %f174;
	ld.const.v4.f32 	{%f179, %f180, %f181, %f182}, [%r27+48];
	add.ftz.f32 	%f183, %f175, %f179;
	add.ftz.f32 	%f184, %f176, %f180;
	add.ftz.f32 	%f185, %f177, %f181;
	add.ftz.f32 	%f186, %f178, %f182;
	ld.const.v4.f32 	{%f95, %f96, %f97, %f98}, [%r27+80];
	sub.ftz.f32 	%f107, %f183, %f95;
	sub.ftz.f32 	%f108, %f184, %f96;
	sub.ftz.f32 	%f109, %f185, %f97;
	sub.ftz.f32 	%f110, %f186, %f98;
	// inline asm
	abs.f32 	%f27, %f107;
	// inline asm
	// inline asm
	abs.f32 	%f29, %f108;
	// inline asm
	// inline asm
	abs.f32 	%f31, %f109;
	// inline asm
	// inline asm
	abs.f32 	%f33, %f110;
	// inline asm
	setp.nan.ftz.f32 	%p4, %f27, %f29;
	setp.nan.ftz.f32 	%p5, %f31, %f31;
	or.pred  	%p6, %p4, %p5;
	setp.nan.ftz.f32 	%p7, %f33, %f33;
	or.pred  	%p8, %p6, %p7;
	@%p8 bra 	BB13_9;

	setp.lt.ftz.f32 	%p9, %f27, %f29;
	selp.f32 	%f46, %f29, %f27, %p9;
	setp.lt.ftz.f32 	%p10, %f46, %f31;
	selp.f32 	%f47, %f31, %f46, %p10;
	setp.lt.ftz.f32 	%p11, %f47, %f33;
	selp.f32 	%f5, %f33, %f47, %p11;
	setp.eq.ftz.f32 	%p12, %f5, 0f00000000;
	@%p12 bra 	BB13_8;

	mov.f32 	%f6, 0f7F800000;
	setp.eq.ftz.f32 	%p13, %f5, 0f7F800000;
	@%p13 bra 	BB13_7;

	div.approx.ftz.f32 	%f50, %f27, %f5;
	mul.rn.f32 	%f51, %f50, %f50;
	div.approx.ftz.f32 	%f52, %f29, %f5;
	mul.rn.f32 	%f53, %f52, %f52;
	add.ftz.f32 	%f54, %f51, %f53;
	div.approx.ftz.f32 	%f55, %f31, %f5;
	mul.rn.f32 	%f56, %f55, %f55;
	add.ftz.f32 	%f57, %f54, %f56;
	div.approx.ftz.f32 	%f58, %f33, %f5;
	mul.rn.f32 	%f59, %f58, %f58;
	add.ftz.f32 	%f49, %f57, %f59;
	// inline asm
	sqrt.rn.f32 	%f48, %f49;
	// inline asm
	mul.rn.f32 	%f8, %f48, %f5;
	setp.eq.ftz.f32 	%p14, %f8, %f6;
	setp.eq.ftz.f32 	%p15, %f8, 0fFF800000;
	or.pred  	%p16, %p14, %p15;
	@%p16 bra 	BB13_6;

	div.approx.ftz.f32 	%f191, %f107, %f8;
	div.approx.ftz.f32 	%f192, %f108, %f8;
	div.approx.ftz.f32 	%f193, %f109, %f8;
	div.approx.ftz.f32 	%f194, %f110, %f8;
	bra.uni 	BB13_10;

BB13_6:
	div.approx.ftz.f32 	%f115, %f107, %f48;
	div.approx.ftz.f32 	%f116, %f108, %f48;
	div.approx.ftz.f32 	%f117, %f109, %f48;
	div.approx.ftz.f32 	%f118, %f110, %f48;
	div.approx.ftz.f32 	%f191, %f115, %f5;
	div.approx.ftz.f32 	%f192, %f116, %f5;
	div.approx.ftz.f32 	%f193, %f117, %f5;
	div.approx.ftz.f32 	%f194, %f118, %f5;
	bra.uni 	BB13_10;

BB13_7:
	div.approx.ftz.f32 	%f191, %f107, %f6;
	div.approx.ftz.f32 	%f192, %f108, %f6;
	div.approx.ftz.f32 	%f193, %f109, %f6;
	div.approx.ftz.f32 	%f194, %f110, %f6;
	bra.uni 	BB13_10;

BB13_8:
	mov.f32 	%f60, 0f00000000;
	mov.f32 	%f191, %f60;
	mov.f32 	%f192, %f60;
	mov.f32 	%f193, %f60;
	mov.f32 	%f194, %f60;
	bra.uni 	BB13_10;

BB13_9:
	mov.f32 	%f61, 0f7FFFFFFF;
	mov.f32 	%f191, %f61;
	mov.f32 	%f192, %f61;
	mov.f32 	%f193, %f61;
	mov.f32 	%f194, %f61;

BB13_10:
	st.global.v4.f32 	[%r2], {%f191, %f192, %f193, %f194};
	ld.param.u32 	%r28, [f_p_i_A_param_0];
	ld.const.f32 	%f65, [%r28];
	st.global.v4.f32 	[%r2+16], {%f95, %f96, %f97, %f65};
	ret;

BB13_11:
	mov.f32 	%f66, 0fBF800000;
	st.global.v4.f32 	[%r2], {%f66, %f66, %f66, %f66};
	ret;
}

.entry f_p_i_B(
	.param .u32 .ptr .const .align 16 f_p_i_B_param_0,
	.param .align 16 .b8 f_p_i_B_param_1[16],
	.param .u32 .ptr .global .align 16 f_p_i_B_param_2,
	.param .u32 .ptr .global .align 16 f_p_i_B_param_3,
	.param .u32 .ptr .global .align 16 f_p_i_B_param_4,
	.param .u32 .ptr .global .align 16 f_p_i_B_param_5,
	.param .u32 .ptr .global .align 16 f_p_i_B_param_6,
	.param .u32 .ptr .global .align 16 f_p_i_B_param_7
)
.reqntid 16, 8, 1
{
	.reg .f32 	%f<31>;
	.reg .pred 	%p<3>;
	.reg .s32 	%r<38>;


	ld.param.u32 	%r17, [f_p_i_B_param_5];
	// inline asm
	mov.u32 	%r9, %elwreg4;
	// inline asm
	// inline asm
	mov.u32 	%r10, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r11, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r12, %tid.y;
	// inline asm
	add.s32 	%r18, %r12, %r9;
	mad.lo.s32 	%r19, %r11, %r10, %r18;
	shl.b32 	%r20, %r19, 8;
	// inline asm
	mov.u32 	%r13, %elwreg3;
	// inline asm
	// inline asm
	mov.u32 	%r14, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r15, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r16, %tid.x;
	// inline asm
	add.s32 	%r21, %r20, %r13;
	mad.lo.s32 	%r22, %r15, %r14, %r21;
	add.s32 	%r7, %r22, %r16;
	shl.b32 	%r23, %r7, 4;
	add.s32 	%r8, %r17, %r23;
	ld.param.f32 	%f1, [f_p_i_B_param_1+8];
	setp.neu.ftz.f32 	%p1, %f1, 0f00000000;
	@%p1 bra 	BB14_2;

	mov.f32 	%f2, 0f00000000;
	st.global.v4.f32 	[%r8], {%f2, %f2, %f2, %f2};

BB14_2:
	ld.param.u32 	%r37, [f_p_i_B_param_7];
	add.s32 	%r25, %r37, %r23;
	ld.global.f32 	%f3, [%r25];
	setp.ltu.ftz.f32 	%p2, %f3, 0f80000000;
	@%p2 bra 	BB14_4;

	ld.global.v4.f32 	{%f7, %f8, %f9, %f10}, [%r8];
	add.ftz.f32 	%f5, %f10, 0f3F800000;
	mov.f32 	%f6, 0f3F800000;
	st.global.v4.f32 	[%r8], {%f7, %f8, %f9, %f5};
	ld.param.u32 	%r35, [f_p_i_B_param_4];
	add.s32 	%r27, %r35, %r23;
	st.global.v4.f32 	[%r27], {%f6, %f6, %f6, %f6};
	ld.param.u32 	%r34, [f_p_i_B_param_3];
	add.s32 	%r28, %r34, %r23;
	shl.b32 	%r29, %r7, 5;
	ld.param.u32 	%r36, [f_p_i_B_param_6];
	add.s32 	%r30, %r36, %r29;
	ld.global.v4.f32 	{%f19, %f20, %f21, %f22}, [%r30];
	st.global.v4.f32 	[%r28], {%f19, %f20, %f21, %f22};
	ld.param.u32 	%r33, [f_p_i_B_param_2];
	add.s32 	%r31, %r33, %r23;
	ld.param.u32 	%r32, [f_p_i_B_param_0];
	ld.const.v4.f32 	{%f23, %f24, %f25, %f26}, [%r32+80];
	st.global.v4.f32 	[%r31], {%f23, %f24, %f25, %f26};

BB14_4:
	ret;
}

.entry f_p_a(
	.param .u32 .ptr .const .align 16 f_p_a_param_0,
	.param .u32 .ptr .global .align 16 f_p_a_param_1,
	.param .u32 .ptr .global .align 16 f_p_a_param_2,
	.param .u32 .ptr .global .align 16 f_p_a_param_3,
	.param .u32 .ptr .global .align 16 f_p_a_param_4,
	.param .u32 .ptr .global .align 16 f_p_a_param_5,
	.param .u32 .ptr .global .align 16 f_p_a_param_6,
	.param .u32 .ptr .global .align 4 f_p_a_param_7,
	.param .u32 .ptr .global .align 4 f_p_a_param_8,
	.param .u32 .ptr .global .align 16 f_p_a_param_9,
	.param .u32 .ptr .global .align 16 f_p_a_param_10,
	.param .u32 .ptr .global .align 16 f_p_a_param_11,
	.param .u32 .ptr .global .align 16 f_p_a_param_12,
	.param .u32 .ptr .global .align 16 f_p_a_param_13,
	.param .u32 .ptr .global .align 16 f_p_a_param_14
)
.reqntid 16, 8, 1
{
	.reg .f32 	%f<1200>;
	.reg .pred 	%p<55>;
	.reg .s32 	%r<166>;


	ld.param.u32 	%r36, [f_p_a_param_13];
	ld.param.u32 	%r37, [f_p_a_param_14];
	// inline asm
	mov.u32 	%r28, %elwreg4;
	// inline asm
	// inline asm
	mov.u32 	%r29, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r30, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r31, %tid.y;
	// inline asm
	add.s32 	%r38, %r31, %r28;
	mad.lo.s32 	%r39, %r30, %r29, %r38;
	shl.b32 	%r40, %r39, 8;
	// inline asm
	mov.u32 	%r32, %elwreg3;
	// inline asm
	// inline asm
	mov.u32 	%r33, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r34, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r35, %tid.x;
	// inline asm
	add.s32 	%r41, %r40, %r32;
	mad.lo.s32 	%r42, %r34, %r33, %r41;
	add.s32 	%r14, %r42, %r35;
	shl.b32 	%r43, %r14, 4;
	add.s32 	%r44, %r37, %r43;
	ld.global.v4.f32 	{%f1005, %f1006, %f1007, %f1008}, [%r44];
	setp.ltu.ftz.f32 	%p2, %f1005, 0f80000000;
	shl.b32 	%r45, %r14, 5;
	add.s32 	%r15, %r36, %r45;
	@%p2 bra 	BB15_72;

	mov.b32 	 %r16, %f1008;
	shl.b32 	%r46, %r16, 4;
	ld.param.u32 	%r153, [f_p_a_param_1];
	add.s32 	%r47, %r153, %r46;
	ld.global.u32 	%r48, [%r47+12];
	ld.param.u32 	%r152, [f_p_a_param_0];
	mad.lo.s32 	%r17, %r48, 96, %r152;
	ld.const.u32 	%r18, [%r17+196];
	setp.gt.s32 	%p3, %r18, 0;
	@%p3 bra 	BB15_3;

	mov.f32 	%f49, 0f3F800000;
	mov.f32 	%f1197, %f49;
	mov.f32 	%f1198, %f49;
	mov.f32 	%f1199, %f49;
	mov.f32 	%f1076, %f49;
	bra.uni 	BB15_4;

BB15_3:
	mov.f32 	%f62, 0f3F800000;
	sub.ftz.f32 	%f63, %f62, %f1005;
	sub.ftz.f32 	%f65, %f63, %f1006;
	ld.param.u32 	%r157, [f_p_a_param_5];
	add.s32 	%r50, %r157, %r46;
	ld.global.v4.f32 	{%f1081, %f1082, %f1083, %f1084}, [%r50];
	mul.ftz.f32 	%f68, %f1082, %f1005;
	fma.rn.ftz.f32 	%f69, %f1081, %f65, %f68;
	fma.rn.ftz.f32 	%f51, %f1083, %f1006, %f69;
	ld.param.u32 	%r158, [f_p_a_param_6];
	add.s32 	%r51, %r158, %r46;
	ld.global.v4.f32 	{%f1085, %f1086, %f1087, %f1088}, [%r51];
	mul.ftz.f32 	%f73, %f1086, %f1005;
	fma.rn.ftz.f32 	%f74, %f1085, %f65, %f73;
	fma.rn.ftz.f32 	%f55, %f1087, %f1006, %f74;
	ld.const.f32 	%f76, [%r17+200];
	ld.const.f32 	%f77, [%r17+208];
	ld.const.f32 	%f78, [%r17+204];
	ld.const.f32 	%f79, [%r17+212];
	// inline asm
	cvt.rzi.f32.f32 	%f50, %f51;
	// inline asm
	setp.eq.ftz.f32 	%p4, %f51, 0f7F800000;
	setp.eq.ftz.f32 	%p5, %f51, 0fFF800000;
	or.pred  	%p6, %p4, %p5;
	sub.ftz.f32 	%f80, %f51, %f50;
	mov.b32 	 %r52, %f51;
	and.b32  	%r53, %r52, -2147483648;
	mov.b32 	 %r54, %f80;
	and.b32  	%r55, %r54, 2147483647;
	selp.b32 	%r56, 0, %r55, %p6;
	or.b32  	%r57, %r56, %r53;
	mov.b32 	 %f53, %r57;
	// inline asm
	abs.f32 	%f52, %f53;
	// inline asm
	// inline asm
	cvt.rzi.f32.f32 	%f54, %f55;
	// inline asm
	setp.eq.ftz.f32 	%p7, %f55, 0f7F800000;
	setp.eq.ftz.f32 	%p8, %f55, 0fFF800000;
	or.pred  	%p9, %p7, %p8;
	sub.ftz.f32 	%f81, %f55, %f54;
	mov.b32 	 %r58, %f55;
	and.b32  	%r59, %r58, -2147483648;
	mov.b32 	 %r60, %f81;
	and.b32  	%r61, %r60, 2147483647;
	selp.b32 	%r62, 0, %r61, %p9;
	or.b32  	%r63, %r62, %r59;
	mov.b32 	 %f57, %r63;
	// inline asm
	abs.f32 	%f56, %f57;
	// inline asm
	sub.ftz.f32 	%f82, %f62, %f52;
	sub.ftz.f32 	%f83, %f62, %f56;
	add.ftz.f32 	%f84, %f76, 0fBF800000;
	mul.ftz.f32 	%f59, %f52, %f84;
	add.ftz.f32 	%f85, %f78, 0fBF800000;
	mul.ftz.f32 	%f61, %f56, %f85;
	// inline asm
	cvt.rzi.f32.f32 	%f58, %f59;
	// inline asm
	// inline asm
	cvt.rzi.f32.f32 	%f60, %f61;
	// inline asm
	mov.f32 	%f86, 0f00000000;
	max.f32 	%f87, %f86, %f58;
	min.f32 	%f88, %f87, %f77;
	max.f32 	%f89, %f86, %f60;
	min.f32 	%f90, %f89, %f79;
	fma.rn.ftz.f32 	%f91, %f90, %f76, %f88;
	cvt.rzi.ftz.s32.f32 	%r64, %f91;
	add.s32 	%r65, %r64, %r18;
	shl.b32 	%r66, %r65, 2;
	ld.param.u32 	%r159, [f_p_a_param_7];
	add.s32 	%r67, %r159, %r66;
	ld.global.u32 	%r68, [%r67];
	shr.u32 	%r69, %r68, 16;
	and.b32  	%r70, %r69, 255;
	cvt.rn.f32.u32 	%f92, %r70;
	mul.ftz.f32 	%f93, %f92, 0f3B808081;
	shr.u32 	%r71, %r68, 8;
	and.b32  	%r72, %r71, 255;
	cvt.rn.f32.u32 	%f94, %r72;
	mul.ftz.f32 	%f95, %f94, 0f3B808081;
	and.b32  	%r73, %r68, 255;
	cvt.rn.f32.u32 	%f96, %r73;
	mul.ftz.f32 	%f97, %f96, 0f3B808081;
	add.ftz.f32 	%f101, %f58, 0f3F800000;
	max.f32 	%f102, %f86, %f101;
	min.f32 	%f103, %f102, %f77;
	fma.rn.ftz.f32 	%f104, %f90, %f76, %f103;
	cvt.rzi.ftz.s32.f32 	%r74, %f104;
	add.s32 	%r75, %r74, %r18;
	shl.b32 	%r76, %r75, 2;
	add.s32 	%r77, %r159, %r76;
	ld.global.u32 	%r78, [%r77];
	shr.u32 	%r79, %r78, 16;
	and.b32  	%r80, %r79, 255;
	cvt.rn.f32.u32 	%f105, %r80;
	mul.ftz.f32 	%f106, %f105, 0f3B808081;
	shr.u32 	%r81, %r78, 8;
	and.b32  	%r82, %r81, 255;
	cvt.rn.f32.u32 	%f107, %r82;
	mul.ftz.f32 	%f108, %f107, 0f3B808081;
	and.b32  	%r83, %r78, 255;
	cvt.rn.f32.u32 	%f109, %r83;
	mul.ftz.f32 	%f110, %f109, 0f3B808081;
	mul.ftz.f32 	%f1121, %f106, %f52;
	mul.ftz.f32 	%f1122, %f108, %f52;
	mul.ftz.f32 	%f1123, %f110, %f52;
	fma.rn.ftz.f32 	%f1125, %f93, %f82, %f1121;
	fma.rn.ftz.f32 	%f1126, %f95, %f82, %f1122;
	fma.rn.ftz.f32 	%f1127, %f97, %f82, %f1123;
	add.ftz.f32 	%f117, %f60, 0f3F800000;
	max.f32 	%f118, %f86, %f117;
	min.f32 	%f119, %f118, %f79;
	fma.rn.ftz.f32 	%f120, %f119, %f76, %f88;
	cvt.rzi.ftz.s32.f32 	%r84, %f120;
	add.s32 	%r85, %r84, %r18;
	shl.b32 	%r86, %r85, 2;
	add.s32 	%r87, %r159, %r86;
	ld.global.u32 	%r88, [%r87];
	shr.u32 	%r89, %r88, 16;
	and.b32  	%r90, %r89, 255;
	cvt.rn.f32.u32 	%f121, %r90;
	mul.ftz.f32 	%f122, %f121, 0f3B808081;
	shr.u32 	%r91, %r88, 8;
	and.b32  	%r92, %r91, 255;
	cvt.rn.f32.u32 	%f123, %r92;
	mul.ftz.f32 	%f124, %f123, 0f3B808081;
	and.b32  	%r93, %r88, 255;
	cvt.rn.f32.u32 	%f125, %r93;
	mul.ftz.f32 	%f126, %f125, 0f3B808081;
	fma.rn.ftz.f32 	%f127, %f119, %f76, %f103;
	cvt.rzi.ftz.s32.f32 	%r94, %f127;
	add.s32 	%r95, %r94, %r18;
	shl.b32 	%r96, %r95, 2;
	add.s32 	%r97, %r159, %r96;
	ld.global.u32 	%r98, [%r97];
	shr.u32 	%r99, %r98, 16;
	and.b32  	%r100, %r99, 255;
	cvt.rn.f32.u32 	%f128, %r100;
	mul.ftz.f32 	%f129, %f128, 0f3B808081;
	shr.u32 	%r101, %r98, 8;
	and.b32  	%r102, %r101, 255;
	cvt.rn.f32.u32 	%f130, %r102;
	mul.ftz.f32 	%f131, %f130, 0f3B808081;
	and.b32  	%r103, %r98, 255;
	cvt.rn.f32.u32 	%f132, %r103;
	mul.ftz.f32 	%f133, %f132, 0f3B808081;
	mul.ftz.f32 	%f1149, %f129, %f52;
	mul.ftz.f32 	%f1150, %f131, %f52;
	mul.ftz.f32 	%f1151, %f133, %f52;
	fma.rn.ftz.f32 	%f1153, %f122, %f82, %f1149;
	fma.rn.ftz.f32 	%f1154, %f124, %f82, %f1150;
	fma.rn.ftz.f32 	%f1155, %f126, %f82, %f1151;
	mul.ftz.f32 	%f1169, %f1153, %f56;
	mul.ftz.f32 	%f1170, %f1154, %f56;
	mul.ftz.f32 	%f1171, %f1155, %f56;
	fma.rn.ftz.f32 	%f1197, %f1125, %f83, %f1169;
	fma.rn.ftz.f32 	%f1198, %f1126, %f83, %f1170;
	fma.rn.ftz.f32 	%f1199, %f1127, %f83, %f1171;

BB15_4:
	ld.const.u32 	%r104, [%r17+192];
	setp.eq.s32 	%p10, %r104, 0;
	ld.param.u32 	%r163, [f_p_a_param_11];
	add.s32 	%r19, %r163, %r43;
	@%p10 bra 	BB15_6;

	ld.global.v4.f32 	{%f1037, %f1038, %f1039, %f1040}, [%r19];
	mul.ftz.f32 	%f138, %f1037, %f1197;
	ld.const.f32 	%f139, [%r17+176];
	ld.param.u32 	%r164, [f_p_a_param_12];
	add.s32 	%r107, %r164, %r43;
	ld.global.v4.f32 	{%f1041, %f1042, %f1043, %f1044}, [%r107];
	fma.rn.ftz.f32 	%f141, %f138, %f139, %f1041;
	st.global.v4.f32 	[%r107], {%f141, %f1042, %f1043, %f1044};
	ld.global.v4.f32 	{%f1049, %f1050, %f1051, %f1052}, [%r19];
	mul.ftz.f32 	%f143, %f1050, %f1198;
	ld.const.f32 	%f144, [%r17+180];
	fma.rn.ftz.f32 	%f146, %f143, %f144, %f1042;
	st.global.v4.f32 	[%r107], {%f141, %f146, %f1043, %f1044};
	ld.global.v4.f32 	{%f1057, %f1058, %f1059, %f1060}, [%r19];
	mul.ftz.f32 	%f148, %f1059, %f1199;
	ld.const.f32 	%f149, [%r17+184];
	fma.rn.ftz.f32 	%f151, %f148, %f149, %f1043;
	st.global.v4.f32 	[%r107], {%f141, %f146, %f151, %f1044};
	mov.f32 	%f152, 0fBF800000;
	st.global.v4.f32 	[%r15], {%f152, %f152, %f152, %f152};
	ret;

BB15_6:
	ld.const.f32 	%f155, [%r17+160];
	mul.ftz.f32 	%f156, %f1197, %f155;
	ld.global.v4.f32 	{%f1009, %f1010, %f1011, %f1012}, [%r19];
	mul.ftz.f32 	%f158, %f1009, %f156;
	ld.const.f32 	%f159, [%r17+164];
	mul.ftz.f32 	%f160, %f1198, %f159;
	mul.ftz.f32 	%f162, %f1010, %f160;
	ld.const.f32 	%f163, [%r17+168];
	mul.ftz.f32 	%f164, %f1199, %f163;
	mul.ftz.f32 	%f166, %f1011, %f164;
	st.global.v4.f32 	[%r19], {%f158, %f162, %f166, %f1012};
	// inline asm
	mov.u32 	%r108, %tid.y;
	// inline asm
	shl.b32 	%r110, %r108, 4;
	// inline asm
	mov.u32 	%r109, %tid.x;
	// inline asm
	add.s32 	%r111, %r110, %r109;
	shl.b32 	%r112, %r111, 2;
	mov.u32 	%r113, shr_1_loc;
	add.s32 	%r114, %r113, %r112;
	st.shared.f32 	[%r114], %f1005;
	st.shared.f32 	[%r114+512], %f1006;
	ld.param.u32 	%r154, [f_p_a_param_2];
	add.s32 	%r116, %r154, %r46;
	ld.global.v4.f32 	{%f1025, %f1026, %f1027, %f1028}, [%r116];
	mov.f32 	%f169, 0f3F800000;
	sub.ftz.f32 	%f170, %f169, %f1005;
	sub.ftz.f32 	%f171, %f170, %f1006;
	mul.ftz.f32 	%f173, %f1026, %f1005;
	fma.rn.ftz.f32 	%f174, %f1025, %f171, %f173;
	fma.rn.ftz.f32 	%f154, %f1027, %f1006, %f174;
	ld.param.u32 	%r155, [f_p_a_param_3];
	add.s32 	%r117, %r155, %r46;
	ld.global.v4.f32 	{%f1029, %f1030, %f1031, %f1032}, [%r117];
	mul.ftz.f32 	%f178, %f1030, %f1005;
	fma.rn.ftz.f32 	%f179, %f1029, %f171, %f178;
	fma.rn.ftz.f32 	%f6, %f1031, %f1006, %f179;
	ld.param.u32 	%r156, [f_p_a_param_4];
	add.s32 	%r118, %r156, %r46;
	ld.global.v4.f32 	{%f1033, %f1034, %f1035, %f1036}, [%r118];
	mul.ftz.f32 	%f183, %f1034, %f1005;
	fma.rn.ftz.f32 	%f184, %f1033, %f171, %f183;
	fma.rn.ftz.f32 	%f7, %f1035, %f1006, %f184;
	mov.f32 	%f186, 0f00000000;
	mov.f32 	%f1186, %f154;
	mov.f32 	%f1187, %f6;
	mov.f32 	%f1188, %f7;
	mov.f32 	%f1189, %f186;
	// inline asm
	abs.f32 	%f153, %f154;
	// inline asm
	setp.neu.ftz.f32 	%p11, %f153, 0f00000000;
	@%p11 bra 	BB15_10;

	// inline asm
	abs.f32 	%f187, %f6;
	// inline asm
	setp.neu.ftz.f32 	%p12, %f187, 0f00000000;
	@%p12 bra 	BB15_10;

	// inline asm
	abs.f32 	%f189, %f7;
	// inline asm
	setp.neu.ftz.f32 	%p13, %f189, 0f00000000;
	@%p13 bra 	BB15_10;

	// inline asm
	abs.f32 	%f191, %f186;
	// inline asm
	setp.eq.ftz.f32 	%p14, %f191, 0f00000000;
	@%p14 bra 	BB15_11;

BB15_10:
	mul.rn.f32 	%f195, %f154, %f154;
	mul.rn.f32 	%f196, %f6, %f6;
	add.ftz.f32 	%f197, %f195, %f196;
	mul.rn.f32 	%f198, %f7, %f7;
	add.ftz.f32 	%f199, %f197, %f198;
	mul.rn.f32 	%f201, %f186, %f186;
	add.ftz.f32 	%f194, %f199, %f201;
	// inline asm
	rsqrt.approx.f32 	%f193, %f194;
	// inline asm
	mul.rn.f32 	%f202, %f154, %f193;
	mul.rn.f32 	%f203, %f6, %f193;
	mul.rn.f32 	%f204, %f7, %f193;
	mul.rn.f32 	%f205, %f186, %f193;
	mov.f32 	%f1186, %f202;
	mov.f32 	%f1187, %f203;
	mov.f32 	%f1188, %f204;
	mov.f32 	%f1189, %f205;

BB15_11:
	ld.param.u32 	%r162, [f_p_a_param_10];
	add.s32 	%r20, %r162, %r43;
	ld.global.v4.f32 	{%f877, %f878, %f879, %f880}, [%r20];
	// inline asm
	mov.u32 	%r119, %tid.y;
	// inline asm
	shl.b32 	%r122, %r119, 4;
	// inline asm
	mov.u32 	%r120, %tid.x;
	// inline asm
	add.s32 	%r123, %r122, %r120;
	ld.param.u32 	%r161, [f_p_a_param_9];
	add.s32 	%r21, %r161, %r43;
	ld.global.v4.f32 	{%f1001, %f1002, %f1003, %f1004}, [%r21];
	fma.rn.ftz.f32 	%f208, %f877, %f1007, %f1001;
	shl.b32 	%r124, %r123, 2;
	add.s32 	%r126, %r113, %r124;
	st.shared.f32 	[%r126], %f208;
	fma.rn.ftz.f32 	%f210, %f878, %f1007, %f1002;
	st.shared.f32 	[%r126+512], %f210;
	fma.rn.ftz.f32 	%f212, %f879, %f1007, %f1003;
	st.shared.f32 	[%r126+1024], %f212;
	shl.b32 	%r127, %r14, 2;
	ld.param.u32 	%r160, [f_p_a_param_8];
	add.s32 	%r22, %r160, %r127;
	ld.global.u32 	%r128, [%r22];
	mad.lo.s32 	%r165, %r128, 1664525, 1013904223;
	and.b32  	%r129, %r165, 16777215;
	cvt.rn.f32.u32 	%f213, %r129;
	div.rn.ftz.f32 	%f11, %f213, 0f4B800000;
	ld.const.f32 	%f12, [%r17+224];
	setp.lt.ftz.f32 	%p15, %f11, %f12;
	@%p15 bra 	BB15_56;

	ld.const.f32 	%f214, [%r17+228];
	add.ftz.f32 	%f215, %f12, %f214;
	setp.lt.ftz.f32 	%p16, %f11, %f215;
	mul.rn.f32 	%f216, %f1187, %f878;
	mul.rn.f32 	%f217, %f1186, %f877;
	add.ftz.f32 	%f218, %f217, %f216;
	mul.rn.f32 	%f219, %f1188, %f879;
	add.ftz.f32 	%f220, %f218, %f219;
	mul.rn.f32 	%f222, %f1189, %f880;
	add.ftz.f32 	%f17, %f220, %f222;
	@%p16 bra 	BB15_36;

	setp.lt.ftz.f32 	%p17, %f17, 0f00000000;
	neg.ftz.f32 	%f989, %f1186;
	neg.ftz.f32 	%f990, %f1187;
	neg.ftz.f32 	%f991, %f1188;
	neg.ftz.f32 	%f992, %f1189;
	selp.f32 	%f937, %f1186, %f989, %p17;
	selp.f32 	%f938, %f1187, %f990, %p17;
	selp.f32 	%f939, %f1188, %f991, %p17;
	selp.f32 	%f940, %f1189, %f992, %p17;
	mul.rn.f32 	%f223, %f1186, %f937;
	mul.rn.f32 	%f224, %f1187, %f938;
	add.ftz.f32 	%f225, %f223, %f224;
	mul.rn.f32 	%f226, %f1188, %f939;
	add.ftz.f32 	%f227, %f225, %f226;
	mul.rn.f32 	%f228, %f1189, %f940;
	add.ftz.f32 	%f229, %f227, %f228;
	setp.ge.ftz.f32 	%p1, %f229, 0f00000000;
	ld.const.f32 	%f22, [%r17+236];
	@%p1 bra 	BB15_15;

	mov.f32 	%f1173, %f22;
	bra.uni 	BB15_16;

BB15_15:
	rcp.approx.ftz.f32 	%f23, %f22;
	mov.f32 	%f1173, %f23;

BB15_16:
	mov.f32 	%f24, %f1173;
	mul.rn.f32 	%f231, %f877, %f937;
	mul.rn.f32 	%f233, %f878, %f938;
	add.ftz.f32 	%f234, %f231, %f233;
	mul.rn.f32 	%f236, %f879, %f939;
	add.ftz.f32 	%f237, %f234, %f236;
	mul.rn.f32 	%f239, %f880, %f940;
	add.ftz.f32 	%f25, %f237, %f239;
	neg.f32 	%f241, %f25;
	fma.rn.ftz.f32 	%f242, %f241, %f25, %f169;
	mul.ftz.f32 	%f243, %f24, %f24;
	mul.ftz.f32 	%f26, %f243, %f242;
	setp.gt.ftz.f32 	%p18, %f26, 0f3F800000;
	@%p18 bra 	BB15_22;

	sub.ftz.f32 	%f249, %f169, %f26;
	max.f32 	%f245, %f186, %f249;
	// inline asm
	sqrt.approx.f32 	%f244, %f245;
	// inline asm
	mul.ftz.f32 	%f969, %f877, %f24;
	mul.ftz.f32 	%f970, %f878, %f24;
	mul.ftz.f32 	%f971, %f879, %f24;
	mul.ftz.f32 	%f972, %f880, %f24;
	fma.rn.ftz.f32 	%f254, %f25, %f24, %f244;
	neg.f32 	%f985, %f937;
	neg.f32 	%f986, %f938;
	neg.f32 	%f987, %f939;
	neg.f32 	%f988, %f940;
	fma.rn.ftz.f32 	%f1175, %f985, %f254, %f969;
	fma.rn.ftz.f32 	%f1176, %f986, %f254, %f970;
	fma.rn.ftz.f32 	%f1177, %f987, %f254, %f971;
	fma.rn.ftz.f32 	%f1178, %f988, %f254, %f972;
	// inline asm
	abs.f32 	%f246, %f1175;
	// inline asm
	setp.neu.ftz.f32 	%p19, %f246, 0f00000000;
	@%p19 bra 	BB15_21;

	// inline asm
	abs.f32 	%f258, %f1176;
	// inline asm
	setp.neu.ftz.f32 	%p20, %f258, 0f00000000;
	@%p20 bra 	BB15_21;

	// inline asm
	abs.f32 	%f260, %f1177;
	// inline asm
	setp.neu.ftz.f32 	%p21, %f260, 0f00000000;
	@%p21 bra 	BB15_21;

	// inline asm
	abs.f32 	%f262, %f1178;
	// inline asm
	setp.eq.ftz.f32 	%p22, %f262, 0f00000000;
	@%p22 bra 	BB15_27;

BB15_21:
	mul.rn.f32 	%f266, %f1175, %f1175;
	mul.rn.f32 	%f268, %f1176, %f1176;
	add.ftz.f32 	%f269, %f266, %f268;
	mul.rn.f32 	%f271, %f1177, %f1177;
	add.ftz.f32 	%f272, %f269, %f271;
	mul.rn.f32 	%f274, %f1178, %f1178;
	add.ftz.f32 	%f265, %f272, %f274;
	// inline asm
	rsqrt.approx.f32 	%f264, %f265;
	// inline asm
	mul.rn.f32 	%f275, %f1175, %f264;
	mul.rn.f32 	%f276, %f1176, %f264;
	mul.rn.f32 	%f277, %f1177, %f264;
	mul.rn.f32 	%f278, %f1178, %f264;
	mov.f32 	%f1175, %f275;
	mov.f32 	%f1176, %f276;
	mov.f32 	%f1177, %f277;
	mov.f32 	%f1178, %f278;
	bra.uni 	BB15_27;

BB15_22:
	mov.f32 	%f281, 0f40000000;
	mul.ftz.f32 	%f933, %f937, %f281;
	mul.ftz.f32 	%f934, %f938, %f281;
	mul.ftz.f32 	%f935, %f939, %f281;
	mul.rn.f32 	%f283, %f937, %f877;
	mul.rn.f32 	%f285, %f938, %f878;
	add.ftz.f32 	%f286, %f283, %f285;
	mul.rn.f32 	%f288, %f939, %f879;
	add.ftz.f32 	%f289, %f286, %f288;
	mul.rn.f32 	%f291, %f940, %f880;
	add.ftz.f32 	%f292, %f289, %f291;
	neg.f32 	%f953, %f933;
	neg.f32 	%f954, %f934;
	neg.f32 	%f955, %f935;
	fma.rn.ftz.f32 	%f925, %f953, %f292, %f877;
	fma.rn.ftz.f32 	%f926, %f954, %f292, %f878;
	fma.rn.ftz.f32 	%f927, %f955, %f292, %f879;
	mov.f32 	%f1175, %f925;
	mov.f32 	%f1176, %f926;
	mov.f32 	%f1177, %f927;
	mov.f32 	%f1178, %f186;
	// inline asm
	abs.f32 	%f279, %f925;
	// inline asm
	setp.neu.ftz.f32 	%p23, %f279, 0f00000000;
	@%p23 bra 	BB15_26;

	// inline asm
	abs.f32 	%f297, %f926;
	// inline asm
	setp.neu.ftz.f32 	%p24, %f297, 0f00000000;
	@%p24 bra 	BB15_26;

	// inline asm
	abs.f32 	%f299, %f927;
	// inline asm
	setp.neu.ftz.f32 	%p25, %f299, 0f00000000;
	@%p25 bra 	BB15_26;

	// inline asm
	abs.f32 	%f301, %f186;
	// inline asm
	setp.eq.ftz.f32 	%p26, %f301, 0f00000000;
	@%p26 bra 	BB15_27;

BB15_26:
	mul.rn.f32 	%f305, %f925, %f925;
	mul.rn.f32 	%f307, %f926, %f926;
	add.ftz.f32 	%f308, %f305, %f307;
	mul.rn.f32 	%f310, %f927, %f927;
	add.ftz.f32 	%f311, %f308, %f310;
	mul.rn.f32 	%f313, %f186, %f186;
	add.ftz.f32 	%f304, %f311, %f313;
	// inline asm
	rsqrt.approx.f32 	%f303, %f304;
	// inline asm
	mul.rn.f32 	%f314, %f925, %f303;
	mul.rn.f32 	%f315, %f926, %f303;
	mul.rn.f32 	%f316, %f927, %f303;
	mul.rn.f32 	%f317, %f186, %f303;
	mov.f32 	%f1175, %f314;
	mov.f32 	%f1176, %f315;
	mov.f32 	%f1177, %f316;
	mov.f32 	%f1178, %f317;

BB15_27:
	add.ftz.f32 	%f318, %f22, 0fBF800000;
	mul.ftz.f32 	%f319, %f318, %f318;
	add.ftz.f32 	%f320, %f22, 0f3F800000;
	mul.ftz.f32 	%f321, %f320, %f320;
	div.approx.ftz.f32 	%f29, %f319, %f321;
	@%p1 bra 	BB15_29;

	mul.rn.f32 	%f324, %f1175, %f1186;
	mul.rn.f32 	%f327, %f1176, %f1187;
	add.ftz.f32 	%f328, %f324, %f327;
	mul.rn.f32 	%f331, %f1177, %f1188;
	add.ftz.f32 	%f332, %f328, %f331;
	mul.rn.f32 	%f335, %f1178, %f1189;
	add.ftz.f32 	%f1174, %f332, %f335;
	bra.uni 	BB15_30;

BB15_29:
	neg.ftz.f32 	%f1174, %f25;

BB15_30:
	sub.ftz.f32 	%f337, %f169, %f29;
	sub.ftz.f32 	%f338, %f169, %f1174;
	mul.ftz.f32 	%f339, %f337, %f338;
	mul.ftz.f32 	%f340, %f339, %f338;
	mul.ftz.f32 	%f341, %f340, %f338;
	mul.ftz.f32 	%f342, %f341, %f338;
	fma.rn.ftz.f32 	%f343, %f342, %f338, %f29;
	min.f32 	%f344, %f169, %f343;
	max.f32 	%f346, %f186, %f344;
	fma.rn.ftz.f32 	%f347, %f346, 0f3F400000, 0f3E800000;
	mad.lo.s32 	%r165, %r165, 1664525, 1013904223;
	and.b32  	%r130, %r165, 16777215;
	cvt.rn.f32.u32 	%f348, %r130;
	div.rn.ftz.f32 	%f349, %f348, 0f4B800000;
	setp.lt.ftz.f32 	%p27, %f349, %f347;
	@%p27 bra 	BB15_31;
	bra.uni 	BB15_71;

BB15_31:
	mov.f32 	%f352, 0f40000000;
	mul.ftz.f32 	%f889, %f1186, %f352;
	mul.ftz.f32 	%f890, %f1187, %f352;
	mul.ftz.f32 	%f891, %f1188, %f352;
	neg.f32 	%f905, %f889;
	neg.f32 	%f906, %f890;
	neg.f32 	%f907, %f891;
	fma.rn.ftz.f32 	%f881, %f905, %f17, %f877;
	fma.rn.ftz.f32 	%f882, %f906, %f17, %f878;
	fma.rn.ftz.f32 	%f883, %f907, %f17, %f879;
	mov.f32 	%f1175, %f881;
	mov.f32 	%f1176, %f882;
	mov.f32 	%f1177, %f883;
	mov.f32 	%f1178, %f186;
	// inline asm
	abs.f32 	%f350, %f881;
	// inline asm
	setp.neu.ftz.f32 	%p28, %f350, 0f00000000;
	@%p28 bra 	BB15_35;

	// inline asm
	abs.f32 	%f357, %f882;
	// inline asm
	setp.neu.ftz.f32 	%p29, %f357, 0f00000000;
	@%p29 bra 	BB15_35;

	// inline asm
	abs.f32 	%f359, %f883;
	// inline asm
	setp.neu.ftz.f32 	%p30, %f359, 0f00000000;
	@%p30 bra 	BB15_35;

	// inline asm
	abs.f32 	%f361, %f186;
	// inline asm
	setp.eq.ftz.f32 	%p31, %f361, 0f00000000;
	@%p31 bra 	BB15_71;

BB15_35:
	mul.rn.f32 	%f365, %f881, %f881;
	mul.rn.f32 	%f367, %f882, %f882;
	add.ftz.f32 	%f368, %f365, %f367;
	mul.rn.f32 	%f370, %f883, %f883;
	add.ftz.f32 	%f371, %f368, %f370;
	mul.rn.f32 	%f373, %f186, %f186;
	add.ftz.f32 	%f364, %f371, %f373;
	// inline asm
	rsqrt.approx.f32 	%f363, %f364;
	// inline asm
	mul.rn.f32 	%f374, %f881, %f363;
	mul.rn.f32 	%f375, %f882, %f363;
	mul.rn.f32 	%f376, %f883, %f363;
	mul.rn.f32 	%f377, %f186, %f363;
	mov.f32 	%f1175, %f374;
	mov.f32 	%f1176, %f375;
	mov.f32 	%f1177, %f376;
	mov.f32 	%f1178, %f377;
	bra.uni 	BB15_71;

BB15_36:
	mov.f32 	%f380, 0f40000000;
	mul.ftz.f32 	%f857, %f1186, %f380;
	mul.ftz.f32 	%f858, %f1187, %f380;
	mul.ftz.f32 	%f859, %f1188, %f380;
	neg.f32 	%f873, %f857;
	neg.f32 	%f874, %f858;
	neg.f32 	%f875, %f859;
	fma.rn.ftz.f32 	%f849, %f873, %f17, %f877;
	fma.rn.ftz.f32 	%f850, %f874, %f17, %f878;
	fma.rn.ftz.f32 	%f851, %f875, %f17, %f879;
	mov.f32 	%f1175, %f849;
	mov.f32 	%f1176, %f850;
	mov.f32 	%f1177, %f851;
	mov.f32 	%f1178, %f186;
	// inline asm
	abs.f32 	%f378, %f849;
	// inline asm
	setp.neu.ftz.f32 	%p32, %f378, 0f00000000;
	@%p32 bra 	BB15_40;

	// inline asm
	abs.f32 	%f385, %f850;
	// inline asm
	setp.neu.ftz.f32 	%p33, %f385, 0f00000000;
	@%p33 bra 	BB15_40;

	// inline asm
	abs.f32 	%f387, %f851;
	// inline asm
	setp.neu.ftz.f32 	%p34, %f387, 0f00000000;
	@%p34 bra 	BB15_40;

	// inline asm
	abs.f32 	%f389, %f186;
	// inline asm
	setp.eq.ftz.f32 	%p35, %f389, 0f00000000;
	@%p35 bra 	BB15_41;

BB15_40:
	mul.rn.f32 	%f393, %f849, %f849;
	mul.rn.f32 	%f395, %f850, %f850;
	add.ftz.f32 	%f396, %f393, %f395;
	mul.rn.f32 	%f398, %f851, %f851;
	add.ftz.f32 	%f399, %f396, %f398;
	mul.rn.f32 	%f401, %f186, %f186;
	add.ftz.f32 	%f392, %f399, %f401;
	// inline asm
	rsqrt.approx.f32 	%f391, %f392;
	// inline asm
	mul.rn.f32 	%f402, %f849, %f391;
	mul.rn.f32 	%f403, %f850, %f391;
	mul.rn.f32 	%f404, %f851, %f391;
	mul.rn.f32 	%f405, %f186, %f391;
	mov.f32 	%f1175, %f402;
	mov.f32 	%f1176, %f403;
	mov.f32 	%f1177, %f404;
	mov.f32 	%f1178, %f405;

BB15_41:
	ld.const.u32 	%r131, [%r17+240];
	setp.eq.s32 	%p36, %r131, 0;
	@%p36 bra 	BB15_71;

	mad.lo.s32 	%r132, %r165, 1664525, 1013904223;
	and.b32  	%r133, %r132, 16777215;
	cvt.rn.f32.u32 	%f422, %r133;
	div.rn.ftz.f32 	%f423, %f422, 0f4B800000;
	mad.lo.s32 	%r165, %r132, 1664525, 1013904223;
	and.b32  	%r134, %r165, 16777215;
	cvt.rn.f32.u32 	%f424, %r134;
	div.rn.ftz.f32 	%f412, %f424, 0f4B800000;
	ld.const.f32 	%f413, [%r17+244];
	mul.ftz.f32 	%f417, %f423, 0f40C90FE4;
	ld.const.f32 	%f408, [%r17+248];
	// inline asm
	lg2.approx.f32 	%f406, %f412; 
	mul.f32 	%f406, %f406, %f408; 
	ex2.approx.f32 	%f406, %f406;
	// inline asm
	sub.ftz.f32 	%f426, %f169, %f406;
	max.f32 	%f410, %f186, %f426;
	// inline asm
	sqrt.approx.f32 	%f409, %f410;
	// inline asm
	// inline asm
	lg2.approx.f32 	%f411, %f412; 
	mul.f32 	%f411, %f411, %f413; 
	ex2.approx.f32 	%f411, %f411;
	// inline asm
	// inline asm
	sin.approx.f32 	%f414, %f417;
	// inline asm
	mul.ftz.f32 	%f36, %f414, %f409;
	// inline asm
	cos.approx.f32 	%f416, %f417;
	// inline asm
	mul.ftz.f32 	%f37, %f416, %f409;
	// inline asm
	abs.f32 	%f418, %f1175;
	// inline asm
	// inline asm
	abs.f32 	%f420, %f1177;
	// inline asm
	setp.ltu.ftz.f32 	%p37, %f418, %f420;
	@%p37 bra 	BB15_44;

	neg.ftz.f32 	%f428, %f1176;
	mov.f32 	%f1193, %f428;
	mov.f32 	%f1194, %f1175;
	mov.f32 	%f1195, %f186;
	bra.uni 	BB15_45;

BB15_44:
	neg.ftz.f32 	%f430, %f1177;
	mov.f32 	%f1193, %f186;
	mov.f32 	%f1194, %f430;
	mov.f32 	%f1195, %f1176;

BB15_45:
	mov.f32 	%f1196, %f186;
	// inline asm
	abs.f32 	%f432, %f1193;
	// inline asm
	setp.neu.ftz.f32 	%p38, %f432, 0f00000000;
	@%p38 bra 	BB15_49;

	// inline asm
	abs.f32 	%f434, %f1194;
	// inline asm
	setp.neu.ftz.f32 	%p39, %f434, 0f00000000;
	@%p39 bra 	BB15_49;

	// inline asm
	abs.f32 	%f436, %f1195;
	// inline asm
	setp.neu.ftz.f32 	%p40, %f436, 0f00000000;
	@%p40 bra 	BB15_49;

	// inline asm
	abs.f32 	%f438, %f1196;
	// inline asm
	setp.eq.ftz.f32 	%p41, %f438, 0f00000000;
	@%p41 bra 	BB15_50;

BB15_49:
	mul.rn.f32 	%f442, %f1193, %f1193;
	mul.rn.f32 	%f444, %f1194, %f1194;
	add.ftz.f32 	%f445, %f442, %f444;
	mul.rn.f32 	%f447, %f1195, %f1195;
	add.ftz.f32 	%f448, %f445, %f447;
	mul.rn.f32 	%f450, %f1196, %f1196;
	add.ftz.f32 	%f441, %f448, %f450;
	// inline asm
	rsqrt.approx.f32 	%f440, %f441;
	// inline asm
	mul.rn.f32 	%f451, %f1193, %f440;
	mul.rn.f32 	%f452, %f1194, %f440;
	mul.rn.f32 	%f453, %f1195, %f440;
	mul.rn.f32 	%f454, %f1196, %f440;
	mov.f32 	%f1193, %f451;
	mov.f32 	%f1194, %f452;
	mov.f32 	%f1195, %f453;
	mov.f32 	%f1196, %f454;

BB15_50:
	mul.rn.f32 	%f458, %f1194, %f1177;
	mul.rn.f32 	%f461, %f1195, %f1176;
	sub.ftz.f32 	%f462, %f458, %f461;
	mul.rn.f32 	%f463, %f1195, %f1175;
	mul.rn.f32 	%f465, %f1193, %f1177;
	sub.ftz.f32 	%f466, %f463, %f465;
	mul.rn.f32 	%f467, %f1193, %f1176;
	mul.rn.f32 	%f468, %f1194, %f1175;
	sub.ftz.f32 	%f469, %f467, %f468;
	mul.ftz.f32 	%f813, %f462, %f36;
	mul.ftz.f32 	%f814, %f466, %f36;
	mul.ftz.f32 	%f815, %f469, %f36;
	mul.ftz.f32 	%f816, %f186, %f36;
	fma.rn.ftz.f32 	%f817, %f1175, %f411, %f813;
	fma.rn.ftz.f32 	%f818, %f1176, %f411, %f814;
	fma.rn.ftz.f32 	%f819, %f1177, %f411, %f815;
	fma.rn.ftz.f32 	%f820, %f1178, %f411, %f816;
	fma.rn.ftz.f32 	%f1190, %f1193, %f37, %f817;
	fma.rn.ftz.f32 	%f1191, %f1194, %f37, %f818;
	fma.rn.ftz.f32 	%f1192, %f1195, %f37, %f819;
	fma.rn.ftz.f32 	%f772, %f1196, %f37, %f820;
	// inline asm
	abs.f32 	%f455, %f1190;
	// inline asm
	setp.neu.ftz.f32 	%p42, %f455, 0f00000000;
	@%p42 bra 	BB15_54;

	// inline asm
	abs.f32 	%f480, %f1191;
	// inline asm
	setp.neu.ftz.f32 	%p43, %f480, 0f00000000;
	@%p43 bra 	BB15_54;

	// inline asm
	abs.f32 	%f482, %f1192;
	// inline asm
	setp.neu.ftz.f32 	%p44, %f482, 0f00000000;
	@%p44 bra 	BB15_54;

	// inline asm
	abs.f32 	%f484, %f772;
	// inline asm
	setp.eq.ftz.f32 	%p45, %f484, 0f00000000;
	@%p45 bra 	BB15_55;

BB15_54:
	mul.rn.f32 	%f488, %f1190, %f1190;
	mul.rn.f32 	%f490, %f1191, %f1191;
	add.ftz.f32 	%f491, %f488, %f490;
	mul.rn.f32 	%f493, %f1192, %f1192;
	add.ftz.f32 	%f494, %f491, %f493;
	mul.rn.f32 	%f496, %f772, %f772;
	add.ftz.f32 	%f487, %f494, %f496;
	// inline asm
	rsqrt.approx.f32 	%f486, %f487;
	// inline asm
	mul.rn.f32 	%f497, %f1190, %f486;
	mul.rn.f32 	%f498, %f1191, %f486;
	mul.rn.f32 	%f499, %f1192, %f486;
	mul.rn.f32 	%f500, %f772, %f486;
	mov.f32 	%f1190, %f497;
	mov.f32 	%f1191, %f498;
	mov.f32 	%f1192, %f499;
	mov.f32 	%f768, %f500;

BB15_55:
	mov.f32 	%f1175, %f1190;
	mov.f32 	%f1176, %f1191;
	mov.f32 	%f1177, %f1192;
	bra.uni 	BB15_70;

BB15_56:
	mad.lo.s32 	%r135, %r165, 1664525, 1013904223;
	and.b32  	%r136, %r135, 16777215;
	cvt.rn.f32.u32 	%f514, %r136;
	div.rn.ftz.f32 	%f515, %f514, 0f4B800000;
	mad.lo.s32 	%r165, %r135, 1664525, 1013904223;
	and.b32  	%r137, %r165, 16777215;
	cvt.rn.f32.u32 	%f516, %r137;
	div.rn.ftz.f32 	%f505, %f516, 0f4B800000;
	mul.ftz.f32 	%f509, %f515, 0f40C90FE4;
	sub.ftz.f32 	%f518, %f169, %f505;
	max.f32 	%f503, %f186, %f518;
	// inline asm
	sqrt.approx.f32 	%f502, %f503;
	// inline asm
	// inline asm
	sqrt.approx.f32 	%f504, %f505;
	// inline asm
	// inline asm
	sin.approx.f32 	%f506, %f509;
	// inline asm
	mul.ftz.f32 	%f44, %f506, %f502;
	// inline asm
	cos.approx.f32 	%f508, %f509;
	// inline asm
	mul.ftz.f32 	%f45, %f508, %f502;
	// inline asm
	abs.f32 	%f510, %f1186;
	// inline asm
	// inline asm
	abs.f32 	%f512, %f1188;
	// inline asm
	setp.ltu.ftz.f32 	%p46, %f510, %f512;
	@%p46 bra 	BB15_58;

	neg.ftz.f32 	%f520, %f1187;
	mov.f32 	%f1182, %f520;
	mov.f32 	%f1183, %f1186;
	mov.f32 	%f1184, %f186;
	bra.uni 	BB15_59;

BB15_58:
	neg.ftz.f32 	%f522, %f1188;
	mov.f32 	%f1182, %f186;
	mov.f32 	%f1183, %f522;
	mov.f32 	%f1184, %f1187;

BB15_59:
	mov.f32 	%f1185, %f186;
	// inline asm
	abs.f32 	%f524, %f1182;
	// inline asm
	setp.neu.ftz.f32 	%p47, %f524, 0f00000000;
	@%p47 bra 	BB15_63;

	// inline asm
	abs.f32 	%f526, %f1183;
	// inline asm
	setp.neu.ftz.f32 	%p48, %f526, 0f00000000;
	@%p48 bra 	BB15_63;

	// inline asm
	abs.f32 	%f528, %f1184;
	// inline asm
	setp.neu.ftz.f32 	%p49, %f528, 0f00000000;
	@%p49 bra 	BB15_63;

	// inline asm
	abs.f32 	%f530, %f1185;
	// inline asm
	setp.eq.ftz.f32 	%p50, %f530, 0f00000000;
	@%p50 bra 	BB15_64;

BB15_63:
	mul.rn.f32 	%f534, %f1182, %f1182;
	mul.rn.f32 	%f536, %f1183, %f1183;
	add.ftz.f32 	%f537, %f534, %f536;
	mul.rn.f32 	%f539, %f1184, %f1184;
	add.ftz.f32 	%f540, %f537, %f539;
	mul.rn.f32 	%f542, %f1185, %f1185;
	add.ftz.f32 	%f533, %f540, %f542;
	// inline asm
	rsqrt.approx.f32 	%f532, %f533;
	// inline asm
	mul.rn.f32 	%f543, %f1182, %f532;
	mul.rn.f32 	%f544, %f1183, %f532;
	mul.rn.f32 	%f545, %f1184, %f532;
	mul.rn.f32 	%f546, %f1185, %f532;
	mov.f32 	%f1182, %f543;
	mov.f32 	%f1183, %f544;
	mov.f32 	%f1184, %f545;
	mov.f32 	%f1185, %f546;

BB15_64:
	mul.rn.f32 	%f550, %f1183, %f1188;
	mul.rn.f32 	%f553, %f1184, %f1187;
	sub.ftz.f32 	%f554, %f550, %f553;
	mul.rn.f32 	%f555, %f1184, %f1186;
	mul.rn.f32 	%f557, %f1182, %f1188;
	sub.ftz.f32 	%f558, %f555, %f557;
	mul.rn.f32 	%f559, %f1182, %f1187;
	mul.rn.f32 	%f560, %f1183, %f1186;
	sub.ftz.f32 	%f561, %f559, %f560;
	mul.ftz.f32 	%f733, %f554, %f44;
	mul.ftz.f32 	%f734, %f558, %f44;
	mul.ftz.f32 	%f735, %f561, %f44;
	mul.ftz.f32 	%f736, %f186, %f44;
	fma.rn.ftz.f32 	%f737, %f1186, %f504, %f733;
	fma.rn.ftz.f32 	%f738, %f1187, %f504, %f734;
	fma.rn.ftz.f32 	%f739, %f1188, %f504, %f735;
	fma.rn.ftz.f32 	%f740, %f1189, %f504, %f736;
	fma.rn.ftz.f32 	%f1179, %f1182, %f45, %f737;
	fma.rn.ftz.f32 	%f1180, %f1183, %f45, %f738;
	fma.rn.ftz.f32 	%f1181, %f1184, %f45, %f739;
	fma.rn.ftz.f32 	%f688, %f1185, %f45, %f740;
	// inline asm
	abs.f32 	%f547, %f1179;
	// inline asm
	setp.neu.ftz.f32 	%p51, %f547, 0f00000000;
	@%p51 bra 	BB15_68;

	// inline asm
	abs.f32 	%f572, %f1180;
	// inline asm
	setp.neu.ftz.f32 	%p52, %f572, 0f00000000;
	@%p52 bra 	BB15_68;

	// inline asm
	abs.f32 	%f574, %f1181;
	// inline asm
	setp.neu.ftz.f32 	%p53, %f574, 0f00000000;
	@%p53 bra 	BB15_68;

	// inline asm
	abs.f32 	%f576, %f688;
	// inline asm
	setp.eq.ftz.f32 	%p54, %f576, 0f00000000;
	@%p54 bra 	BB15_69;

BB15_68:
	mul.rn.f32 	%f580, %f1179, %f1179;
	mul.rn.f32 	%f582, %f1180, %f1180;
	add.ftz.f32 	%f583, %f580, %f582;
	mul.rn.f32 	%f585, %f1181, %f1181;
	add.ftz.f32 	%f586, %f583, %f585;
	mul.rn.f32 	%f588, %f688, %f688;
	add.ftz.f32 	%f579, %f586, %f588;
	// inline asm
	rsqrt.approx.f32 	%f578, %f579;
	// inline asm
	mul.rn.f32 	%f589, %f1179, %f578;
	mul.rn.f32 	%f590, %f1180, %f578;
	mul.rn.f32 	%f591, %f1181, %f578;
	mul.rn.f32 	%f592, %f688, %f578;
	mov.f32 	%f1179, %f589;
	mov.f32 	%f1180, %f590;
	mov.f32 	%f1181, %f591;
	mov.f32 	%f684, %f592;

BB15_69:
	mov.f32 	%f1175, %f1179;
	mov.f32 	%f1176, %f1180;
	mov.f32 	%f1177, %f1181;

BB15_70:
	mov.f32 	%f1178, %f186;

BB15_71:
	st.global.u32 	[%r22], %r165;
	ld.global.v4.f32 	{%f641, %f642, %f643, %f644}, [%r20];
	st.global.v4.f32 	[%r20], {%f1175, %f1176, %f1177, %f644};
	// inline asm
	mov.u32 	%r138, %tid.y;
	// inline asm
	shl.b32 	%r142, %r138, 4;
	// inline asm
	mov.u32 	%r139, %tid.x;
	// inline asm
	add.s32 	%r143, %r142, %r139;
	ld.param.u32 	%r151, [f_p_a_param_0];
	ld.const.f32 	%f594, [%r151+96];
	shl.b32 	%r144, %r143, 2;
	add.s32 	%r146, %r113, %r144;
	ld.shared.f32 	%f596, [%r146];
	fma.rn.ftz.f32 	%f597, %f1175, %f594, %f596;
	st.shared.f32 	[%r146], %f597;
	ld.shared.f32 	%f599, [%r146+512];
	fma.rn.ftz.f32 	%f600, %f1176, %f594, %f599;
	st.shared.f32 	[%r146+512], %f600;
	ld.shared.f32 	%f602, [%r146+1024];
	fma.rn.ftz.f32 	%f603, %f1177, %f594, %f602;
	st.shared.f32 	[%r146+1024], %f603;
	ld.global.v4.f32 	{%f657, %f658, %f659, %f660}, [%r21];
	st.global.v4.f32 	[%r21], {%f597, %f600, %f603, %f660};
	st.global.v4.f32 	[%r15], {%f1175, %f1176, %f1177, %f1178};
	// inline asm
	mov.u32 	%r140, %tid.y;
	// inline asm
	shl.b32 	%r147, %r140, 4;
	// inline asm
	mov.u32 	%r141, %tid.x;
	// inline asm
	add.s32 	%r148, %r147, %r141;
	shl.b32 	%r149, %r148, 2;
	add.s32 	%r150, %r113, %r149;
	ld.shared.f32 	%f604, [%r150];
	ld.shared.f32 	%f605, [%r150+512];
	ld.shared.f32 	%f606, [%r150+1024];
	ld.const.f32 	%f607, [%r151+100];
	st.global.v4.f32 	[%r15+16], {%f604, %f605, %f606, %f607};
	ret;

BB15_72:
	mov.f32 	%f608, 0fBF800000;
	st.global.v4.f32 	[%r15], {%f608, %f608, %f608, %f608};
	ret;
}

.entry f_p_f(
	.param .u32 .ptr .const .align 16 f_p_f_param_0,
	.param .u32 .ptr .global .align 16 f_p_f_param_1,
	.param .u32 .ptr .global .align 16 f_p_f_param_2
)
.reqntid 16, 8, 1
{
	.reg .f32 	%f<42>;
	.reg .pred 	%p<2>;
	.reg .s32 	%r<23>;


	ld.param.u32 	%r11, [f_p_f_param_1];
	ld.param.u32 	%r12, [f_p_f_param_2];
	// inline asm
	mov.u32 	%r3, %elwreg4;
	// inline asm
	// inline asm
	mov.u32 	%r4, %ntid.y;
	// inline asm
	// inline asm
	mov.u32 	%r5, %ctaid.y;
	// inline asm
	// inline asm
	mov.u32 	%r6, %tid.y;
	// inline asm
	add.s32 	%r13, %r6, %r3;
	mad.lo.s32 	%r14, %r5, %r4, %r13;
	shl.b32 	%r15, %r14, 8;
	// inline asm
	mov.u32 	%r7, %elwreg3;
	// inline asm
	// inline asm
	mov.u32 	%r8, %ntid.x;
	// inline asm
	// inline asm
	mov.u32 	%r9, %ctaid.x;
	// inline asm
	// inline asm
	mov.u32 	%r10, %tid.x;
	// inline asm
	add.s32 	%r16, %r15, %r7;
	mad.lo.s32 	%r17, %r9, %r8, %r16;
	add.s32 	%r18, %r17, %r10;
	shl.b32 	%r19, %r18, 4;
	add.s32 	%r20, %r11, %r19;
	ld.global.v4.f32 	{%f30, %f31, %f32, %f33}, [%r20];
	setp.gt.ftz.f32 	%p1, %f33, 0f00000000;
	add.s32 	%r1, %r12, %r19;
	@%p1 bra 	BB16_2;

	ld.param.u32 	%r21, [f_p_f_param_0];
	ld.const.v4.f32 	{%f38, %f39, %f40, %f41}, [%r21+144];
	st.global.v4.f32 	[%r1], {%f38, %f39, %f40, %f41};
	ret;

BB16_2:
	ld.param.u32 	%r22, [f_p_f_param_0];
	ld.const.f32 	%f2, [%r22+140];
	mul.ftz.f32 	%f26, %f30, %f2;
	mul.ftz.f32 	%f27, %f31, %f2;
	mul.ftz.f32 	%f28, %f32, %f2;
	mul.ftz.f32 	%f29, %f33, %f2;
	cvt.sat.f32.f32 	%f7, %f26; 
	cvt.sat.f32.f32 	%f9, %f27; 
	cvt.sat.f32.f32 	%f11, %f28; 
	cvt.sat.f32.f32 	%f13, %f29; 
	st.global.v4.f32 	[%r1], {%f7, %f9, %f11, %f13};
	ret;
}



